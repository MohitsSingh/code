YYYY/MM/DD

2012/05/01
-----------

Exploring Tiny Images .... (Devi Parikh et. al)

Given high resolution images, humans are able to provide fast and precise answers
on the identity of the objects in the scene. What is the time for response ? 
If it is small, then can we bound the amount of information incident on the eyes
before a decision is made? 
I would like to know exactly what is the "raw data" that the brain receives - obviously,
this is >= sufficient to make a decision. 

---------------
run object proposals in multiscale...

s
what is the best scale for an object? 
does michael use just 

how many pixels make an image


2012/05/13
----------
The notion of recognition using tiny images has not been fully applied to images such as 
e.g, PASCAL. Assume we have a good classifier for tiny images. Our task is object detection.
Given a new image, we want to find all the tiny (e.g) 32x32 subimages (over location and scale)
that are distinctive w.r.t to the object class. We can then apply the classifier to each part (thinglet)
independently. Of course what might follow is an inference stage to combine the different detected parts, or
simply a greedy heuristic to choose the most prominent detections.


30
2012/5/14
----------
labelling tiny images is hard. A Single sift descriptor, or just the raw gray values, are far from doing the job.
Maybe the sift descriptor fails since it's location is chosen arbitrarily rather than on a keypoint. 

One problem is that I chose arbitrary image segments (according to the segmentation). Other segments might be 
misaligned or of a different scale, so a single global descriptor will not correctly find similar segments.

I'm also currently trying a bag-of-words approach (which I'm pretty sure nobody's tried before on images of this size).
There is a lot of information in a 32x32 image and I can describe it using BOW just as well. 
This should also give me some flexibility regarding the spatial distribution of the features. Indeed, it gives slightly better results,
but still disappointing much of the time. 

---> Note that my disappointment might be unjustified: is there really a rich enough diversity of segments, that
would contain the correct match for the ones I'm looking for ? I believe there is, but I need to prove it. 

One way to check this is to constrain the search for segments from the same class only (i.e. "cheat").
This would enlarge the chance that I stumble upon the correct segment, or one that should pop up as similar to the sought one.

Uri suggested that I make my search more flexible by allowing each pair of images patches to be matched by looking in a slightly broader neighborhood and looking for the a in A and b in B whose correlation score (or other matching) is 
the highest.

Another way is making the search assymetrical, by breaking a query image into segments but keeping the "training" images
intact.
For each segment and each training image, search the training image for candidate locations in multiple scales (such
that almost any rectangle will eventually become 32x32 or similar). How many scales should I check? The most extreme
case is increments of 1 pixel (!) per scale. Given that the image is ~hundreds of pixels wide, I require hundreds of scales!!
A more sane approach is increments of 10 pixels, so we have a few tens of scales. I expect the divesity of sought
templates to make up for the lack of scale continuity.
 
Yet another variation on this idea is to drop the idea of segmentation altogether, but search exhaustively 
or randomally many subwindows of the query image inside the training set, using one of the methods mentioned above. 

A different but less accurate approach is to use image retrieval methods - extract descriptors at multiple scales from each image and try to find each fragment in the collection of images. One a fragment is seemingly found, use 
a verification.

I can also use the exemplar svm of maliciewicz, to find each fragment individually.

---------------------
Here are some experiments I can conduct:
1. extract objects (using the ground truth bounding boxes) from all image. Learn a classifier to classify
them into the different object types. This will reduce the problem to that of classification, not detection.

If this works well, I'll have to:
1.1 Conduct an efficient search for the correct segments
1.2 Make the classifier robust to deal with "wrong" segments.This can be done by adding a "background" class...
but why should I expect the background class to be accurate?
Training a 1 vs all classifier for each class would result in multiple classifiers. How should the output 
of multiple classifiers be normalized and merged?

.... this is too cumbersome. I will have problem advancing from the stage of a classifier to a detector. 
I should train the detector to deal with "wrong" windows as well - e.g, attempt to achieve the same results 
as "selective search".

Once I have a more-or-less normal object detector, I will proceed with two stages:
1. normalize the detected segments to 32x32, as I have already done, and apply the same learning framework 
to try to classify them. See how this helps or hinders the result. It would probably
hinder it, since the prominent feature will be lost. So...
 
2. Enter the fragments: incroporate the fragment based detection. This means that a learning framework should
be able to learn how to assign parts with varying appearance to the same object class. A single SVM is expected
to perform rather poorly on this task as it is not really generalization, but technically like learning different
classes. 
	2.1 One approach is to first cluster all of the parts obtained from a single object class and training a
different SVM for each cluster. A more extreme approach is to train a different SVM for each fragment...

	 

[another thought: 
What I'm actually looking for is a 32x32 image fragment. This doesn't really mean that all segments should be 
resized to this size, but maybe only those that are of similar size when segmented (in their respective scale).
So, instead of finding small and large segments and resizing all of them, segment each scale into relatively small
parts, and from that scale, choose the parts which are close in size to 32x32... will this really help? It will remove
large segments from their natural scale, and "hope" that they will be recovered again at a smaller scale.] 

17/5/2012
---------
use L0 smoothing to train on images. This will remove lots of texture from the images. IS this useful texture or 
not? 
Anyway, learning to classifiy such images might lead to better generalization since all the not-important details
might be removed by the smoothing.


Create a dataset of hands holding things (...) , and try to identify : hand holding X, (X in x1,..xn) or hand
holding nothing.




23/5/2012
---------
Hands: 
classifying the action done by a hand: hand holding X...
most of the time the hands aren't holding anything, but are in a certain pose.
What about hands at rest? This is usually on some surface or an edge of a surface, so the hand is shaped to fit whatever
it is resting upon. 

What shall be the categories? 


Do we need an explicit or implicit 3D model of a hand? 
  

The classification of hands isn't trivial. I am once again forced into the corner of identifying small images.
Action recognition is achieved using tens or hundreds of examples, but what I want to do is identify what is 
the object being held. So I am left with less examples per object type, some critically low.

I should either relax the database (look at hands holding / vs not holding anything)
or increase the number of examples by improving the dataset. 
Use the ideas from "hand detetion from multiple proposals"
 

I can sort of successfully identify when a hand is doing nothing - here's the confusion matrix 
where the first element is a hand holding a bottle and the second is a hand holding nothing
   7    13
   11   59

This was done using NBNN. 

Some concrete todos:
1. learn how to make the felzenszwalb code work.
2. check if instead of using drinking, holding pens will be a better dataset. This is nice since the pen
has a very weak appearance (just a small line) so it cannot emerge as something dominant in the data. 

31/5/2012
ok, I'm training using the felzenszwalb code. Should I keep using entire images? 


fragments 
---------
how should I train the fragments? 
The maliciewicz code learns 1 vs all for each example using HOG. How about I do the same thing using BOW? 
furthermore, do the same thing using hog/bow but for each fragment!!!

So, try to identify each fragment using hog/dpm/bow. This will probably use up a lot of running time, but should
work really really well. 


run BOW with multiscale. If features are extracted at sizes 2,4,6,8, extract also at sizes 4,6,8,10,... etc and
recompute histogram.  choose most confident score. 

will preprocessing an image help or hinder the recognition? 
Smoothing is intuitively like removing the finest later of details. Adaptive smoothing, if it preserves boundaries,
will remove fine texture and let the descriptors capture more shape. 


5/6/2012
--------
Here's a nice idea - use a multi-resolution model for learning classes : two instances of a class 
will have very different BOW representation if viewed at different resolutions... 

So either normalize all windows to a certain horizontal size, or split the learning set to discriminate
between windows of similar size!! This makes sense since it is at a "native resolution"
and we will not suffer from subsampling / upsampling. There is no reason to believe that the represenation
in BOW of a 50x50 cow will be anything similar to that of a 500x500 one.

The first approach is much simpler to plug into the current paradigm.


7/6/2012
-------
Well - there are some points to be careful about when rescaling bounding boxes.

1. high aspect ratio - if we want the width of a bounding box to become 64 after normalization,
this can cause some bounding boxes to become very large. for a example , a width x height = 1x10 bounding box 
will become 64x640 after stretching. Instead of setting the base to a given size, we'll set the area to 64*64 after
stretching each side by a constant. 
Even when doing so, we still remain with the problem of bounding boxes with a high aspect ratio.
I think we should remove them alltogether as they are not informative. 

We should actually remove in advance all those whos aspect ratio is very small or very large.
I did this for smaller than .15 and larger than 6 (which is roughly 1/.15, come to think of it :-))

What about difficult examples? 
Not sure I should use those - they  might be really small. Maybe I should have several aspect ratios
or sizes in the training set. 

Another option is either ignoring boxes whos aspect ratio is too different from that of the training set,
or not resizing them. 

27/6/2012
---------
I found the problem. it had to do with the unbalanced training / testing data. 
Now I am experimenting with vocabularies of different sizes and resizing the training windows. 
For differently sized vocabularies, it is rather surprising that good results can be attained using small vocabularies as well (for a vocabulary of 25, i get .22 ap, when only looking at the aeroplane images). This leads me to thinking that the whole vocabulary issue has been largely overrated. Indeed, using a huge vocabulary (thousands) couple with strong machine-learning can achieve good results but I think the results don't justify the computational cost and resultant size of feature space. I think that using much simpler features would enable me to create smarter feature pooling and combinations.
I visualized the visual-words map for a vocabulary of size 25 and it sort of made sense... I think that using a texton histogram with 8 or 16 directions can work fine if we use a good spatial pooling method. 

Regarding resizing the training / testing instances to a constant size, I've tried several variants of this and all of them made the results worse. How can this be? I imagined that bringing all of the boxes to the same scale would result in better generalization properties, since we are effectively working in a more compact feature space, while hopefully preserving the difference between positive and negative results. I also tried just adding the same samples but with different scales to the training set while keeping the test set the same... This also had a diverse effect on the results. I shall have to come back 
to this later.
What really bugs me is the resizing of windows... what I did so far was resize the windows so that the *area* becomes
as close as possible to 128 (or another number) pixels. This retains the relative size of examples , for a window aXb and
2aX2b, both are resized to the same size. 

Another (technical) point which might hurt the results is that I first resized cropped windows and then extract descriptors...
Doing it differently, I can first resize the image, extract the descriptors, and then crop out only the descriptors than I need. I can also directly compute the descriptors at the required scales by setting the size of the vl_phow parameter...
Excellent - I just found the "bounds" parameters now which can control the image rectangle inside which descriptors are computed.


now what I wish to do, is find for each object its easy to find parts. Instead of directly trying to find whole object, 
I can find easy parts and assemble them into the object of interest...
  


8/7/2012
--------
In the task of multi-class classification, as in pascal, the precision-recall curve is computed for each 
class independently. This means that each classifier will give an image different scores, regardless of the scores given by the others. Can't this cause unnecessary false alarms? For example, if an image is given a high-score by a plane classifier, why should the sheep classifier consider it? in other words, it is better
to use arg-max for each classification, or just output the classifier scores for each object class? 

For a real world system, I think the arg-max is more reasonable. For two reasons. One is computational - there is little sense in running every classifier on an image (or part of an image). If we want to find out the class of an object, it makes more sense to "search" the classification space to do it quickly. 
The other is "operational" - an object cannot usually be assigned more than one class (in the same hierarchy level). Once a class is assigned, the appropriate action regarding the object should be chosen. 

------
Regarding the scale of examples for the "fragments" experiment - 
I have tried several re-scaling themes to the basic windows extracted by the selective 
search code. All of them downsize some of the windows (the ones that aren't small already).
None of them seemed to improve the result. This is strage because I'd expect the learning algorithm to have better performance on images from more similar scales... 

Can it be that the features are so sensitive to image scaling that a simple re-scaling "ruins" them so they can no longer serve as discriminative features? 
Or is it that I have rescaled the images without re-computing the dictionary for the BOW model, so now it makes total gibberish? I find it rather hard to believe that any of those options are the correct answer. 

16/07/2012
----------
I had a long conversation with Shimon, in which we talked about where to go from here; 
I am more inclined to work on object recognition whereas he pushes more to the direction
of action recognition. In addition, I think that the MIRCS (Minimal ReCognizable fragments) are a good direction for research. So the conclusion is that I will continue
with action recognition, but using mircs; 
Some points about mircs:
- Minimal means a small computational cost & low dimension for the classification task
- Once a mirc is identified (with some confidence), I can use internal interpretation to verify the identity of the mirc, or tell something else about it, i.e, a sub-minimal
fragment (to be dubbed SMIRC or MIRACLE :-) ). 

There are three stages to mircs (some can be independent)
1. Unsupervised learning of mircs: what are the mircs? What's their scale, location?
How many are there? 
2. mirc detection and verfication
3. smircs (internal mirc interpretation)

Follows is a plan for the near future
1. For the DPM code, try to reduce the size of the images so the large edge is
n pixels. See how the results are affected and continue reducing n until the results deteriorate sharply. 
2. For a chosen resolution r from stage 1 (multiple r's can be chosen)
2.a.  Try to find sub-regions in each identified object instance which can be used for
verification (for example using decision forests). You can extract local binary patterns for the verfication (or sifts, whatever...). Learn how to tell correct examplers from incorrect ones using these sub-regions. 
2.b Annotate sub-regions in the hand-images in which object (such as pen, cup, cellphone) can be seen. Apply the felzenszwalb detector to these objects over all testing instances. Check the precision /recall curve. See how the curve improves if
you combine the score of this object with the score of the classifier (regression).
See how the curve improves if you constrain the search to the correct region. 

[Note - this test might be biased since the DPM already implicitly learns the appearance of the e.g., pen as the training set contains images of hands writing with pens. However, I believe (can I verify it?) that the details are too fined related to the entire image for the DPM model to learn it. One way to assert this is to learning
a one-part model (i.e, HOG) which will not allow the model to focus on the pen in the image.] 

After constraining the search to a local image region, if no pen is found with a good confidence, this means that maybe this isn't really a hand holding a pen. An interesting question is on the order of things - should the search be done sequentially or simultaneously...

18/7/2012
---------
*****Learning to learn***
----------------
In the exemplar SVM framework, each object instance is pitted against millions of negatives and a weight vector is learned for a linear classifier. 

One would expect, that having learned a good classifier for many exemplars, given a new object instance to learn, the amount of work that needs to be done will decrease. I believe this to be so since objects are not only a collection of gradients in different directions and locations, but it has some structure as well. 

So given a new exemplar, I would like to find the locations in it that are important... 



testing the power of a classifier
--------------------------------
how can one test how powerful can a classifier of a given class be?
for example, we apply a linear svm to learn the weights for HOG features. But the classifier still makes
mistakes. Is this because the feature itself is not informative enough, or is there a limit to the accuracy that can be achieved by a linear weighting of this feature vector?

22/7/2012
---------
Over the past week, I have made a quick implementation of the paper : Unsupervised discovery of discriminate mid-level patches, by Singh, Gupta & Efros. The paper aims to create a collection of mid-level patches which are both common and discriminative, via a discriminative clustering framework. 

I believe that this framework can lend itself nicely to the automatic discovery of mircs. 

31/7/2012
---------
Now I have a set of patches which were learned using (almost) the framework of Singh, Gupta & Efros. 
I want to find out which of them are really discriminative with respect to the given input class.
One option is counting the relative probability that each patch will appear in the class-images vs. the non class images.

Note that when learning the patch detectors, I didn't check if they are actually present or not in the no-class images, not even by cross-correlation as was suggested by the paper. But maybe this is ok; I can check for each classifier how much confusion I get when running it on both groups of images.
Classifiers with high confusion are either chosen on patches which are hard to classify, or are just not really representative of discriminative patches.


8/7/2012
--------
I have tried to test the discriminative power of the discovered patches. 
I fired each classifer on each image in the test set and collected the top results,
including their scores. So I can calculate an average precision given the detection scores and the ground-truth labels : a detection is considered a true positive if it fall in the image of the desired category, and a false positive otherwise. 

Some things to note about this:
1. Not sure that average precision is a good measure - If I am looking for a very distinctive feature, an overall good average precision might not indicate this, as a constant precision of .5 is not as good as a precision of 1 up to .2 recall and then a precision of 0. E.g, I want to **put more weight on the high-precision area**.
2. It is interesting to note that many of the detectors are for body parts - elbows, faces, shoulders...
3. I noticed that most detectors have a very very low AP! Indeed, body parts are detected, but nothing was made to make these detector discriminate against other classes. Singh makes sure that when mining for hard negatives, none are added which have a high ncc ratio. But this is only to make the classifier for the discovered patches more accurate - there is no guarantee that the patches will not appear in the "natural world" images.
So, for some reason, the detectors were trained using images which also contain similar patches and the resulting classifier remain good - most patches belonging to the same cluster seem to represent the same body part. 
Maybe this is due to the fact that although similar patches exist in the natural world set, they are relatively rare enough. This is doubtful, since I used as the natural world other images of actions, which contain many of the same visual themes, e.g, humans in various poses. 

Interestingly, the low-ap detectors seem to capture more diverse themes
and themes that do not relate to body parts, or lower scale features (zoom-out).

See the ~/code/mircs/ directory for a visualization of the top detections for the detectors with the best AP.
 
The fact remains that the parts are simply not discriminative enough. I need to search for the parts in a different manner.

I need to find the parts that appear in image set A with much higher probability than in B. 

Ok... then search for each feature of A the nn in each image of B! (see pages).

Let a be an image in A. For each feature Fa in a, find the nn in all features in image b in B. Do this for all images in B. Now you have a distribution over the minimal distance. Seek a whos minimal distance to B is maximal (or mean, or median...). 
Do this for all images in A. you have a set of discriminative features. Can you cluster these features? Check if the features are common enough in A. 

Learn a classifier for each such found feature. Make sure it can be detected in A. Make sure it has a low false alarm rate in B (it should have an even better (lower) false alarm rate than before since you learned in disriminatively).

Do this while splitting A and B into training and validation sets to make sure you
don't lock onto random features. :-)

----- By the way, why not achieve the same result with Singh et. al ?
1. As mentioned above, the learning isn't really discriminative...
2. Random patches are sampled to initialize the k-means. Although there are many clusters (N/4) to begin with, there is a small chance that the most class-informative features will become a cluster. Common features are not informative... So maybe the initialization with k-means is the main factor here.
  
3. Small clusters are thrown away. Is this a good thing to do? 

--------------------
 
13/8/2012
---------
====>What is a good *exemplar*?<=====
---------------------------------------
Recent work has tried to find some structure in the classifiers found by SVM 
on HOG features (what makes a good detectors). 
But now I would like to find from all image windows, those windows which would potentially make good exemplars, or in other words, will yield accurate results if I try to train a classifier for them. 

Currently, I'm trying to find image windows which, according to a constant metric (L2) are rare in the negative class images. But maybe it is easier to learn weights for a window which is slightly less rare in the L2 norm , so overall the detector for this window better discriminates between the positive and negative classes....

Another thought - I am trying to localize windows which are rare in the negative class. This is done in a scanning window fashion, considering all windows in the image. But doesn't it also mean that there are *lots* of redundant checks?
***
I can take windows from a regular grid with lets say, half a window apart, or from a single scale! If I take windows from a relatively small scale, I don't have so many positive windows, on the other hand, I scan many negative ones!!
***

On the other hand, why not try to learn an exemplar SVM simply for each of not-so-many windows in the training images, and see which one comes up with the best average precision? That way, I don't need to search for the best window in L2, I do it straight in *SVM* space. :-) 
I can remove the windows which continuously fail when mining for hard negatives, 
and then prune away all but those which seem easy to discriminate :-) 

15/8/2012
---------
ok, it's sort of hard to find features which are both common enough in the class images and rare in the non-class images. The best discriminating featurs with respect to the non-class images are also rather far away from the majority of class images :-(. There remains to do some experimentation on this (such as the number of hog cells).

Now I shall go back to efros' method, to first find some element which are both common and easily detectable. Once I find these elements, I will again use the MDF procedure to find MDF's around them...

19/8/2012
---------
It is interesting to note that the new state-of-the-art face +landmark detector fails to detect faces of people drinking due to occlusion. At first I thought that I did not run the face detector But then I checked also the web api of face.com, and it also fails quite remarkably on those examples. It seems that occluding the area of the mouth is a simple way to confuse a good face detector.
 
 20/8/2012
---------
We have many different weak detectors. How to boost them to get a strong one?
This is exactly  boosting and I can check it later.

Observe the clusters obtained by efros' method. The top ones are mainly face parts. Why is this so ? 
1. Faces are very common in the dataset.
2. Faces at certain poses indicate drinking slightly better than one in other poses.
3. 


*I think that it is rather hard to find discriminative patches right from the start since the search space is rather large.

--> using the first phase mdf detections, obtain discriminative patches which
come from the same location in the image as well.

04/09/12
--------
Secondary clustering improves the results significantly. For example, the "efros" detector achieves .11 AP on the test set while re-ranking the top 150 detections using secondary clustering for the results of that detector improves to ~.145 and using clustering by xy (weight==1) improves to ~.16 !!

Now I checked the 5th cluster, for which the original AP is .07, now the AP (reranking top 150) is .1, a 145% improvement!
for the top 500 we get 13, a ~200% improvement


Now I can do two things: 
1. check for smaller patches as the clustering using xy improves consistency.
2. search only around the cluster centroid location... 


10/9/12
-------
MIRCS don't have to be common - just have good precision at low recall. 
If the mircs are complementary, then we are done, as finding mircs with high confidence assures us that the class of interest is found.

Now I need to check if mircs are complementary. This can be done by making sure that the top ranking clusters (those with high precision at low recall) cover well the set of ground truth images.

If so, I can improve precision further by using the sub-mircs, only calculated where there is a good confidence for a specific mirc. This can be done recursively - wherever there is not enough confidence for a fragment of level n, apply the detector for a fragment of level n+1. 

13/9/12
-------
I have tried to improve the precision of mircs by collecting detections of a given cluster and creating clusters from sub-images of those detections. The resulting detectors can indeed boost performance significantly, even beyond state of the art.

The experiment was as follows:
1. Find clusters with good performance on the training set
2. Use the positive detections of those clusters to extract sub-images. cluster the sub-images and learn a detector for each cluster. (phase 2 detectors)
3. Apply the phase 1 detectors on a validation set. 
4. Apply all sub-detectors on the detections from phase 3.
5. Rerank the top 150 or so according to the phase 2 detectors.

I can also take the highest score for each image using the k-top ranked phase 2 detectors (according to AP over the phase 1 detection validation set). 

Some phase2 detectors significantly improved performance while other did not. This is because nothing was done to make them discriminative w.r.t the class. Using efros' method might be problematic since there aren't many phase1 detections and splitting them into two sets will result in too few examples** 

[ Actually, I can create more diverse examples, by uniting the top detections of several phase 1 detectors. This is because many phase 1 detectors seem to lock on to similar things, as the clustering process does not force them to be different. So I can in fact either create meta-cluster (out of some phase 1 detectors) or just take the union of all of them and work on that). This will show that I don't trust the phase 1 clustering very much. On the other hand, it will still result in a situation where the clustering process has less diverse images to work on, and the sub-clusters will hopefully latch on to those features which are really more discriminative. ]

The problem is predicting which phase 2 detectors are good ones. I tried doing this by some validation process:
1. use P1D on a validation set. On that, run P2D. Rank P2D according to AP.
2. Run P1D followed by the reranked P2D on a test set.

This results in unfortunate gibberish - the ranking using step 1 doesn't promise a good ranking in stage 2 and in fact it hinders the results.

-Another point is the clustering process -when sampling the images, I want to sample densly. However, this results in clusters which are made from overlapping patches in a single image. But If I don't do so, it is left to chance to sample patches that will be similar enough in hog space. 
Maybe I can sample densly, but change the k-means so that no two patches are allowed from the same image, encouraging cluster diversity.  

I can reach reasonable results when clustering using 8x8 features for both phases. I will try to resize further the images to capture finer details.

inflation factor of 1.5 with 192 seems to give good clusters.


24/9/2012
---------
I have had it with the k-means step. The results seem reasonable, but I believe the random sampling step is a bad initialization, since there is not a good chance that the meaningful patches will be sampled at the appropriate scale and translation. This will prevent them from clustering together, which will hinder the entire process. 

Another problem is that k-means allows two patches from the same image to be in the same cluster. This is essentially ok, except when the two patches are partially overlapping and then the clustering becomes meaningless. So Efros removes overlapping patches in his sampling, but this only makes the problem worse, as there is even less diversity in the clustering process. 


Instead, I'd like to densly sample all images and find tight clusters in appearance space. 
So I can do the following:
For each training image:
1. Sample all patches at all resolutions
2. Find k-nn neighbors from all patches in all other class images.
3. Train a classifier for each k-sized cluster and find how well it does in classification.

As sampling all patches seems rather unrealistic, one improvement is
using each image as a seed image. For this image, I can sample many patches at a single scale only and then scan the other images in multiple scales for their neighbors - this will save cluster redundancy... ***Wrong!***... Patch A sampled at S1 is not like Patch A' sampled as S2 as it either contains is or is contained, and so the neighboring patches will be different. 
[ I can, however, suggest that if a low resolution patche's nearest neighbor is known, then so are the "correct" neighbors of its high-resolution sub-patches] 

 Anyway, collect the k-nn of each of the sampled patches and think of it as a cluster. What can be done to reduce the number of clusters? Arguably, some clusters will be almost the same. I have to think about this point.

An important point is that I don't really need all windows to begin with, as some will obviously not be so discriminative. I only want those with good potential to become discriminative, so I will choose those which fit the MDF criterion.
The problem with these features the last time over was that they were not so clusterable in themselves, and so clustering the MDF windows and then training classifiers was not good. 

This time, I can use the MDF patches as seeds for the clusters, and find exhaustively for each MDF window it's nearest ones in the class images.Thus I can initialize better clusters. 

Another alternative is that, given the efros initial clusters, I can first refine each of them by finding the k-nns of the initial cluster's members exhaustively within the class image set.

***Check the flipping*** 

---> Maybe it doesn't really matter if the phase-1 detections are consistent, as long as they generally have the same appearace. I can learn MDF's from within them in order to find which are the discriminating patches (cascade...)

27/9/2012
---------
Here's an outline of the current algorithm to find good clusters:
1. use the MDF criterion to score each each patch in the class images.
2. Choose the top-k scoring patches in each class image (say 10) and avoid overlaps.These are the MDFs.
3. Scan all class images for the k nearest neighbors of the MDF patches. Now you have cluster candidates. 
4. Reorder the elements in each clusters according to quantization error. 
---> The quantization error should probably be a stage in the preliminary sorting stage, since at stage 4, there is already only one sample from each image; So if the correct sample was missed, it means it will not be recovered. 
  

28/9/2012
--------
1. MDF features *should* do at least as good as efros' (how can randomally chosen clusters be better than deterministically chosen ones?). This probably has to do with the number of sampled images.
2. Run the MDF discovery on the entire training set, not half, and remember to use flipping. This should create better features. 
3. Also run the Efros process on the entire training set.
4. For both results (2,3), find sub-mdfs , for the top 20 or so clusters (those that have good AP). 
5. See improvement on testing set - > think how to combine several MDF's detections  

----
Think on how to use mdf's directly, as in a knn framework. Each image patch will get its grade based on the distance to the MDF's of the class. The grade can also depend on the distance to the MDF's counter-feature, e.g, the one that is closest to it in the negative class.


29/9/2012
---------
1. ***Ok, mdf with cross-validation by efros works better than anything else.
But instead of finding nearest neighbor at stage 0 in discovery set, train SVMS using only the first example as the first iteration, and then use these as a detection step. This should improve result.***

This improvement can also be made to Efros' process: sample 150 patches from each image. for each patch, learn an exemplar SVM. then cluster them according to the SVM weights, or just use each one separately.

2. Do MDF mining on the phase1-mdf results. Also, describe the phase-1 detections using **attributes** (forsyth, 2009) as additional features :-)

3. Also also, use the object-bank detectors on these small patches.? 


30/9/2012
--------
MDF results in 17% AP. 
MDF with preliminary SVM before clustering / finding nn results in 19% AP.

4/10/2012
---------
It is interesting to note that when testing with inflating factor / image size which is 1.5 of the original (inflate factor in training was 1 and image size was 128), results improve significantly. 
Why is this so?
1. The detections of phase-1 don't contain the mircs - enlarging the area slightly makes sure it's there. 
* This is correct. From now on, the area sought is 1.1 times larger than the area learned from. (for example, 140pix / 128pix). 

This really improves things - getting .24 ap for now (but on half-set).


I would like to filter out incorrect phase-1 detections. Maybe I can do this using internal cluster consistency: find sub-patches which are consistent within phase-1 detections. Patches which do not comply with this consistency may be rotten eggs. How should this consistency be computed?    

----> The smircs can be smaller. I want them to be centered around the mouth area, showing a cup or something else concealing the mouth. The eyes being there are not indicative. 

5/10/2012
---------
Given a phase1 detector (P1D) and its corresponding set of phase2 detectors (P2D) I checked which of P2D best improved the precision of P1D via re-ranking, on a validation set. Call it D. I then tried D to re-rank the detection of P1D on the test set. The results weren't good: it was actually the worst re-ranking out of the possible 8. Additionally, none of the re-rankings worked well.So there are quite a few issues here:
1. Choosing the best P2D by validation doesn't help as it should. 
2. The fine details that need to be captured to discriminate between drinking/non drinking aren't totally captured, or are two various. 

* the best P2D is the 6'th row of 
/home/amirro/code/mircs/tmp/secondary _discxy_128_11_mdf_t1.jpg

* The classifier still isn't perfect. This can be seen in /home/amirro/code/mircs/tmp/comment1/. 
Note the first image after re-ranking. Locally, the round piece of machinery (?) slightly resembles the round edge of the glass near a mouth. 

* the test set, the best P2D was the 8'th row of/home/amirro/code/mircs/tmp/secondary _discxy_128_11_mdf_t1.jpg


* Note  /home/amirro/code/mircs/tmp/comment2/.
Here are the re-ranked images according to the best P2D on the validation set. 
While this P2D finds (rather) well the poselet-like image parts that it was trained on, as most detected parts by the P1D in this case are not characterized as drinking by this specific pose, it fails in re-ranking them correctly.
In fact it would seem that the detections are two various, each capturing a slighty different pose or variation of drinking. This is also why several P2D should be combined cleverly, and only the most confident results from each P2D considered.   


* I will now run an experiment to simply "dump" all of the MDFs in a single collection and see if the SVM can generalize well enough. This will definitely solve the problem...
But maybe a more clever way to do so is use a decision tree, or a random forest, which will be a natural way to combine the different features...(or boosting?)

--> I can perform the logistic regression only on outputs on the P2D !!
This is better, since the local patches used for decision are much more constrained; although my ground truth is still only labels, in this case a "true" detection is more reliable.  

The problem with calibrating the results of the SVM's is that I am working in an unsupervised setting, so I can really tell which are the correct detections and which are false.
This is especially true for P1D, as the unsupervised detection on parts can be a mixture of different visual concepts.
For P2D, The top-detections of P1D belong to a similar visual concept, so the parts detected within them using P2D probably relate to the same visual concept, to I think the process will be more reliable.


----
the center of the P1D to be already discriminative, so it is in a sense an MDF.

14/10/2012
----------
Now I want to find the fine-grained differences between the drinking and none-drinking elements. 

For one think, HOG may not be the best descriptor as it captures shape rather roughly. I can change this by sampling angles more finely (as vedaldi's code allows), but this will probably remove the ability to generalize. Another option is to extract attributes, which can hopefully capture more semantic aspects of the objects in the image, which is "has mouth" or "metallic", "round", etc. 

For now I am still using HOG since it makes the implementation easier. I did find out, however, that using a nearest-neighbor approach works better than training an SVM for this purpose, maybe because there aren't many examples to work with.

The results seem logical - the most discriminative region seems to be the one around the mouth. I will try to further reduce the size of the region to see if I can focus on the mouth automatically. 
Adding more negative neighbors helps.Allowing some scale change also helps.

  
 
16/10/2012
----------
What I want to do now is find two kinds of mircs. Absolute Mircs (ABM) and Action Mircs (ACM). AMB are usually contained within ACM images.So if I have a set of candidates for ACM I can condition further searching for clues that this is indeed an action mirc on the detection of an ABM. 

I want to maximize the probability than an image contains an action mirc.

It is interesting to follow the work of Yao et. al which use a random forest in order to select discriminative regions in an image. However, they choose region according to a fixed location, where an svm trained on the features inside this region from all training images produce results which best split the data. 

Instead, I would like to choose regions by appearance (and also possibly slightly using the location).
Another difference is that I don't think it's necessarily correct to maximize at each stage the information gain criteria. This is because searching directly for a discriminating feature may result in various false alarms. Searching for such a feature conditioned on a feature which is not necessarily discriminative but may narrow down the search can be a wiser thing to do. For example, for the drinking class, searching directly for glasses/wine bottles/straws will be successful if the classifier is up to it, but rather problematic if many false alarms are found in the image. On the other hand, searching first for a non-informative feature (on its own), such as a head, can help us narrow down the search very much...But if the head is more reliably found by the detector, won't this have a better information gain on it's own?    

17/10/2012
----------
The paper on Multiple Component Analysis by Piotr Dollar seems interesting. 

I would like to try a framework similar to the random forest, where the decision on the class is made by a series of detectors.
For example, for the drinking class, you either need to detect a face followed by a glass near the mouth, or detect a glass and find a face near it, etc. 

[Maybe I need a decision *graph* rather than a decision tree - the classification can go both ways and I need to find out the directions of inference. The key is to detect the salient / easy things first and then conditioned on them find other elements. But this needs to be formulated - for example - a detection is a walk on the graph; more like a state machine --> A markov decision process].

18/10/2012
I wrote the pseudocode of the algorithm in LyX (decision_tree in Google Drive).
There are also several important problems posed there. But I think the most interesting one is how to learn a new classifier / detector for each tree node. 

I can take on one of two strategies: 
1. Pre-learn a large set of classifiers/detectors and then apply the selected ones to the images in each node. This will save a lot of time, but it might cause the classifiers to be less tuned to the specific nodes' image. 
2. Learn a set of classifiers per node.
For the second option, I can ask how to do this efficiently. I can also not to try to make it efficient.

Anyway, here's a slight change over the current patch-sampling paradigm. 
1. Sample overlapping patches (say maximal overlap of .5) in each scale. 
I don't even need to sample so many scales, say increment of sqrt(2) as is traditional. I can even argue later that a single scale is sufficient.
2. Search for the nearest neighbor in each of the positive images and train classifiers for the obtained clusters. Efros style cross validation may be applied.

This has two advantages - The first step is for initial seeds. The seeds don't have to be very dense, since I believe that translating slightly the location of a seed window will most times also cause the same translation in the nearest neighbor. So sampling highly overlapping patches will cause some redundancy. The second advantage is that the search for the nearest neighbor is done densly, whereas in Efros' method it is done only on the pre-sampled image patches. 

** Note that I can already at this stage make the nearest neighbor scheme better, if I use e.g, LDA, as Malik has recently done. This will cause the distance function to be less naive that using regular L2 **


29/10/2012
----------
I sort of implmeneted a decision tree (phew...). Now there is the question of the choice of parameters/features:

How many trees
how to choose a classifier/detector in each node, specifically
	should each node begin a search for a new classifier, or 
	should all classifier be from the same pool, only tested each time
Need a way to decide for each node if it's location is explicitly conditioned on that of it's parent
Need a way to make the classifier in a node different from that of the parent - 
how is an identity of a classifier defined? 
In regular trees, a subset of the features is chosen. 
Even in a tree with an SVM, choosing a region in each node is like zeroing out the rest of the image.
But I'm using a detector. 
One solution is to use a detector only at the top level and afterwards use regions which are at a constant location relative to the above detection.

I can also alternate between regions allowed to move (detectors) and those in a constant location (classifiers) 

This will actually be detemined by a switching variable. [need to design]

   
6/11/12
------
1. Need to make the clustering3 not crash (break into several processes)
2. run the flandmark on all images...
3. to make conditional tree learning feasible, learn only on top-scoring detectors 
4. try to use object-bank as mircs?
5. get the flickr 6000 images...
6. use face detection to reliably find faces in all images.

----------------------------------------
2/12/12 (see diary for the entries between 6/11 and today)
----------
I've tried to refine the results of the action mircs by looking at some internal structures such as bottles,
cups, cans, etc. adjaced to the face. 
This doesn't really improve classification results, as most of the time either the internal detector have very poor
performance or the correct detections of such objects were already highly scored by the original detector. 

It seems the original detector discovers something of a poselet of a specific drinking action; the high scoring false alarms are usually faces with the same pose, which are not drinking. 
I should detect faces with higher confidence, and remove all the false alarms. 

I can try to see how far I can get if I remove all the non-face detections manually.

** I am sort of stuck. Need to do things in a more ordered fashion.
Several things to do:
I should make sure that the detection of a head is done correctly. If it is not, then using eyes as hints can hinder
the detection. Why?
It seems I have high-scoring eye detections also on images where there is not head. I wanted to use the relative location of the eye to the head, to improve this, but it only gave me some false eye detections. 
Maybe I should not "trust" eye detections which are too low to begin with.

1. create an eye detector
2. create a mouth detector
3. see how to combine with each of the top-5 detectors from previous stage!!
(you probably already did this...)
see what those detectors "miss"; these are probably head poses which are not similar at all to those learned.
  
4/12/2012
---------
The head+eye+mouth+cup detector improves results;
however, still there is a large gap between what I would expect in performance, for example, for the eye detector 
and mouth detector, especially since I am constraining the detections to specific locations. Actually, constraining the location only hinders the results. This can be due to different factors:
1. a technical problem
2. maybe the training itself should focus only on this area

(remember: maybe some of the 15 detectors weren't so good; try using only a subset of them...)

I would like to make a finer distinction of, given a head image, is the mouth drinking or not, i.e,
is there a bottle, cup, can, etc. interacting with the mouth.

I can either:
1. train a HOG based detector as I did so far with more examples
2. train a detector with finer features (allowing for less individual movements of independent cells)
---> maybe using an analysis of the lines detected in the image. You can even use the edge image
as input to the HOG computation...
3. train a classifier over different features, such as a combination of parallel lines, corners, etc. 

I should also check how well a job I am doing for this specific pose; it could be that this pose only 
occupies 25% percent of the set of poses, in which case improving the results will only be a matter of capturing
more poses. 

----
Make an upper head detector, which detects the eye area.
Learn a regression from the location and scale where eyes are detected to the area of the mouth.

% try to learn only an eyes (perhaps rectangular, not square) detector

** last try - you can learn the area of the head with more cells...


* you can make some virtual samples for the svm by shifting around, slightly scaling, etc the exemplar. 
You can also use some correct activations. 

* another nice idea for unsupervised clustering:
1. start with a single exemplar E. 
train a detector D(E).
find the top detections. Mark them as d1....dk.
for each of d1...dk, train a detector D(di).
check how many of d1...dk are discovered by D(di).
You can also check if E is detected by D(di).
Note - check how likely it is for E to be the nearest neighbor of di. If it already is, maybe this isn't a strong
way to check. 


**Ok - the reason that constraining the location of a detector hinders he results is probably because the constraint
should be pose specific. Indeed, I calculated the statistics from such a pose for which it is correct, but the detections
of this pose include also head for which the eye is not in the expected location. So good eye detections's score are attenuated, while the detections of false eyes in images that do not contain them are kept. So I should actually keep in mind the score of the original detection, or re-score the original detection by that of the eye...
Anyway, I should move on to assume that I have a good detection of the head at a specific pose, and based on that make the fine-grained distinction between drinking and non-drinking. 

I should mark all the cups, bottles, straws, etc in this set and try to train a classifier for them. 

But first, check the pose...

% interesting, shifting the images by half a cell seems to improve results...

% I have a bug which is that nearly all the detection results (except those in the gt) are highly negative...
why is this?
the most negative ones are slightly under -1, so that's probably ok.... it seems the SVM "thinks" that there 
is not a single positive detection example in the entire training set. 

** Todo - try to train an exemplar svm using maliciewicz's code and see if you get the same results...

try multiple-instance training * according to the paper "On recognizing actions in still images via multiple features".
This didn't work well, but I used a rather "constrained" version of the algorithm. For each image, I found the nearest neighbors to each of 18 "concept" HOG windows, which are the union of HOG features which served as positive examples 
for each of the top 5 "face+drink" detectors. 

Make sure face detection works well enough, for images in the dataset, and disregard non-face images, using a certain threshold. This is important, as I don't want to apply face-related tests on non-face images.


* The mouth,eye,cup detectors should not be about detecting those locations: The location is well-defined given a good
enough head detection. Rather, make a classifier (regression?) to tell something about this area which indicates if it
is drinking or not. test for fine properties, such as something protruding out of mouth, shape of cup, etc.

I found some mouths detected in a wrong place; this means that there were no detections in the mouth area...
When trying to classify mouth vs non-mouth with a single hog classifier, I get slightly better than chance.

--- A tip from guy, I can try manually specifying location of the mouth (or head, or eyes) and use that to
make the location for classification more specific. 

19/12/2012
----------
I've run the opencv face detector on all the images. I ran both the side-face and frontal-face detectors. The results are not so good. 
But I can make a better face detector based on them.
How?
-choose a few and train
-randomally sample a few and train (chance to obtain mostly faces is not bad)
- can do clustering
- can scan around detections to get better alignment
- can make a good eye-detector (or other landmarks) for verification
.... 

23/12/2012
----------
Given a good face detector, I can isolate the area of the mouth and try to classify it. It seems that a simple bag of words model,
(with spatial pyramid kernel) doesn't really do the job. 
Training a non-linear svm on the HOG features of that area does a better job, but still not so good.
Also, it seems that many of my problems arise from the fact that the detected area isn't a face at all, so focusing on the "mouth" area loses its meaning;

Another try - I can break up into several different detectors, which relate to cups, bottles, straws (de ja vu).

I can iteratively refine the face detectors on the training set.

* an idea: 
* learn (parametrically or non parametrically) the set of valid deformations of an object class. For example, a face may open the mouth, close eyes, etc.

I am clustering the positive face detections of drinking. Then I will train a classifier on each meaningful cluster.
This can help but still doesn't solve the fine-grained problem of looking at the mouth alone.

I want to segment the mouth area from the object which is interacting with it. To that end, I need a model to distinguish between face pixels and 
pixels of objects incident on the face. I can try to find features which are unexpected given a face. 
Given a face, model its color variance. Find pixels which are inside the area of the face (mouth area) but do not conform with this model. 
Or find the edge of the face and find where the colors along the edge don't conform with the model. 



25/12/2012
----------
I think I understand the problem better now. It is a problem of fine-grained categorization, since one has to distinguish if the object interacting with (or occluding) the mouth is a cup, bottle, cigarette, toothbrush, bubble-wand, etc. 
There is significant recent work on fine-grained categorization, and most of it focuses on looking at specific areas in the image 
and then matching them to different templates, representing the differences between the target categories. 
The templates in themselves seem to be rather rigid (although some may be rather small). Ranging from bird-categories, to human attributes, to dog-breeds,
all categories are treated as parts of the objects : When checking if a man is wearing glasses, HOG features are extracted for head poselets (L. bouradev), and those are classified as wearing glasses or not. For different bird types, many small templates are checked, but the color or lines on a head / wing are still *part* of it, with a constrained geometric relationship. 

The case for action recognition is more complicated, since it would not suffice to use a rigid template anymore, for several reasons:
1. The interacting objects are small and with high variability
2. The interaction can assume a range of appearances 



-----
27/12/2012
---------
0. find a good way to get long objects in the image
an idea - fit a curve to each segment!
[interesting, mean-shift can produce non-continuous regions!]

1. use a lip/mouth detector, trained from own data.
2. find elongated lines/ objects from lips. Verify if straws or not.

30/12/2012
----------
The variability in trying to find even straws is rather large. I have implemented some simple rules to find the straws coming out of mouths in images;
these are elongated objects starting at the expected location of the lips and continuing down at a set of angles.

I'm checking eccentricity,  

Here I face problems which are:
1. not always finding correctly the mouth region
2. having false-alarms on elongated objects (or just artefacts of the segmentation process)
3. misses due to hands holding the straws at the mouth area, thus mostly occluding them
4. as always, false face detections (still a problem!)

So I can still provide a further measure of supervision, which is marking the location of the mouth at each image, assuming that a later stage at the system will provide me with this location.
It is becoming apparent that for understanding the action mirc, the original detection (the independent mirc) has to be first well understood and interpreted, e.g, for finding out if a person is drinking or not, we have to detect the face and hypothesize with good certainty what is the area of the mouth. If this isn't done reliably, subsequent tests will be meaningless.

** ok, I trained a mouth detector based on 5 drinking lips (lips + straw in them). Negatives are non-person images.
At least some of these detectors rank the drinking images from the test set very well: running only on faces of drinking people,
the drinking lips are detected with excellent precision. 
Now I'm running on all (>5000) images (many of which contain the face detections). I shall then once again check the properties of lines, etc,
but this time I can make sure they start at the lip area. 

I keep asking myself- won't a well-localized detector be able to make this disctinction as well ? 
The work of Yao. et al on fine-grained categorization using randomization & discrimination looks at small areas and classifies them.
But this won't work, 
1.since the images are not promised to be aligned 
2. since the areas are chosen randomally and the chance to hit a small but meaningfull area isn't good
3. since too-simple features are used? Not sure.

Ok, finished running. Indeed, the lip detector seems to work well enough, even without constraining the location to the bottom part
of the images. I can move on to check the properties. 


current tasks:
---------------
I. straw detection:
1. Run straw detector on lip images. 
2. Find false alarms.
3. Explain false detections.

4. cup detection / bottle detection - find some more rules and apply them as well.

II. Face detection: current face detection isn't really good enough (there are still false alarms...).
Can try several things:
1. Tune threshold so there are not many face false alarms. This requires roughly marking each face detection to be true or false.
2. Ideas on how to improve face detection:
	2.1 Use external dataset (the one you can now download) to learn your own face detector.
	2.2 Learn the face detector as you already did (with k-means clusters of face detections) but with DPM.
	2.3 apply the same detector you already have but with more resolution levels. Apparently this has a good effect on detection rates.
	2.3.1 It is interesting to think if one can apply a coarse to fine approach or another search method so not all resolutions have to be 			scanned for the detection.
	2.4 Use the verification process you've been thinking about - find sub-patches which are supposed to be detected with high confidence in 			the faces and use them as verifiers. This is similar to the DPM approach
	2.5 for any method, first apply congealing on the training set and only afterwards learn the classifier. This should essentially improve 			the results (on the other hand, it could cause a problem, because maybe the positive samples will contain less variability).




I want to make an MRF - total parsing of the face image:
each pixel can be explained by belonging to face, interacting (cigar, straw, glass), or other (background, hand).

***
7/1/2013
A good lip detector, given a face, should find the lips in the correct location.
If it doesn't then there's no lips (with high probability).
If they are in the correct location, check the appearance. If a large portion of the lip sub-image does not comply with the color model of the 
rest of the image, this is probably drinking...
*note that the lips are almost never found for a drinking with cup
**check how well localized is the peak of the lip-detection. It should be rather localized for true lips and much more smeared for false lips.
!!! continue along this line tomorrow !!!


I am exploring the test for drinking with cup. The idea is that the cup occludes the face region at the lips, so there should be a blob with a color 
difference from the color model fitted to the face.
In addition, I think that convexity is a good cue for occlusion (the convex blob is the occluder). 

10/1/2013
---------
The interpretation of mircs should be sequential. First, Mircs are identified in the image. This stage enables interpretation of sub-parts
of the mirc. Assume for now that using sub-parts for further veficiation of the mirc has already been done, and that we are left
with mircs of very high confidence. Based on this, we can proceed to find object interactions. For example, a person with a straw in his mouth.
Of special interest are the cases where the interactor (interacting object) is not identifiable on its own. So it's relative position to the 
parent mirc and the change in mutual appearance are cues for its presence. 
Modeling the position of parts given a parent detection is what is done in DPM. But the appearance of the sub-part is assumed to be rather rigid, 
such as is the case of a wheel of a car. For a straw in a mouth, translation relative to the parent part is only one of many allowable transformations. 
First, it is "anchored" in the mouth (it should start or end there). Second, it's angle can change. For some cases,
allowing the detector to search for rotations as well can solve this. For others, the mutual appearance of the parent an child can change 
based on the interaction parameters. For example, a cup may be unoccluded or almost fully occluded, based on the hand holding it. 
A cup detector will sometimes fail due to occlusions, unless the scoring function will know to ignore those occlussions, or if the detector is trained 
on many hands holding cups. But hands holding things can assume a combinatorial number of appearances, due to different poses and many possible objects. 
A more reasonable approach is a two staged one, which reasons that there is a hand holding something and then uses the subtle hints facilitated by this 
understanding to identify the held object. 

For face images, I need to find the area of the mouth, and tell if this looks like a mouth or not.

* interestingly, for non-drinking images, there seems always to be a contour of the bottom-left side of the face (which would be occluded if there is a cup,
bottle,other things).

1. Crop the expected mouth area from each image.
2. Run edge detection.
3. Check if there is a long arc at the lower / left part of the image.
4. If there is, reject (this probably cannot be a  mouth drinking with cup...)

* use can use the greg-project for face detection.

23/1/2013
--------
I've been trying to find if a person is drinking by analyzing if the face is occluded. 
This is done by detecting skin areas and then checking if anything disrupts the normal shape of the lower part 
of the face.
One way I did this was checking for convex definciecies in the face area. 
Another was trying to fit a curve to the lower part of the face and then finding which lines intersect this curve,
with an angle which is near-perpendicular. 
The methods can fail if I dont find a good skin color model, or as usual if the face isn't a skin. Also,
fitting the curve doesn't always converge to the correct face area. 
Look at assorted_faces.jpg. Who is drinking in these images?
Note some images are not people at all; Say we can prune them out.
Now we detect faces. I first notice the eyes. They seem easy to detect. Then I look at the lips. If something big occludes the face, it is obvious to me. Otherwise, I check if something is interacting with the lips. 
It is very clear to me if the lips are visible or not. Occlusion means the face's expected continuity stops expectedly - 
can look at pixel profiles going along the face and their expected behaviour!!


24/1/2013
----------
*Important to do cross-validation on SVM parameters, (now you have a small grid-search script).
*The output by libsvm is with reverse sign; maybe the experiments until now were bad?
*The current idea is to find face->mouth->drinking. 
If mouth isn't find (or found in wrong place) then probably drinking, unless this isn't a face at all. So combine face, mouth scores in svm. Also, if mouth is found, need to check if drinking or not. If not, there should be a good fit to lip curves (which can be another verification stage, perhaps by a kernel-svm). If the mouth is verified, check for elongated segments. 
Can also go the other way around, long segments should end with a good scoring mouth. Run eye-detector as well, simultaneously with mouth detector, to determine if a mouth is present and it's location. Note that the probabilistic model should include the fact that there isn't a mouth.
Look into "Layout consistent random fields".

25/1/2013
---------
* Can I use cross-validation on exemplar-svm? One positive example, many negatives. I can split the negative set, but not the positive one...

28/1/2013
---------
It would seem that the facial landmark detector of deva ramanan works well after all, at least if I use the right model and the right
resolution. 
Now I have many drinking mouths vs. non drinking mouths, and I can use this information to create a classifier. 
Note, I can also use the information in ground-truth labels to first find a mouth with and interaction, i.e, smoking, blowing bubbles, etc.

I should also run the deva-ramanan detector on all images, to detect side-views of drinking people.  

31/1/2013
---------
Given constrained images of face detections, the ramanan landmark detector does well.
I have thus extracted mouth areas from drinking vs non drinking people, and now I have a pretty good way to find the mouths which 
are interacting: train a HOG classifier over only this well-localized mouth area. This in itself doesn't bring extremely good 
results for drinking, since the high-scoring results also include smoking, blowing bubbles, brushing teeth. But it's a good start.

Now I should find the fine-grained differences between those. sometimes the differences are in a larger area (one not visible using the cropped mouth image alone). 

5/2/2013
--------
I now have many examples of images tightly cropped around the mouth area, which can serve to distinguish different actions.
Obviously a classifier based on the HOG descriptor isn't able to do this. I've also tried extracting features from segments in each image and using a multiple instance learning framework to train a classifier. This doesn't work very well at all. 
I think that the process of understanding such small images must first parse them into expected and unexpected parts, and try to explain the unexpected parts both by their appearance and the mode of interaction with the expected ones. For example, in drinking, separate the mouth from the straw and then interpret the (unexpected) straw as straw/cigarete, other. 


17/2/2013
---------
For the greg project - you can check for each image, the relative grade of the best detection w.r.t to the other detections in that image.
Also, what is the location of the detection of the sub-part with respect to a normalized coordinate system defined by the detection of the main patch. To obtain several detections, decide on some threshold. For example, let the threshold be related to the grade of detections in the positive set. 


18/2/2013
----------
Things to do:
1. In your EM implementation, use the log sum exp trick to avoid overflow.
2. State more clearly, perhaps more concisely your thoughts in the proposal.
3. You can expand Yao's model to behave as a *detector* tree, rather than a classifier tree. 
4. re-Write the MDF from Bayesian consideratios; a weakly supervised labelling problem where there is data X, mostly mixed,
but some samples have p(x|y==1)>>p(x|y==0) . 

24/2/2013
----------
What to do next?
I have some reasonalbe results for detecting straws out of mouths. The results include non-straws and also occasional non-objects.
One thing to do is to remove the non-obects. The other is to reason about the appearance of the non-straws vs straws, although this isn't always discernable. 

Another option is to move on to cups, bottles, etc, which will surely improve the overall performance...
I shall try this briefly for now.

26/2/2013
---------
Do some experiments to show that Bag-of-Words / Deformable parts model isn't good enough to model the interaction.

27/2/2013
---------
Try as I might, the bag-of-words model simply doesn't cut it. 
Also, hand-crafting features for glasses is rather frustrating and didn't work so well. I don't think it's the right thing to do, either.
I believe that there should be some segmenation stage first : the face is identified and then the rest of the parts, i.e the occluders.

I am returning to a slightly old idea of trying to find what occludes the face. If there is a significant occluder, then I can analyze its appearance. This time I am using the object proposals framework to segment the face image, hoping that the resulting segmentation will be 
more meaningful that the one returned by the low-level one. 

28/2/2013
---------
Alon Faktor has suggested a good idea, which is to check how well I am doing without the "confusers", which are smoking, brushing, etc.
This can allow me to see if I am in the right direction, by temporarily ignoring the confusters. It's probably also better to first
find an occluded face and then try reasoning about the reason of occlusion.

1. Check how well you are doing if you remove the confusers from the dataset. 
2. Apparently there has been a mixup in the face detection indexing (m_test_t, etc). I am running on math-03 again the facial landmark detectors.
I hope this improves the results.
3. There is still a large amout of misdetected faces. There are a few possibilities on how to fix this:
	3.1 Run all 4 versions of the face detector (version 1..4) to detect a maximal amount of faces. Do this by first checking for a subset of the images,
	where the faces are missed. This can also be done for higher resolution and more scales between octaves, just to make sure.
	Then return the highest scoring face from each image (where it's scored by the facial landmark detector). 
	3.2 Run the full face detector by Ramanan ("full_independent") on the entire dataset. This is better to do with the cluster.
	3.3 Try rotating the face images slightly before running the facial landmark detector (in-plane rotation).
4. Occclusion - how can an occlusion be detected? See if you can use low-level features to train a classifier to detect occlusion.
5. Train again your own face detector. 


***... It seems that when I remove the confusers, the results can improve significantly. In fact, the main problem is the misdetection of most of the
faces involved in the action...which looks like something that should be solved rather easily.***


4/3/2013
--------
I would like to learn automatically from the dataset the nature of the interactions. 
An interaction can be identified as a location in the image where two or more objects are 
combined. Their combination results in the action. For example, cups + mouth/face results in drinking. 
On one hand, this causes partial occlusions. On one hand, with only drinking images, 
how can I tell if there is an occlusion, or even if I am seeing two distinct objects? On the other,
it is important to know where one object ends and the other begins, so I can explain each object and their
interaction. 
Luckily, the occlusion and interaction is not the same for each class. I can use unoccluded faces from non-class images
to detect the presence of an occlusion in class images. I want to learn the different components which make up the image, both in their occluded form and 
unoccluded (if available) and find a way to model the combination.

drinking faces are found, how can I improve results? 
On another note, I'm re-running the face detector, this time with a better resolution, to obtain more faces. Running it at multiple resolutions
improves results.
But constraining the detection to the area of the real face (by cropping the image) really improves the landmark localization.

... continue writing the proposal.

	
Maybe I can for a time focus on some other actions just to make sure that I am not overfitting drinking.
	
I am trying to scan the entire images with some action mircs.This didn't work too well, probably since the classifier wasn't trained
(or used) at the right scale or cell size.

Need to make sure that the face detector works well enough.
1. Take the top k face detections by ramanan. 
2. Cluster (hopefully this will find the different views)
3. Train classifiers (can incorporate skin color as well).
4. Run on dataset and detect faces.
5. Run ramanan landmark detector. 

7/3/2013
--------

Baselines:
1.Write a script to train a deformable part-model for a selection of image paths + bounding boxes.
2.Write a script to run bag-of-words stuff on any set of images and labels.
3.Make the "Combining Randomization & Discrimination" code work and apply it to the stanford-40 dataset.

3.Define the current setting for the algorithm : 
For drinking : 
We are given a collection of face detections + landmark estimations (i.e, the lips area). 
The goal is to detect a minimal area of an image which can discriminate well enough between the actions. 
Is this the correct approach? Perhaps, but zooming in on the lip area isn't necessarily the way to do it. I can achieve a so-so performance
by doing so .16-.2 AP. Removing distactors (similar actions such as smoking, etc.) helps, which is encouraging, because I would like my system
to be distracted in the same way humans are. 
So Maybe the selected area is not sufficient. Indeed, showing some more of the image can help in making the final decision (is the object attached to the mouth a cup? 
is it a microphone?)

Generally, I need to identify some of the objects. Usually I can first identify the person in the image. But not necessarily; Can the object be 
identified before the person? I am rather sure that there is usually an *anchor*, that is an object (either person or other) which is identified first.

To what extent do I need to "parse" the identified image location ? There are several approaches here, that I can begin to explore
a. not at all - all that is required is the co-occurence of objects.
b. identify two objects and check their spatial relationship.
c. find how the objects interact: the straw is *in* the mouth, the person is *near* the bike, etc. 

Note that interaction doesn't always have to involve two adjacent objects. For example, when feeding a horse, the hand doesn't have to actually *touch* the horse,
but the spatial relation between the person and the horse is somehow constrained. 

[It seems unreasonable that action recognition can occur without object recognition. Of course, with enough low-level features, both from the objects themselves
and from the context, it may succeed. ]

Back to drinking, I have currently begun to explore the setting where faces are detected; Now I should check which objects are "attached" to the face".
How shall I do this? There are several approaches:
1. Totally low-level features, i.e, BOW, DPM
2. Learn attached objects (unsupervised, objects are unknown)
3. Learn attached objects (supervised, objects are known (cup, bottle)
4. Learn attached objects by functionality/objects attributes: is open, round, translucent, facing up, convex.

--First, construct a dataset of the face images. There are two scenarios:
1. Use only the detected faces, i.e, faces above a cetrain threshold. You can either report results on this subset, or assume that nothing was detected 
in all the other images, so the results will be comparable 
2. Use all images:
	2.1 Where faces are not detected, mark the location of the head.
	2.2 Where faces are not detected, use regression/guess the general location of the head.
	For both cases, see if you can run again the facial landmark detector to improve results.

-- Use either the objectness measure, object proposal, or some other segmentation method to suggest objects of interest. 
You can cluster the objects using several features:
1. HOG
2. BOW
3. Location relative to face or with respect to some face keypoint, including adjacency, occlusion, etc. 
Proceed with the same strategy as "On Recognizing Actions in Still Images via Multiple Features", i.e, the MILES
framework. 

Things to learn here:
1. How is multidimensional regression done? For example, predicting bounding boxes.
2. How are the different mixture models considered in , e.g., Deformable part models?

I direction I can explore is that of Multiple-Instance learning, to detect the relevant objects in the image.
On thing that can be done to improve the MILES algorithm is to train detectors for the features chosen by the SVM. 

** you might have a flipping problem; when clustering positive examples, first create a k-means dictionary. Then fliplr each sample and assign exactly one of the 
flips to a cluster, discard the other one. You can observe the clusters you got later; If you see that there are both a left-facing and a right facing similar clusters,
you can flip entire clusters to match the desired direction.



13/3/2013
---------
Consider the case of brushing teeth.
The toothbrush can appear at many different angles. I don't think that I am "scanning" the different angles in my head when I test if it's a toothbrush. 
I see something that doesn't fit the face and then fit a direction and other visual attributes to it. What is the subtle difference between a toothbrush and a straw? 
There can be  difference in the shape of the mouth (similar to there being a change in grasp for different tools!) 
The appearance of the toothbrush or it's angle can be  unlikely for a straw. 
And sometimes it's not enough - I know that something is there, but I need to examine where it goes to understand better the functionality.

Functionality is another cue for the indentity of an object. 

Interestingly, it seems that using color-sift improves results. Ullman also showed that color is more important in these MIRCS.
Ok, focusing on a really small area further improved the results.(10-->15).
inreasing it slightly (1.5 times each axis) ruined everything. 

Note that using BOW on the entire face image also produces 15%, but seems to return more consistently faces with an interaction.

so .5*global_scores + local_scores achieves ~20.8, similar to state of the art. 

Now I am trying to relax the face detection (which is strange) and train and test on more images

This improvement shows that using enough local cues perhaps I can indeed do better. Still I claim that localizing the occluder and analyzing
it will be the correct thing to do. 


Can occlusion boundaries be so important? Why should they help me? 
1. Because they can aid in the localization of the occluder and finding it's shape.
2. Because they can help me tell if there is any occluder at all.

If I wish to avoid explicitly finding the occlusion boundaries, I can instead tell if the local appearance fits a model. There will
be a need to use several models, and they should probably incorporate color as well. 

A different approach is finding the held object and how it is connected to different body parts.
For example, something connecting hand and mouth will somewhat indicate what the person is doing.
To do that, I have to detect the face, hand, and that an object connects them.

Hard negatives should be taken from class images : anywhere but where the objects appear. 

use hands!!

First - let's improve the face detection. I think it really hinders my results that I don't find all faces.

The following combination of features can lead to state-of-the-art: (23 % vs. 20.7 % by fei fei)
1. at mouth : hog + bow 
2. at face: hog + bow
entire image: hog

train each one separately. output is 5 svm scores for each image. learn linear svm to combine scores. 

How can I (a human) tell between drinking vs. smoking/other activities? 
I know about the objects involved, the typical shape of the lips, pose of hands, type of interaction (occluding lips, inside lips).

It is encouraging that for brushing teeth I also achieve something like stoa. (~40%). 

Some points:
1. Some of the false detections have non-occluded mouths. There's nothing in the image to indicate brushing....
see docs/brushing_results.tif. For instance, if I show only the results obtained by using the mouth area, (see docs/brushing_results_using_mouth.tif ), it gets even worse. I think it occurs because the pattern matched to a horizontal line, which is 
indicative of a toothbrush, can be confused with the line of the mouth. 
I want the model to explain the image.
For example - where is the toothbrush? Where is the cup? 
I can add the likelyhood image as another channel. 

The main confusers have something in the face (toothbrush,cigarette, camera,etc.). 

20/3/2013
----------
I'm marking all faces/head in the images in which they were not detected (test set).
Do this for the train set (drinking images) too, and see how this improves detection results.
When adding only the faces of drinking class to the test set, results are boosted from 23% AP to 38%.  (A later experiment turned this to 32%, perhaps due to
the way I chose the windows). 
When adding all (I added 2000 out of a missing ~3000 so far), the results drop to 15%. This perhaps isn't surprising
since many of the faces I added are very small, rotated, back-facing, etc. Since such faces did not appear in the training set,
the classifier doesn't "know" that they should be false.
Still, what causes such a large boost when adding only the true faces?
I started with a test-set of roughly 2000, 100 of which are true faces. I added ~50 positive faces. How is it possible that performance
was boosted so much?

** simplify the code...(done a bit)

But I should choose if I want to improve face detection or not. Probably not, at this stage. Now I need to improve the decisions based on the face 
as an anchor. 
The local appearance of the face suffices to create results as good as stoa. But the fine-grained distinction between the confusing classes can
be learned by either observing supporting objects or pose. Right now I don't want to find better pose detectors.

I want to analyze the *interaction* : This is telling something about a face, hand, etc.

I can start by learning interactons :-), interaction primitives. This will describe the way that a body part is interacting
with another object. It will include a description of the agents and some attributes.
Example:
For a drinking person
Primitive :
	Face is present
	*Mouth* is occluded
	Occluder is round, transparent, contains liquid
	
or , there is an object in the mouth
or , the mouth is open / closed, purse

other actions:
near eye (telescope, microscope), near ear,

in hand, type of grasp, etc.


* So how do I know if a body part is interacting with another?
--> Discriminate between acting / non-acting body parts. 
Break into subclasses (yes, again! :-) )

02/04/2013
----------
I've added saliency, using R. Margolin / L. Zelnik's recent saliency measure. Currently it's simple; I use the HOG description of the saliency image of the face area;
This improves results. Now I have 25.5% AP. 

I think I can now focus again on my MDF idea, this time driven by saliency... saliency should be computed on both local and global scopes, as results would be different. 
continue this line of thought. 

4/4/2013
--------
What am I doing? 
1. summary of current state
The current AP for drinking is .271, which is significantly more than state of the art. I am using several feature types:
1. bag of words: (1)face area, (2)mouth area, (3)face area with features weighted by saliency
2. HOG : (4)entire image, (5)face area, (6)mouth area, (7)saliency image of face
3. patch : (8)face saliency map, resized to 16x16 , (9) saliency (mean value) of mouth area

training / testing : note that for training, I took the output of SVM on the dataset on which it was trained. This seems like overfitting,
although splitting to train/val and taking the values from the validation set worsens the results. Maybe it is because in the training 
set I don't have enough tranining data. I can also try to make the results more "reasonable" by fitting a logistic regression to each 
of the SVM outputs individually and using those as my features. This can also prevent the need for stretching them afterwards using mean/variance. 

2. improvements/ what to do next, how to make things more efficient
I should compute saliency maps for the entire collection of faces, as well as the features, just once, and keep them. This way I don't have
to muck about with doing it each time.
2.1 how to use current advancements for future research or other topics (e.g, saliency)
I would like to test generally how weighting the features according to the saliency measure improves results. This should be rather simple;
calculate for the VOC images the saliency maps, and calculate BOW representations with and without the saliency weighting. 
See how this affects results. 
3. list of missing things, and specify which things I care about
Face detection is not good enough. Should I really care about this? It's somewhat orthogonal to the main line of research. Suffice with the already discovered faces 
and worry about it later. 
4. What do I really need to do
I keep extracting low-level features from the face area. While this improves results slightly, it doesn't lead to a breakthrough.
Why?
1. Not all faces are detected correctly.
2. There is no explicit model for the appearance of the interaction object or or the form of interaction
3. Only the face area is used.

1 - not for now.
2 - possible solutions: (a) model the appearance of interacting objects
(a)1 : low level appearance
(a)2: functional appearance: a cup has a rim, is round, etc. This is more related to general object detection, i.e, detecting objects by functionality. 
3. focus on additional areas, such as hands.

!!
Find common configurations of body-parts with objects.
(1) find poselets in image. This can correspond to different body parts : torso, head, hand, elbow, ...
(2) using objectness, saliency, MDF, or some other candidate selection method, find clusters of objects which are tight in configuration 
space w.r.t the poselets. For example, cigaretes in hands,...

** it seems the current work on drinking is really narrowed down to finding the difference between drinking and non-drinking objects. 
I think that I need to do "action detection": tell that a person is doing some general action and describe it.
"The person is holding an object", "taking", "putting in mouth"....


18/4/2013
---------
Try to find the active objects, to detect them.

1. Detect cups / bottles w.r.t faces, in train_faces2. Supervised:
	1.1 Train using HOGS (simple to do), with the addition of learning the desired relative location to the head's center. Can also cluster the combination
	of relative location + appearance --> another slight improvement
	1.2 learn in semi-supervised manner : objectness, multiple instance learning, just as in the other paper.
	
	
* The action can be determined by the object and the way it is used:
Say for each image I have the object of interest marked. Then does the task boils down to determining the identity of the object ? 
only if it can determine the action. If there are two actions with the same object, then we have ambiguity. 
The relation between the object and person is also indicative : holding and manner of holding (type of grasp), 
adjacent / interacting with body part : eye, ear, mouth, hand, leg.
I do believe that if the object location / extent is indeed know, I can really improve the quality of classification. 

To show this, start by marking cups vs cigarettes in all the drinking / smoking images. Then add the score of the cup/cigarette classifier to the final classifier.
Ok, I did this, but I have to be careful not to cheat, as I let the classifier know only about the locations of cups/cigars in images where they appeared
near the face. I added this score only to fairly confident detections, since adding to less confident detections would unjustly boost them. It seems 
the results are slightly boosted, but it's hard to tell what really happened here. For the standalone-classification (only cups vs. cigars)
I get %80 accuracy on the test set, which is sort of dissapointing. 

I have a strange bug now with the bag-of-words features - the classifier seems to perform very poorly on this specific task.
Oops, it's a matter of sorting the results. Sorted, there an AP of 89% with hogs. 

I can truly test these ideas only if the area of interest is discovered by the algorithm automatically...

define: 
A = area of interest
A=1 -->correct area of interest
A=0 -->incorrect area of interest
p_a = probability of a specific action a given an area.

p_a = (p_a|A==1)p(A==1) + (p_a|A==0)p(A==0) =(p_a|A==1)p(A==1) + (p_a|A==0)(1-p(A==1))
note: P(A==1) is the probability (score) for an area of interest. 
(p_a|A==0) is a constant, which can be taken to be 1/40 (the proportion of this action in the dataset).
p_a|(A==1) is the action conditioned on a specific area, i.e, appearance based score for an area of interest per a specific action.



Detect action:
- collect images (i.e, mark indices) of people doing something with the head. Learn how to classify these from "clear" faces.
Do this by sampling descriptors around the head and adjacent to it. For example, a grid of overlapping sectors or rectangles,
or simply anoumalous regions. Propose several description schemes for this as it has to be reliable; The idea is to identify
the action object in order to reason about its appearance. 

22/4/2013
---------
I have marked on the faces in the dataset the location of objects for drinking, smoking, brushing teeth and talking on phone. 
Then I extracted HOG, BOW, location feautures and put everything in an SVM. The results are very good, especially with the location.

Shimon says that the false alarms are due to lack of internal interpretation which explains the image, such as the pixels belonging to the object
at hand the type of grasp, location of fingers, shape of mouth, etc. 
How can this be learned?

....

I want to find a way to automatically detect the location of action object and if one is present. 
Now I have made two sets, one of "action" faces and the other of "non action" faces. I want to use e.g., multiple instance 
learning to see what are the features that discriminate between the two faces types. 

1. Extract regions as candidates.
For each image, I have a set of candidate regions r_i_1,...,r_i_n_i. A region has a relative location and appearance.
2. Use multiple instance learning to find the "important" regions, i.e, those which are indicative. 
3. Check back-projection of region improtance on image.

How to extract regions? 
1. A (semi) dense grid
2. Some objectness measure
3. Saliency measure

23/4/2013
---------
1. Can learn from high resolution and apply to lower resolution images.

2. 

The saliency model seems promising, but it doesn't seem to prune out false alarms in non-class images.
It actually worsens the results for now since for non class images, the saliency actualy dominates the score. 

29/4/2013
---------
Ok, now I will create a verification process, which will be made of several attribute components.
1. Is the mouth area occluded or not?
	-If so, is the occluder a cup? 
2. If not, is the mouth occupied by an object? 
	-Which object?
	

-->Learn how to split the mouth area into possibly one of two states: 
interacting / non interacting. 
This will be the simplest explanation : Image = clear mouth or Image  = clear mouth + object. 

--> sample descriptor along the face keypoints to learn between interacting - non interacting. 
Type of descriptions:
1. A local (patch) description for each keypoint, which can be:
	1. BOW
	2. HOG
	3. Color histogram
2. Aggregate data from the peripheral keypoints, make a small buffer around them and describe:
	1. Intervening contours
	2. Unlikely objects given this contour / inner face

Use self-similarity descriptors. 



Face detection: Run face detection with more scales, fuller model.

** segment the faces!!!

find the average face mask, do superpixels, 

23/5/2013
----------
There are two ways to proceed:
1. Make class-specific rules for filtering false positives for specific classes or subclasses.
2. Work on making the algorithm focus on the right areas of the image. 

1: The setting : obtain class vs. non class candidates. I have already partially done this, as I have marked 
the image areas with the action object in the four classes mentioned above. I've shown that these classes can be
separated with relatively good accuracy. How well can I expect the full framework to work, if I assume
that the action / non-action works with a given accuracy? I can simulate this by assuming that the chance
for the "action detector" to be correct it T and then replace the scores of 1-T of the output of the specific
classifiers with 0, essentially making them "miss" the required class. 

When I ran the same classifier on the entire dataset, the results were much worse. This was to be expected,
not only due the larger number of images but also because the classifiers were not trained against any other other remaining classes (including the non-action classes) in the dataset. 
Then I tried using the automatically cropped lip-images alone, which shows relatively good results for drinking, smoking. 
Of course for phoning the results meant nothing as the relevant image area was not even included. The results for brushing
were also less accurate, presumably also due to an incorrect image extent.

--> Focusing on the correct image area can be helpful.
Now, I shall also train the classifiers using a large always-false class, which is the complementary of the selected images.

This step seems to improve the results for both the limited case and the "extended" case (where testing again all images).
Especially the accuracy improves at relatively low recall rates. 

Still, many false alarms seem to stem from poor localization of the correct image area. 

How can I decide if a face is interacting or not? 

I am now training a regression model, which will be learned per-class, to find the expected location
of the action object in that class. Given the expected location, I can proceed to classify based on appearance.
This is done by averaging rectangles (marked in ground truth) by weights inversely proportional to the distance
to the set of keypoints. (kernel regression). 
This can be done for each class independently. I should play with the size of rectangles (extent).
Continue with this on Monday. 
Also, learn how to tell that a mouth is not doing anything


28/5/2013
---------
Action-objects : 
Make a generative face model. Find surprises in faces. Distinguish between face background and face interacting objects.

1. Segmentation - want to know where faces begin and end.
2. Saliency - maybe a general measure of face saliency.

2/6/2013
--------
I am back to analyzing specific mircs of drinking.
Parameters make a large difference:
1. Area where searching for bottle in lips (or similar).
2. Size of window (number of bins) size of cell (sbin). 
3. Minimal Scale of searching
4. flip / don't flip
5. number of hard mining iterations
6. Probably the C parameters in SVM, but I didn't try this yet.

Now I will make the following steps:
1. Learn via DPM
2. Do some "positive mining",  by taking the positive results on training set and re-training.
3. Add features, such as BOW (of different types), Self-sim, etc.
4. Get some more examples from google?


3/6/2013
--------
Visualizing HOG descriptors' weights after SVM classification 
(see here: C:\Users\amirro\Downloads\1-s2.0-S0031320313000277-gr5.jpg ) reveals that irrelevant
features are regarded in the weight vector. Why should the detector care about features totally outside the person?
One explanation is that there is a common context to many of the class objects. But should this context be used? 

10/6/2013
---------
Moving on to other classes : 
Using poselets to estimate the action object. 


Plan generation
-----
1. Learning Linux:
	1.1 how to use the cluster properly
	1.2 efficient file system (search, copy, etc. opertations)
	1.3 how to find if software is installed, gcc, opencv, other 3rd party stuff.
2. General Reading.
3. Planning.
4. Implementation.

Plans: (marked by priority, 1 is highest)
------
1. Do work on low level features to find drinking sub-classes.
	- landmark localization
	- extract mouth area
	- run the straw appearance detector



2. Continue downloading some more examples for sub-classes


2. Clustering (ita). Make the clustering code available, find out the hand images / annotations that you already have.


3. talk with Ofer about Git, common software repository
3. Do MDF between smoking and drinking
3. Continue marking the action objects in the remainder of object-related images, including the test images.

3. A question : let's relax the definition of detection : if a point is detected inside the bounding box of an object, then the object is detected. Let's make a part detector, and 

30/6/2013
---------
currently:
1. using peripersonal.m to narrow down search areas for drinking action
2. trying multipie-independent to try to (again!!) find faces where they weren't found
todo!! do this using SGE

1/7/2013
--------
[A model should include (either explicitly or implicitly) the general pose of the face, landmarks and strict relation between them, such as that the mouth cannot be in the same location of the eye. So, need to find eyes, then mouth somewhere below the eye, depending on the general pose of the face.

Why is the challenge greater than simply having a good pose estimation? ]

Corrected some bugs in train_patch_classifier.
**It seems that many false alarms can be removed by detecting the correct face general pose; this way, the boundary of a side-facing face will not be confused as a strong edge inside it (for example, when looking for straws).
Also, scan within a small area of the candidate region**

The plan : train again the mirc detectors, this time using similar poses for negative examples. 

** Note! C really matters, probably the number of hard-mined negatives too. For small patches, why not just collect all of the "negatives" at once, then there's no need for hard negative mining. Also, train using RBF kernel ?
Write a cross-validation procedure. 

[ 
Plan : Make a set of primitive features:
a. line segments from region A to region B. 
b. regions of certain texture / gabor response
c. mini-hog features, quantized (single cell? ) 
b. lbp features
<-- This reminded me of the work of Sanja Fidler & Ales Leonardis]. 

After having consulted with Guy, he suggested that I first find the mouth and then the straw coming out of it. Seems pretty simple, right?
Well, it probably is - I have just not implemented it correctly before, the sub-parts should be anchored correctly on their parents.
I need to detect the lips reliably, and use this detection to start looking for the straw. Also add the notion that lips should be strictly under the eyes.

I am trying to detect the lips in the training images, but without much success. How can something so simple still seem difficult? 


Ok, talked with ullman a bit. Let's make a sequential process.
Search for a face -> search for lips given face detection - > search for elongated object out of lips, using several different representations or their combinations. 

7/4/2013
--------
tried a bit to learn how to use SGE with python. currently waiting an answer from ofer. 
visualized some gbp results, they seem to capture the straws nicely,now working on loading only parts of the file,
also writing a lite version which contains only the UCM .

ok , I'm running the gpb on parallel using the cluster on all face candidates. 

--> Applying some rules to the blobs discovered by this method produces pretty good results. There are still problems
here and there, which are :
1. other classes of objects (but this could be solved by fine-grained categorization of the appearance of the blobs).
2. incorrect head pose (mostly solved by the pose estimation, but still not perfect). 
3. nothing in mouth at all. Maybe find some measurement of the distinctiveness of the candidate object.

* I will attempt to check cases where I can increase the resolution...?

** work at most 1 hour per day to train your own face detector. You can do it from home, train the felzenszwalb. Train models for several resolutions.

* distinguish between two cases, (i) (coarse) i.e, straw vs. cup, (ii) (fine), between straw / toothbrush

* keep candidate for further processing, i.e, cascade mode. So be "liberal" at first when allowing candidates to pass through


*** ok, in an attempt to detect the lips better I have: 
1. trained a seperate classifier for each gt lip image where the lips are with straws.
2. Each detector is applied separately, the collection of all scoring results are normalized to [0,..,1] per image and detection rectangles are summed per image. Each detector can have more than one detection per image. A probability map is created. 

--> Now it seems that for images with lips, there is a good localization of them and I can use those as now masks for the top-end of the straw.
Also, I can check the max (or mean...) score of the ensemble of detectors to give an overall grade of "lipness" for the specific face. 

8/7/2013
--------
I am now doing two things:
1. Aggregating all of the low-level features of image regions in the test images so I can later anaylze them to find e.g, straws
2. attempting to make the linemod.cpp code work.

--> the linemod can crash dependent on image size, I don't yet know extactly how. trying this now.

9/7/2013
--------
Things seem to be going well with the low-level features for straws. As I told guy, I can formulate things as an MRF where unary factors are confidence in detection of e.g, segments and binary factors are relations (geometrical, appearance) between them. 

11/7/2013
---------
The current results on finding elongated object extending from mouth seem good. What's next? 

instead of extracting my features (or in addition), I will now extract several low level features using the 
code of "recognition using regions", which may also aid in : (1) finding better candidates (2) distinguishing right from wrong candidates.

**and another thing, segment the entire image and find elongated objects that end (start) in faces, this should increase the number of candidates. Maybe this should be the starting point, detect faces , detect long object and see if they intersect in the mouth area. 

I've talked with guy, who challenged me to think about what I want to show, what is the general goal. 

Some false-alarms of the straw areas are lips or other parts. I need a model that includes the detection of lips, straws, and the plausible geometric relation between them, such as that they may not overlap too much but they must touch each other. 

14/7/2013
---------
Todo:
1. Should have a single, good detection of the location of the mouth. This would add both interpretability and confidence that the detected "stick" is a correct one. Either take the argmax of the generated probability map or the landmark localization results to display the outline of the mouth.
2. Check that the long "object" do not continue on the other side of the mouth (they should start in the mouth)
3. Check what is on the other end of the straw : is it a cup? is it a toothbrush? if it's "nothing", then it's a cigarrette...
--> Run GpB on the entire set of images using the cluster and keep the results.

So, should I now start training cup detectors ? 

How should the learning happen automatically?
Learn common appearance of segments appearing at end of discovered segments. There are two types, "transition" segments and object segments. "transition" segment can link between object segments.

Ok, I want to start by applying a classifier and pruning it to endpoints of straws. I've tried doing so for cups but there was quite a failure there, probably because of the large variability. Meaning, there are many false alarms per image, even though I am using several cup sub-classes. The current features are HOG. 
To make a richer model, I will train a BOW classifier. To that end, I am extracting BOW features for a dictionary of 4000 from all s40 images. Then I can learn a rich appearance based classifier. 

When I have features for each and every image, I can more easily apply a learning framework which will more easily incorporate both the appearance (BOW) and shape (GPB). 

21/7/2013
---------
I now have features for all of the images, I can make a classifier for the cup region. This requires a lot of memory, due to the large dimensionality of the feature space and the number of features. Also, the results don't seem to make a lot of sense. 

* make sure that there is no problem with the dimensionality of different windows.
* balance the positive vs negative sets, by either:
1. duplicating the positive
--> Training with this strategy, chi2 kernel and 2000 negatives seems to yield good localization results on test images. 
I will make a run on many images to see how well is the scoring function.

2. using multiple iterations of pegasos, taking only a subset of the negative at each time .

*Continue the segment-following strategy. find the end of the "straw" at each image. point out the location where there is no more evidence of something long starting at the mouth.

* look at segments where they begin in the mouth. find a local direction an continue going in it as long as the segment is thin. 

% why invent a new pipeline? train a deformable part model for cups. But first, do the previous one....^^^^^


% todo: 
1.fix fixLabels.m. 
2. continue searching for longest segment around mouth, in original (f2) images (no more monkey business with the cropped images)
3. (i) go to the end of this segment and anaylze the appearance there, (ii) analyze the appearance of this segment.

% why does the cup detector not work at the end of the straw? even when I do it "correctly"? I shall try *AGAIN* to train a HOG classifier,... maybe even a nearest neighbor one? check it out.
I have trained a cup classifier and I'm running it on all images. A cup should be detected in the vicinity of the mouth / straw...
but then I have shown nothing new at all, even if it does work, unless I make it generic.


**The point is not about the specific features used, but about the sequence of decisions made in each image and how to learn this sequence. 

** to track straws: find a "gabor" like response (perhaps hogs) at each image point and connect detections of such responses to long curves. 

**The region graph should also include regions which have been united by the GPB proccess, these can serve as "shortcut" regions.
--> can look at the shape of the regions, if the local segment is thin, then it is ok to follow it. 


**I should only "trust" top of cup detections if there is indeed a  straw linking them to the mouth. This can be verified later.
*** use dijkstra to find shortest path from the mouth to other objects, this can indeed be done on raw pixels,where the connectivity can
be determined by the relative location (which means the search will be at a given orientation) and of course the globalPB energy at each pixel,
or the UCM.

Learn also toothbrushes, hands, etc. 


* I tried today to train cup detectors. HOG didn't work so well, although I am in the middle of an attempt to improve it by scanning
over multiple orientations.
* In addition, I tried a bit of recognition using regions. This essentially does a hough voting for regions, where the vote is weighted by contour features which are fitted for the region. I should try to train the system by supplying it with my own annotations. I suspect that it should work nicely for cups. 

In addition, I should continue using the hog / bag of features representations, I feel that I have not exhausted them yet. 
A good cue for a cup is that it is being held by a hand. I can learn both color and appearance of hands and use them to prune false cup detections. 

to find long connected edges, use the thresholded ucm image, and find straight line segments in it using kovesi's method. 
The line segment can start far away from the mouth but shoud eventually "hit it".

* Make a region graph. An action is defined by a sequence of regions. There are actor regions (e.g, face), object region (e.g., glass),
and intermediate regions. Intermediate regions form a plausible link between actor and object. For example a straw and hand may link between face and glass. For feeding a horse, some straw should do it. For other objects, there is a null intermediate object.

There may also be no intermediate region, e.g, the actor and object are directly adjacent.

* for the graph traversal:
(1) start at different segments within the face
(2) start at different initial directions
(3) have familiar segments (cup,hand, straw) and unfamiliar segments (for transition)

There are two strategies for finding sequences of segments. One is actual graph traversal, and the other
is finding the group of segments within a certain sector. The group of segments will probably not be very different,
so I will use the directional region-of-interest for now.
Actually, I dont even have to choose a starting segment, just a starting point. The starting segment will be roughly chosen on it's own.
The only thing I must ensure is the continuity of segments along the way. Ways to make sure this happens is (1): constructing the sequence of segments via those that intersect only a straight line. (2) finding segments incident on the region of interest and 
finding a path through them from one point to the other;  I can use the A* algorithm or a shortest path where only moves in a certain direction are allowed. I will start with the first option.

Ok, apply the cup detector to everything...done. apply also the rest of the cup detectors.

* but still have to model the appearance of each link in the chain. 
* also, what are the rules of the chain? It can be a sort of grammar (how to enforce no more than two hands? how to learn grammars?)
M (mouth)
S (straw)
H (hand)
C (cup)
M -> MS | MH | MC
S -> S | SH | SC
H -> HC ...etc. I will have to make sure that this is correct later on. Again, I can learn it automatically from labeled data.
For now, learn the appearance of: 
straws.
hands holding straws. 
hands holding cups, cups, bottles. 


files : sequential_detection.m - the main loop.
	annotateDatabase.m - annotation file, uses labelme.

I would like to check the drinking features near faces alone. To that end, I will return to the old Idea of "interruptions" - 
Look at the segmentation of facial regions:
Collect images of faces and their segmentations, see if you can find a common shape of a face which is not in action vs. in action.

it's in : detect_interaction_Seg.

* local features for the segments:
1. Bag-of-Words
2. Shape of segment boundary (HOG)
3. Shape of segment area (HOG). (can have a combination of the two...)

* features for the decision process - 
relative scale, location, 
shape of shared boundary, is there a share boundary? 
relative color? 

* maybe use the confusion between classes, a straw may be confused with a cigar ? or just train a per-class classifier.

* work in parallel on the MIRC problem.

* start a simplified version of the decision process. Now we have classification results for straw, cup, etc,
for a multitude of images. 
Experiment 1:
For each image:
Find connected chains of face->straw->cup, i.e, score chains of length 3 starting from face according to the region graph. 
May start with regions contained only in the first layer, for simlicity. 
Another option is to retain the original region graph but to "inherit" score from containing regions. 

* todo - also prepare some slides / animations displaying how relevant image regions are contained in the segmentation hierarchy. 
* think about adding patches as well as segments to your collection of candidate regions. For example,
one end of a straw ends with a mouth. The mouth may not necessarily emerge as a segment on it's own. 

* think how to reason about occlusions. If a hand is detected with high confidence, find if the adjacent segment may constitute an occluded cup.

* how do I learn about common sequences of regions for certain actions?
one suggestion: observe sequences of length 1, 2, 3 .


* well you can discard the previous numbers, they were done on the training set. 


* model the shape and length of boundary between different objects. 

* use histograms to model the relative location, area ratio, etc between the objects. 

* Start doing specific models:
1. For the straw - > something, first detect a straw with enough confidence from the mouth area. Then attached to a straw must 
be a cup. Other segment routes are not interesting.
2. For the cup, detect a segment adjacent to the mouth area, which is with a sufficient "cup" confidence. 

* kernelize all calculations...learn a separate classifier (?) for each feature type and then combining 
all of them. Pre-extract the regions to save time. 
* add the shape model to the unary factor
* use the ferrari et al stuff to learn the shapes...
28/8/2013
---------
* I am currently:
1. running the binary features (concatenated, s1, s2) on everything
2. running the united classifier (with the composite feature extractor), no re-scaling of data, on everything
3. learning DPM models for the different parts of drinking
4. I ran objectness, which I can incorporate into the unary features.
5. I ran /home/amirro/code/3rdparty/release-learn-shapes-v1.3/learn_shape_release.
6. I ran vgg mkl demo

todo: 
1. normalize the results of my classification to output reasonably ranged values. 
2. utilize the ferrari code (segments, etc) in order to classify the objects.
3. run the self-similarity code as another descriptor on: 
 3.1 object shapes
 3.2 object pairs (the form of interaction)
 3.3 the objects themselves
 3.4 the interaction between objects, (i.e, the real image parts relating to this). 
 ** you should also extract regular BOW features from these regions, e.g, maybe a 1x1 or 2x2 BOW feature, describing
 the form of interaction.
4. After the DPM model has finished, run it on the entire dataset, perhaps using multiple orientations.
5. using a held-out validation set, learn the parameters of the graphical model : what are the relative weights of the different
features (unary vs binary) for each of the models. 
6. learn how to combine the different features using an MKL framework. 
7. Ok, the learn_shape_release, etc seems to work, now I only have to figure out how to make it work on my data...


29/8:
ok, since you already have feature extraction methods for regions, use the same feature extractors , e.g, sift for
the region interfaces; do this before you start delving into self-similarity and other such features.

from todo (28/8):
1 
2
4 : have a quick look on the results of the dpm. aggregate the results, look at them. (30 minutes);
5
6
7

I've tried the whole system. It works terribly. I want to check if this has to do with the relative features I extracted. Also, 
the DPM seems to perform very poorly, perhaps due to object alignments (many possible rotations for each object type...)


* try something else: 
(1) normalize all regions to 80x80 and then extract the features. + testing, in progress,
this did not seem to have a positive effect, but note the problem with gamma below, maybe try this later.
(2) run *exemplar svm* instead of the DPM, this should account for some of the rotations, etc.
to do this, write a good framework to prepare data for the exemplar svm framework.  + in progress. 
(3) work some more on the binary features: use a set(!) neighborhood of 30x30 pix around the center of the interface between objects
to model it. 
(4) stop for a moment with the multitude of feature types, I think it's holding you back. Stop and think about it; try to stick to a single feature type, but use it extensively for the different things. 
(5) see if the score calibration is ruining things. 
(6) save the list of images for which there are good head detections (the ~1900 for the test set and ~1400 for the train set).
use this data to learn discriminative features only around the head (e.g, straws...)
(7) do some simple image processing. 
(8) train the deformable part model to find faces, using the many faces that you have in your dataset. this should be simple enough. just use non-person pascal images as negatives. This should partially solve the side-face problem...
(9) once again make sure that a straw comes out of the mouth in a very precise manner. If it doesn't, or the mouth is not visible,
there cannot be a straw!


* note, I think gamma==1 (instead of .5) in the homkermap improves results...
* try a smaller dictionary. 

3/9/2013
-------
1. You were just running all the features of the 256 bow dictionary. it finished and you're training a model. 
2. you were about to extend the learnmodels function to train a face detector, using NonPersonIds as negatives and adding an option to specify the class name.

--> 
* reduce the score of segments overlapping with face. 
* learn a binary relationship for segment types 
* continue the face detection scheme
5/9/2013
** make the binary relations include the relative location within the object where it is connected with another.
For example, split the bounding box of the object into 3x3 quadrants and mark the place(s) where it touches the other object. 
You can also model the interface between the two objects with such a grid, which includes a local neighborhood of both of them (this is similar to the log-polar relative feature extractor).
* I have run a preliminary face detector and it seems to perform ok, now I shall use pascal non-person ids as negative images.
** note that the face is often segmented quite accurately, meaning that the cup/straw will be directly adjacent to it. This looks very useful.
** extend this method to other body parts as well, such as hands, legs, etc. 
** I'd like to extend the method to the other parts as well, e.g, use non-person ids for hands, cups, straws, etc. 

** learn via cross validation the weighting parameters for the different elements in followSegments2, including face. also try simple bag of parts model. 
how should cross validation be put in the 
** again, learn parts only in the context of other parts...(where they are adjacent)
* features of relative size, location between body parts
* poselets features?
** make a big script for the entire process. define a smaller validation set perhaps to speed up the training (e.g, 1000 negatives). 
** again, incorporate the spatial relations. 

** make segment labels mutually exclusive in training images
* you were computing the entire bag-of-words thing again

9/9/2013
** todo - you seem to have a blunder in applying the model. Do the following:
1. delete all res_s40
2. make sure that regions are inflated before running the the learning and classification.
3. try to revert to the old learning scheme?
4. learn a "real" face model using the DPM and some external dataset. 
5. run again the training, testing code for the different classes. 

***** Note: to avoid overfitting, you refrained from the final training step in the Pegasos class. Maybe this can also be done by using a smaller number of iterations? 

* try it again with the following possible modifications:
a. toggle filling of regions
b. toggle final training step in Pegasos
c. toggle multiplying the number of positives vs. negatives
d. toggle suppressing overlapping regions...
e. the homkermap
f. different normalizations

*** ok, one counterintuitive thing is that when I concatenate 3 homogeneous kernel maps (instead of making a big one of 3 concatenated histograms) I get more reasonable results. maybe I'm doing something wrong. **YES** - if the histinitUnaryModelsogram is supposed to represent a distribution of different features, it shouldn't be normalized per descriptor type, rather, first count how many times each descriptor type appears, and only then normalize. Can this be fixed after normalization? 

* if a node should be another's neighbor but is instead a subset, can I say it is a neighbor?

***  negatives from pascal images has a positive effect on results, see if can improve results by mining more hard negatives. 
*** make a simple model which sums segment scores in a rectangular window. Do this by creating maximal segment score images, summing and blurring. 

* todo:  learn the common elements by taking segments from the images themselves and clustering them using the chi2 distance or some other measure. This way you don't have to painstakingly define the different elements...
adding even more features does improve things, but I really have to normalize them properly...

* todo : make some nice pictures of this tomorrow to show ullman
*** setting the spatial tiling to 2x2 seems to qualitatively improve results. try also 1x2, 1x1, and only 3x3, also 1024 visual words and different combinations of color descriptors. ---> add this into the cross-validation procedure discussed above. 
why is [1x1 2x2] worse than [1x1] or [2x2] ? try normalizing the histogram only at the end...
**TODO!! in getImageDescriptor.m, I removed the per-sector normalization.

10/9/2013
current todo list:
(1) train a face detector with more training images, and also 1,2,3 spatial pyramids. apply to the entire dataset using grid. Apply detection and use non-maxima suppression.
(2) train just once a new detector, using [1x1 2x2] for 1024 and look a bit at the results.
(3) continue to work on the sequential detection.
11/9/2013
----------
[1] make some results for documentation --> made in ~/notes/images/segment_chains/
[2] use only the top k results (diversify segments,sorted by descending score ) for each type of object. Then you can have binary features for all pairs, not just those which are adjacent. One such feature will be *continuity*, which will be true only if the segments were adjacent in the segments graph. 
[3.1] use a collection of binary features. 
[4] write code to combine multiple feature types. 
[5] write code to train a deformable part model, while taking care to align the examples. 
	(variant) add HOG scores; maybe do this for rectangular windows (e.g, revert to rectangular hog as well? ) or use the above suggestion. 
[6]segments with scanning window: a segment will receive either the score of the highest overlapping window or the average of windows inside it.
[7] If A, A U B are segments in the original graph, is B a segment too ? (for example a hand holding a bottle, only the bottle -- > only the hand? )

* write code to maintain the current set of parameters (e.g, caching), to make result reproducible.
* try using the weizgrid code.
** segments may overlap, this induces a new binary relation of relative location/overlap and containment

[8] maybe unite the cup and bottle categories. 

*** When training relative features, use the following: if A and B are two adjacent objects, use for negative examples the following: A and objects of type ~B, or objects of type ~A and B. This is because using both ~A and ~B can contain a "correct" relation between the two.... Or just use ~A, ~B, that might work as well (not sure of the effect of going to the above trouble).

*-> try using boosting instead of homkermap...ok, that didn't work very well, I guess I expected some magic from boosting. 
*** However, I would like to try Piotr's toolbox to search using sliding window for all sorts of other things, think about this a bit later. 
* use other objects as negatives as well, e.g, hands are negative for cups, and also faces, etc. 
* use negative images taken from s40, not pascal. See if it matters. 
* write a big script to for all these tweaks already. 
* YES! hard negative mining (from the s40 images) was probably what I was missing . 

Review these ideas tomorrow. Specifically, put a focus on the hard negative mining,which means also negatives from known categories, and on the binary features. 
12/9/2013
(now a bit confused, since use s40 negatives was false, I have to check it again)

* relative layout: face cannot be between hand and cup.
hand should be at most at height of cup, not above.
straw must be above cup

cup may either end at straw or coinside with lips area -- > for now try only cup in lips relation.
 
I'd like a cup to overlap with the lips but not with the rest of the face. One option is to find a reliable face segment, this will not overlap with the cup at all (as it is a different segment).
-> Ok, an experiment is just scoring all images according to the best cup- scoring segment intersecting the lips area.
-> another exp. is now obtaining the best scoring face segment (in the face area) and then finding just a segment adjacent to that which is either cup, straw or bottle.the segments look nice, be sure to remove sub-regions of such segments which are not inside the face bounding box. 
-> do the stuff where you learn only to classify adjacent to face segments.
-> smarter pooling forthe objects. 

* a bug with region overlap? 15,99, drinking_128-> done.
* object centric pooling, LLC
* better face detection with piotr dollar's code? maybe also detect many small parts
* align samples, or create more of them for BOW/DPM learning. Doing this for DPM didn't help at all!!. I will do this later for BOW.

* I'll return to straws later, for now, score regions according to their location (hands, cups,...)
* how about : B.P.Yao's work, combining randomization and discrimination... - with a change: Each node in the tree will search an image region for the plausible candidate, and the next region/object to search will be determined by this one. 

*** note: you can use the enumerate function in sliding segments to find groups of adjacent regions for the scoring process, it's really fast.

* Let's break down the situation. 
When do I need to find a straw? 
If the straw is easily detected, then I can use it and find the cup. If it's not easily detected, I say that it is not the main component in detecting the drinking action, for example when it is very short because the cup is adjacent to the mouth. If the cup is not adjacent to the mouth and a straw is weakly visible, then:
1. Maybe it should be found in a verification process (not detected a-priori)
2. If the cup is not very indicative as well, what should I do?
Start by finding with high confidence the easy straws. 


For an image: can this image be a part of the face? if yes, make an interpretation of that part, by matching / building it from
other sub-images. 

k = 1049 , test

29/9/13
I have tried filtering the image in the vicinity of the mouth with gabor-like filters in various directions. As the features concentrate on the mouth area, they become more and more informative for the drinking activity. 
The most important component is finding a clear mouth with a strong response for one of the directions. 
I am also using phase symmetry in order to find symmetric edges, e.g, not step edges. 

1/10/13
Ok, I'm interpolating the local neighborhood of the straw, which create a pretty good canonization of things. Now I shall learn a classifier for straw vs non straw elements. 

Trained a classifier, using small hogs, seems to work ok. However, some false alarms are really similar locally and should not be regarded as negatives; I want to detect all straw-like things coming out of the mouth, and then use another method to tell the difference between them. So first find a way to find these good candidates and only later find the discriminating element. 


The straw may be long, but it is important that it begins in the mouth area. Thus, make sure that the image for the hog is rotated around the center of the lips, so a strong hog response far from the face will not cause a false alarm. 

I am calculating again the responses, this time limiting max-rad to 15 pixels. 
In parallel, since the lines should be long, I will make filters for long lines and continue to check their responses only if they connect to the mouth. 

2/10/13
-------
The straw detector works reasonably well.
Now, find the score of regions which are reasonably within a bounding box at the end of the straw. 
This means, pre-calculate the scores for all regions and then use what you got from the straw detectors.


Follow the end of the straw(the general direction) and see what's at the end. 

***todo: extract a canonized image region from the bottom of the straw. 
** apply a top-of-cup detector (by hogs) at this location. Grade accordingly. 
* take a few minutes to run your framework on the visual phrases dataset you recently downloaded. See how well you perform there...


6/10/13
---------
current todo: 



A : make a top-of-cup detector.
1. extract the top portion of cups (e.g, top 20%). Canonize all to same size, by fixing width and taking a fixed portion of height : This seems a bit tricky because the images are not aligned to begin with. So I will either use a nearest neighbors approach or jitter all the images in the training set to have a better fit. 
2. train a classifier using the voc-negatives dataset.
3. use the bottom-of-straw regions to limit the region for which the bottom of cup is sought. 
4. re-grade the image accordingly. 


7/10/13
I did not succeed so far with the top-of-cup classification scheme. The confusion with e.g, cigarrettes is still really great for some reason. Guy suggested that since I used cups with straws, the straw element was confused with the cigarrette. 
I shall try to use all cups, and the straw element will be ignored.

Also, I shall apply the gabor filter again... 
cd 

8/10/13
I've tried again to detect the straw via the seqmentation cues, this failes because the segmentation often misses the straw.


15/10/13
--------
1. trying to cluster cup images to make a better detector. 
This is done by iteratively jittering images to fit the distribution of the other images. It seems to work nicely, but I'm 
not sure yet that it is helpful. An idea I have here is to do it gradually - iterate using small perturbations until saturation (or a fixed number of iterations). 
Then use a larger perturbation, again, until saturation. This can be done for a series of increasing perurbation parameters. 
2. Training a k-adjacent segments detector (shape model).
3. Obtain images of many faces in order to find the non-face regions in the action images. 

18/10/13
--------
Not so much luck in creating a better cup detector, the cups are too diverse. A more supervised approach is probably needed. Or maybe splitting the cup types into many different ones. Also, the KAS detector also fails; I should limit it to the vicinity and approximate scale of the face, maybe then it would do better.

Another approach of finding convex curves inside the face area seems to bring up good candidates, where false alarms are mainly due to curves belonging to the face itself. I will use the remainder of the keypoints to find face curves that should not become candidates. Also make a distinction, if the lips are fully visible, make sure that there is either a straw or a cup/bottle is very near them. (This is a heuristic I should think about...).

For now, check the locations of the facial landmarks. 

1. Try to train a new face detector to find the profile faces. 
2. 

localization of the mouth region is very important. There are a few subclasses of drinking, where a distinction should be made between occluding / non occluding objects. 

Model the "interface" of the mouth with the different objects. This is the action mirc.

21/10/2013
----------
Run ramanan again, using the following policy:
1. where no face is found, double the size of the image. 
2. use rotations of -30,0,30 degrees.
3. after a face is found with the p_146_small model, double the resolution and set as input the detected bounding box. 

model:
 1: cups
 2: bottles


24/10/2013
----------
1. For each thing learned, apply cross-validation to make sure no overfitting, etc. 
2. Face - Saliency - apply the saliency measure to the face areas and reason about the amount of saliency inside the face,
outside the face, etc. Then apply the shape (HOG, whatever) detector
3. Try to also use the codebook-saliency by alon, maybe add location to the feature vector.
4. Train a deformable-parts-model for the face detector.
5. Check out the pose regression code of piotr dollar. 
6. Check the training framework of piotr dollar; try to make a face detector using these; 

2: experiment_0001 : Learning the saliency map itself seems quite meaningless, probably because there is a need to fit a different coordinate system according to the locations of each face's landmarks. But constraining the saliency to the region of the mouth vs. the rest of the face is a very good cue; Especially when combined with the local contour analysis, it begins to bring up some good results.
4: experiment_0004 did this and seems successful.

To top everything, I would like to add some traditional features to the candidate regions selected by this process. 

use the primal sketch stuff..

28/10/2013
----------
1. I've re-computed the facial landmarks for the entire dataset, using my new face detector. I shall now create a new data structure to summarize this data... Oops, I ran the landmark localization with a too-high treshhold... doing it again.

2. Do some reasoning about the pose of the head. for different poses, consider different locations. 
3. experiment 0006 should be like alon's saliency, or a variant (like 3 in 24/10).

My new face detector yields much improved results. I am now re-calculating the saliecy for those images;
as a consequence, I'll have to repeat some more of the experiments.

30/10/2013
----------
The face detection scheme seems to work nicely, but I am not very happy with the landmark localization itself. 

Now, I have found that using saliency does indeed bring up some nice candidates, but of course it is not enough. 
1. I will create a pose-specific reasoning for drinking. 
status:  doing it. 
2. run again the landmark localization, with enough area around the face to allow capturing the sides of the face.
status: waiting for new face detetors (e.g, 4).

3. make the face detection better!
status : clustering

training the detector for 10 different models. it's going to take a while!!
For some reason, it got stuck, training again. 

4. run ita's hand detection code : running...(killed at night to do nms before saving results). 
Finished running, now consolidating results. Remember to send back ita the results.

5. find a new scheme - define convexity w.r.t face: if face is looking aside, there is one range of possible orientations for the curve, 
if frontal, another range 
31/10
5--> rethink on how to extract contours. maybe use the KAS code for that, seems quick and simple.
Option 1: run KAS on the input faces.
Option 2: find convex contours (start angle, end angle, convexity.). 

--> I can do it, but 1: I don't want to be brittle to small convex deficiencies, and I want to be able to describe many types of shapes --> 
use something like the bag-of-boundaries approach. start with a shape-context based approach, to model the shape of the segments. 

*** I think I'd like to "throw away" the ramanan code, i.e, just use my own face detection, which is probably less brittle.
This will cause the detected faces to be more aligned. I can also keep the detected keypoints but retain my own detector's bounding boxes. 

consolidate the following features:
1. Saliency
2. Pose
3. shape + location of segment (what shape descriptor to use?)
4. Color distribution w.r.t to face color, and w.r.t to the 

Looks like my DPM based face detection is quite well localized. Given a detection of a specific pose, I can tell with reasonable confidence the location of the facial landmarks, since the detection is well aligned with other examples of this detection. 

*Collect all results from the training set and normalize the scores to range from 0 to 1. 
Now define a filter for the outline of a side-cup.

* have a go at finding hands - obtain many hand images and cluster them. train a DPM as in the face experiment.
* add your own code to detect things at multiple orientations. 

* it seems that the nearest neighbor approach for finding pose does quite well. 
for each face, use this to find an estimate of the pose, classify using pose-specific features.

* use the hand detection code skin probability part to find skin / non skin regions.

 * run the ramanan code on the dataset of the 12,000 images from aflw. 



10/11/2013
---------
add an alternative way of finding contours, which is sorting the UCM image by value and taking each individual contour on its own.

Remember the work on straws? Now I can do it more cleverly, since I have a better pose estimate. There should be different rules for finding straws using frontal and side face.

Want to learn pose-specific contours and features. One way to do it is if I had the pose for each face, then I would need to associate each face in the training set to a given pose, and then learn specific feature for it.
I. Assume I have the pose: location of mouth, eyes, nose, contour of face, etc. 
	1. Using the ground truth markings, find cups, bottles, and cluster the results by pose similarity. This will define 		both the desired extent and appearance of the objects. What about objects further away from the mouth? Will have to worry about this a bit later. 
	Clustering by pose (1) - find similarity by proxy; for each face find it's nearest neighbors in the aflw set, and 	group 		faces if their corresponding neighbors are similar.
	Clustering by pose (2) : for each face, get it's facial keypoints by copying them from the aflw corresponding image.
	as some keypoints are not always present,treat this with care. can also copy the roll, pitch ,yaw parameters and 	compare using it, instead of the keypoints themselves. 
	
	describe the curves within the face area of the ground truth segments, using Guy's code. Then you will have a 		descriptor of the curve.
		
II. Obtain the pose for each face. 

I guess that my main worry is dealing with subclasses. One Strategy is to model each sub-class independendly. Subclasses arise by either object or viewpoint differences. There a quite a few subclasses in this class, which make it difficult. 

Detect geometric shapes of curves, by a combination of several primitives. 
One interesting curve type is a U shape, which can be approximated by 3 straight line segments:
1. Break the edges in the image into line segments. Do this first on the sorted-lineseg segments, i.e, line segments obtained by sorting the ucm by edge strength and doing lineseg.
After some geometric reasoning, you should obtain a candidate set of U shapes. But, how to sort them? This is only in order to save time/remove false alarms: Assume you already have the desired set. It shouldn't be too large, since it is constrained to a specific location. For each element, you would need to extract some more features. So instead of using
bottom-up segments, you can instead enumerate over some possible U shapes (the relevant set for each pose) and check the properties of those candidates.

How to define a U shape? 
1. Center of "center" segment and it's orientation (3 parameters). 
2. Angle (symmetric) from center segment to side segments.  (1 parameters). 
3. length of center segment and lengths of side segments. (2 parameters). 

I should also consider a 2-segment object, which can be more fit for corner-line parts, but I won't do that for 
now. 

Overall - 6 parameters.  

For each pose and object I can have a prior for the location and shape of the U-form.  

Another cue to look for is the intervening contour : For the expected lower half of the face contour, only faces which are intersected / "hit" by a line crossing this contour can participate in an action. But for this I really need to model the pose better than currently being done. 

Another thought - the U shape, if not rotated, can be thought of as a simpler shape. For example, an ellipse that defines a region.  Then center-surround statistics can be checked on this ellipse. It actually dosn't matter so much - 
just a matter of convenient parameterization. 
--------------->
 1. make segment adjacency graph. 
 2. find the angle between every couple of segments.
 3. find sequences of segments with same turning direction.
 4. for start and end points, see if the next segement can be "side" segments of the U shape.

11/11/13
-------
Qualitatively, the DPM doesn't seem to do a much better job than DPM re-ranked using the score of the Zhu / Ramanan localization. But there are still places where my landmark localization is better. So I can use my landmark localization on the faces re-ranked by Zhu/Ramanan, or I can just do it for several face candidates per image, and re-rank using e.g, the consistency of the keypoints on nearest neighbors. 

12/11/2013
----------
After speaking with shimon, I have started to implement some simple rules, the first of which is that an object is largely covering the mouth. 
This on it's own is largely successfull and I can further improve it.

Now I turn to side-faces where I want to find the exact point where there is a mouth and there have an indication of 
whether there is something incident on the mouth. The facial landmark localization does a pretty good job. 
I can yet improve it, notably especially the pose estimation, which is important for focusing on such cases. 

I shall now:
1. use my improved pose estimation to prune out results or guide the decision process.
2. try to distinguish between partially occluded / not occluded at all mouths (as it seems that fully occluded mouths are sort of easy to find).

I can probably find all sorts of convex curves in the mouth region, I still need to quantify their shape, etc. 
The edges here will sometimes be strong and sometimes weak. 
The point of incidence on the mouth is important. 

**How to tell that a mouth is occluded? train a mouth classifier! For several poses. For each image, "detect" the mouth
in the expected location with all classifiers. If the highest score (watch out to use pose-tuned classifiers!!!) is low, 
then the mouth is probably occluded!! :-)

** In the past, you've trained a good classifier for side-drinking objects. try to do it again, an "detect" only on side-faces, in the vicinity of the correct mouth location. 

Use a more raw feature than UCM, such as GPB / canny, etc. 
14/11/2013
-----------
Some gross mistakes stem from wrongly enstimated extent of face and mouth regions : 
k = 28,
3304--> bad vertical extent, note this is a side face.
1937: wrong segmentation, due to lighting effects. Note segment extends above eyes (actually touches top of image)
3458: similar; segment touches both sides of image (too wide)
3005 - wrong face mask; note this usually happens in small faces (low resolution). But this is a dataset bias
so I can't use it.
*Also, for this image: ucm is strong, entire face is covered, but due to wrong face mask it is not reflected. 

3003: this is definitely caused because I'm using UCM instead of GPB! 

1802: again, irregular shape, horizontal extent too large, edges probably caused by UCM

2735: too much face area occupied; *not enough out-of-face area occupied*. --> note, the out-of-face-area occupied by the cup should be near the mouth (can't be on other side of face);
Maybe all this can be expressed in terms of a relative occupancy mask, pose specific. 

222: cup region should *almost* never cover eyes. 
3467: if there is a too-high face overlap, I should not care about the UCM.
238: shape of segment; have a shape-descriptor.
809: location of mouth wrongly estimated
2611: mouth totally unoccluded --> should have a mouth occluded indicator
139: totally wrong face mask
2886: not a face
875: indeed drinking, but for wrong reasons :-) ; self-similarity to face is too large;probably because of UCM region.


15/11/2013
-----------
plan:
I using the new dpm-detected landmarks, create the following:
1. estimated face outline : how to do this simply? fit a convex hull to the detected (==transferred) keypoints. 

2. face bounding box
3. estimated mouth bounding box. 
II run again experiment 5 with these parameters. 

note: bwperim returns y,x!! this means you may have a bug using ramanan's code...
check it out...Nah, it was just a bug in visualization


maybe use CPMC or shape sharing for object segmentation, ...do this to estimate the outline of "clean"
face images.

but first, use the provided ellipses to estimate the outline of the face!

The edge of the face shouln't contribute all the weight of the edge of the object intefering with it; there should be an edge
either within the face or out of it too. I've currently addressed this by removing edges coinciding (close enough) to the estimated edge of the face before calculating the edge strength of the object. It seems to help a bit. 
Still, many mistakes stem from incorrect localization of the boundaries of the face. 
The current approach seems to bring up pretty good candidates. I want now to learn how to further improve these candidates' scores.

Before I do that, I'd like to have another go at better face localization. 
It seems that I am able to have a nice face mask, using UCM + the face mask prior. 
I should later consolidate these results to see if they help me when trying to find occlusions.

Now I will try to improve the localization of the mouth by doing a local search given the nearest neighbors
--> This seems to have improved the results.
* Make a summary of the process so far
* compute self-similarity features
* run face detection + GPB/UCM on phrasal recognition dataset --> make a setup file to run the whole process
on an arbitrary directory.

21/11/2013
----------
Fixed a bug relating to the boundary detection unaligned with the face itself.
Now I'll try to imorove a bit on the boundary detection itself. 

Last thing you did: 
ran again the boundary localization with inflated bounding boxes, now the masks are in the global coordinates of the 
(cropped to person) images; this should solve compatability issues. 

22/11/2013
----------
Find the masks using a local segmentation is nice, but it's "trying too hard", since the face boundaries are not estimated correctly when they are most needed, that is, in cases where there is a partial occlusion.
Instead, try to fit a "clean" face outline to that of the observed face, by trasforming an outline from a similar clean face to fit the observed edges, but with a penalty on warping that is very different from the identity. This will allow the model to lock on to visible edges while avoiding edges caused by occlusions, etc. 



structure of face (frontal)
outline: 
[68:-1:61 52:60 16:20 31:-1:27]

right facing:
[6:-1:1 16 25 27 22 28:39 15:-1:12]

24/11/2013
---------- 
After dedicating a week to fitting a better face mask to the detected faces, I am returning to the zhu/ramanan solution for now, as it is mostly satisfactory and I can later deal with its failure cases; For now, I want to see how to better exploit the cases where it succeeds. 

Here are the things I'd like to learn / use.
1. occlusion : how to tell if there's occlusion?
Use the simple logic you've developed so far. 
2. Drinking: for occluding objects, classify 
using extracted per-pose features. 
For a given range of poses, extract features in the occluded area, such as shape of region,
sift features. 
How to describe the shape of the region? 
1. Shape occupancy mask, relative to mouth and face.
2. diameter, area, orientation, etc.

How to learn?
1. Use the face detections above a given threshold so all of them are correct faces.
Call this set F. 
2. For each face in F, find adjacent objects (in the inflated bounding box around the face).
3. Split according to object type.
4. Learn a classifier which extracts features from the part of this object near the face **versus occluders** from negative classes.

*** run the GPB code!!
25/11/2013
-----------
** the face outline is really important!!
** in what cases slightly deflating/inflating/snapping to edges the face outline can help? 

*define a dont care area around the face boundary, the object should strictly cross this boundary. 

Common errors/conditions

(A)ppearance,(B)oundary, (M1)outh too large, (M2)outh incorrectly placed
(N) not a face, (S)hape
Other
(? )Explore further
#Image	Symptoms		Remarks

836	B
889	B,M1
900	object touching mouth but not intersecting
1125	N
139	B
120	B
2335	A, S
3125	B **
74	B **
3577	B
3304	S, A
2040 	?
1683	A (Appearance of other face regions)
2924	M1,M2
1802	B
3803	B


* check size of mouth bbox
** slight inflation of boundary will help


*** use the other body detector to constrain head/face detections. First, use it as is.
Afterwards, can also train one which is pose-based, for example using the H3D database.


27/11/2013
----------
Needed milestones / blocks:

Face Detection (improve)
Face Landmark localization (improve) : can follow a similar path to Ferrari et al, who use a gaussian regression to estimate the face pose.
Detect occlusion : Can use the occlusion dataset by Piotr Dollar to estimate my
occlusion algorithm or improve it. 
Segmentation: Use multiple segmentation algorithms, maybe switch to SEEDS

* run the upper body detector to guide / prune face search results.
* better yet: use the regression of piotr dollar for the pose estimation!

28/11/2013
-----------
I am close - using piotr dollar's code to incorporate all features and use them for the classification. 
Now also use his code to computer the low-level image features for each image.

ok, using the raw features alone definitely seems to add something to the results, now I'll try combining everything...



4/12/2013
---------
I don't have enough positive examples for the training... 24 samples of cups; is this enough? I need stronger features,
must be able to tell for sure that a face part is occluded.

Train features which are anchored on the different keypoints. (which means, have a different feature vectors for different poses?)
The previous step (explicity finding occluders with my specific features) seemed to work well; I should take the top occluders from each image and use only them as candidate inputs for the random forest, instead of mucking about with lots of unnecessary negatives.

More things to do:
1. increase segment pool by adding more segmentation types.
2. run GPB specifically on the face images (again...), enlarged versions, in order to obtain more segments/diverse segmentations. 
3. devise specific features for mouth area (edges in specific directions, etc.) 
4. add more features to the random forest, such as the saliency in the mouth area, in the entire area, etc. 
5. sometime - try to refine the location of the mouth; this is very important. !!, 

6.Work more on the occluding object detection, it is very crucial!
* maybe dilate the mouth region and face near the mouth for purpose of intersection. Can further see if there is an intersection with a slightly larger object outside the scope of the current window. Although not strictly necessary, can remove false alarms. 

7. can use some of the links in the reading list to e.g, improve face detection, etc.

8/12/2013
--------- 
should fix the code for candidate region extraction of positive training samples, right now it discards 
some samples, I want all of them, as long as the score is not too low. 

need to do:
1. make sure ground-truth is good, which means:
Mark all faces (of positive images) in ground-truth. This can be done using piotr dollar's code, as long as it eventually 
translates to the groundTruth dataset. Run the face detection and landmark localization on this. 
--> Also create an annotations directory, to store all future annotations, in an orderly fashion, e.g, which relates to the absolute coordinate system of the original image.
2. Generic features that can be calculated are: hog, bow on the face, person, entire image regions. This will boost the results, but will not be novel. 

3. I've noted that constraining the score of the face to a reasonable one, the lips are found with quite a good accuracy.
Hence, it makes sense to also train a classifier only on such examples. I should also split to roughly side and frontal views, so features will not be mixed. I will now train a classifier to use this data and this data alone, and see how it goes!

4. What to do with the "occlusion" cues? They work reasonably. Either use them as a score on their own to use other scores or rely on the occlusion are for further results. Can stop refining them for now. *May perhaps run an experiment on the COFW dataset to find occluded parts. *

Return to the specific features of experiment 5 (I think):
Given a reliable face detection, we now have a good outline of the lips. If this is a frontal face, extract the following features:

1. Is mouth occluded? 
	Can I provide different levels of confidence for the answer? 
	Can I tell between totally occludded and partially? Think about it.
	Here's one solution - no need to really tell, just assume that there is 
	a good correlation between my occclusion score and the chance of occlusion.
	Apply subsequent steps weighted by this assumption.
2. Overall mouth score
3. Given that the lips are highly occluded, look for the shape of a cup; This is the so called "U" shape,
which is pose dependent, with angle related to the looking direction of the face. 
4. Given only partial occlusion, look for a straw, e.g, two parallel lines...
TODO!!
5. A question (has been asked before) : Given the **isolated** appearance of the occluding object, can I deduce it's functionality?
If this is not so, then I need to find the mode of interaction - exactly where does it meet the lips, etc. 
Otherwise, focus on these objects alone, and see what you can do : crop out the occluders, visualize them, also,
consider doing something similar to the ICCV paper on fine grained categorization by alignment (grabcut->hog->align->regress keypoint location->extract features at keypoints->classify).

--> partial answer to this: apparently not always the correct segment is detected. Following are some observations:

**IMPORTANT** The area where the region goes "outside" the face should be near the mouth. For example, if the mouth is at the right side of the image, I don't care about a region protruding out the left side. This should be generally used: looking left - object of interest is to the left. looking right - obj. of interest to the right. 
*** Incorpotate all the facial landmarks when checking for occlusion, in your manual rule. For example, the eyes shouldn't be covered. (at least not both of them) This will rule out more false segments. 

You should rule out segments totally inside the face area.Another heuristic is to remove segments that span the entire vertical or horizontal axis of the sub-image. They may also (ALMOST) never touch the upper bound of the image, although there are exceptions. They shouldn't touch the upper border of thr face, according to keypoints. 

6. Just a quirk - apply the regression method of piotr dollar to the aflw images' pose estimation. Ok - it seems to work nicely.

Maybe convexify the vertices of the mouth, to avoid quirky looking results.

* Discovering th occluding object is moving along nicely, which means I can now extract features more reliably...
* note that I should separate the occlusion extraction stage from that of the "drinking" detection, since there are many non drinking occluders. Perhaps to that end I should indeed use the COFW dataset...

12/12/2013
----------
Back to the basics: 
Find specialized features. Start with cups : for frontal facing people, find the top-of-ellipse
which delineates the cup.
 23/12/2013
-----------
I am implementing a state machine that sequentially checks for all sorts of geometric conditions, applied to primitives near the mouth. It is starting to look good, but I am wondering how to implement the state machine so rules and primitives (also compound primitives) as easily added. 
I'd like to make the rules and features as modular as possible (maybe object oriented) but the problem is that not all rules can be applied to all primitives; for example, a T junction between a point and a line is not well defined. 
So I can create rules of the form R_i_j(theta) where i,j may be the types of input primitives, and theta is the parameters of the rule; possibly R_i_j_k, k'th rule between primitives i and j. Will this support adding compound primitives? 
 

25/12/2013
----------
Each primitive in the state machine should have many features. Feature are unary, which may also be with respect to the global features, such as location of the mouth, etc, and binary - between features. Perhaps later, the mouth and face can be added as special primitives, which will eliminate the need for unary features which relate other primitives to them. However, this can only be resolved if the mouth and face are everyone's neighbors in the graph...

The desired location of the required primitives may also follow a known / learned distribution, w.r.t other parts or a global coordinate system.
Add the hand detection output of Zisserman, in order to have the "hand" primitive. 

When should I trust bottom up features and when should I use top-down guidance to find them? another thing - I can use ridge / edge attributes as additional appearance features for the found primitives. A straw is more ridge-like. 

Have a relative scale for searching neighboring edges. For example, for higher resolution images the width of the straw can be larger. 

* add the "gap" features

There are many examples where the state machine is "stumped" because a neighboring edge is missing. This can be solved by few ways:
ids of such examples: many of the straw images.
1. looking for non neighboring primitives (predicting where a primitive should be an testing for it).
2. adding more primitives (canny, gpb)
3. using a different shape-matching method .

Look at 842. Here a gap feature should be added (or a feature indicating "fingers").
When to add a gap feauture? Always! where the neighbors of the gap feature are neighbors of continuing in the same "direction" sought by the current feature. using a gap feature incurs a penalty.

% The bottle example (image 812) shows an example where given the initial part-ellipse, I should search for it's support
% at both sides of the ellipse - i.e, detect symmetry with a with in a given range; make an object detector for this object (e.g, bottle) and apply it at the hypothesized orientation; try applying now the DPM bottle model at the desired orientation.
This worked well on a few examples; the bottle was among the top detections when the image was rotated to the correct orientation. Should this imply that I am to find the correct orientation to look at, then apply DPM at the desired point?

just do shape matching for the cups. Try again the ferrari et al for detecting the cup-shapes. 

match rough shape, do detection in segments. use "oriented bag of words", for detecting rotated objects.

So... should I apply a generic shape detector for the cup/bottle/etc or not? 
That *could* work, if I find a way to deal with the unknown orientations, etc. But an important point is, I dont need the entire object! after finding the ellipse, etc, analyze it like a mirc!!


* the shape verification process doesn't have to follow a clockwise sequence on the edges of the cup - just verify the desired properties.
Break the problem into sub-classes. It is geometric in nature. The face is partially occluded by something which can be a container of some sort. 

Try to find common attributes for which a minimal invariance is required: for example, prefer sub-parts which are similar only if not rotated, but look for rotated versions if no such are found. 

26/12/2013
----------
1. Complete the annotation for the drinking class: face bounding boxes, location of mouth, eyes. 

1/1/2014
--------
the moves on the graph should be according to UCM connectivity: node are UCM edges. edges are junctions between nodes, and primitives are those coniciding with nodes well enough. This will sparsify the graph enough. 
A question : what about primitives which are partial to edges? 


6/4/2014
---------

Make a set of *features* between primitives. 

1. turn direction (left, right). Note this is always defined for the closer of two endpoints, and
we also receive a parameterization of which edge was flipped so this would become true.
2. isContinuation (i.e, edgepoints are near and relative angle close to 0 .(modulo 360).
3. isParallel : in "front" of each other, relative angle close to 0.
4. ellipse in front of other ellipse : convex hull of all ellipse points include all points.

7/1/2014
----------
It looks like I'm trying to make my own shape detector. Why not use existing shape detectors?

tomorrow (wednesday)
1. Obtain a dataset for cups.
2. Learn the dataset using e.g, ferrari et al (or the new HOG w/o hard negative mining framework)
3. apply to a subset of the cup images.

another tought : use *image connectedness* to choose the next segment. This can be done in two ways:
1. the shortest distance function you defined to the mouth.
2. the cluster-affinity value of the fast gb code!! :-)
3. some combination of both : while the final goal is the mouth, the affinity between two image locations can
be measured in terms of the cluster affinity.

Define the energy both in terms of connectedness to the desired target region and unary factor (fitness) of the candidate primitive. 

add "gap" features. What is the size of the gap feature?

TODO:
-----
Add to each segment:
1. a unary score which is it's value in the generalized distance transform (e.g, it's gpb score)

2. For each pair of segments, the distance between them should depend on their connectivity on the edge map;
try not to hallucinate . You can do dijkstra between them, for example.

more TODO :
----------
look at previous todos, to make a coherent, new list, and:
1. use the downloaded sun database to train deformable part models and shape-based models for detection of relevant objects.
1.1 maybe use the segmentation transfer code by ferrari et al to segment many images of bottles, cups, etc. 
2. use a hand detector to find hands in your images.
3. model the relations between primitives in a richer way. Do so in order to be able to easily create states and transitions in your state machine.

4. in the state machine, make a priority queue to choose the next best state to develop.

Find a way to minimize the number of neighbors for each segment. 


5. The process: 
	1. find candidate -- > hypothesize --> look for supporting candidate -->hypothesize...

6. process at multiple scales / blurs / threholds / colorspaces the images for the elsd. 
suppresss multiple lines  / ellipses cleverly. 


7. can score candidate primitives by distance (dijkstra in pb) to face : "can reach this point from face easily"
This relates to sample 33

bottles:
12: good, going from the bottom of the bottel to the mouth seems to work. 
trying other examples.
18 : the bottle is detected partially, but it's ok, I think...

maybe make a tip of bottle detector. This isn't always applicable, look at 39, where the bottle is low-res;
This is mostly guided by pose: 
So - decide for each case, if you should rely more on pose or on visual cues...
Generally, low resolution images a probably more pose-dependent...But all high res images can be brough to low res (unless they are close-ups of a specific part, where this wont bring the pose of the body)
67 : no ellipse (although bottle is overall pretty clear), and a hand is detected. 


8. add some more checks as final face overlap of object, objectness, etc. 

13/1/2014
---------
1. I am trying to run the beyond hard-negative mining code. It seems that it's working nicely, but it does need a lot of memory, that it roughly nImages*K where K is the maximal number of non-overlapping HOG windows per image. For thousands of images, this can still accumulate to several  of Giga-bytes of memory. But the good news is that is independent of the template size (I think). Note that we only need to collect the negative data once if we know non of the positive classes appear in it. 

14/1/2014
---------
I have decided to move onto a fragment based method. I will have a set of local detectors for corners of cups, parts of hand, bottoms of cups, etc which together will be stitched to verify that I am viewing the drinking action. 
To that end, I started looking at image-net and sun-dataset for data. 
I want the process to be easy, i.e, enter a set of images, output a set of discriminative fragments with location relative to the center of the object. 
For image net, there is only a small subset of the objects annotated with bounding boxes. I can either use this as a raw input in order to find sets of patches, but then the location is not known so well. Maybe the segmentation transfer of ferarri et al can be of help, or another co-segmentation method... 
Or - I can use the provided bounding boxes. That's what I'll do for now. The idea is to identify reliable parts of the object instead of trying to find it entirely, since then I can miss it altogether, when it is often occluded. 

20/1/2014
---------
Things start to work nicely. **I thing I have a bug in the visualization  or usage of the face box. 

I am correcting the detectors by applying a refinement stage : running each detector on the training data and obtaining again the bounding boxes for the top-scoring detections. 
Using a square detection window, I can apply all detectors at each rotation / scale. 
But learning using square patches introduces many problems, probably because of the edges of the image. I somehow want to make the patch square but not care about the surroundings. I could learn a non-square patch and then pad it so in detection time I use a square patch. This is sort of silly... if it's not good for anything besides runtime, then let's leave it alone. 

Still, maybe I can pad the images from which I learn, in order not to introduce the border effects. Don't pad with zeros, but maybe replicate or something. or add noise outside so there won't be any consistency. 

Now, I am running my refined detectors on the s-40 dataset, and I'll see how it performs. Afterwards I'll take just the top of each bottle / cup in order to learn it alone, so I won't be affected by occlusions .

Another thing - when checking intersection with the face, I should actually be checking intersection with the head, since it seems to be much larger. ** This still seems to be true, I remember having a much nicer outline of the face once... you're probably using the keypoints wrongly now. 


27/1/2014
---------
The drinking vessels object detectors are very weak on their own. In addition, there is seemingly no need to check non-occluding objects as a starting point. Hence, I can start by looking at occluders and classify them. 
At first, define an occluder as a segment which is partially overlapping with the face area. Also may try to make the occlusion-reasoning code of hoiem, etc work. 


So,
have the following blocks ready:
1. outline of face and location of key-landmarks
2. an indication of potential occluding segments
3. a way to score each segment as a detection of an object of interest. 

29/1/2014
---------
Here's a possible pipeline:
1. detect objects only in the area of interest (i.e, near the face)
2. estimate possible occluders. Remove (or give a lower score) to objects which have a low chance of being occluders / foreground. This will be a combiation of high level cues, e.g, a segment partially overlapping a part of an estimated location of another detected object (e.g, a segment overlapping an estimated location of the face)
3. Re-score the occluder and occluded object using the estimated occlusion pattern.

Steps in order to do so:
1. Create a possible pool of occluding segments. 

** A region "penetrates" an area if it enters it sharply and partially. Create an occupancy mask of a segment with respect to a region (using A log-polar mask) to define this. This can also be extended to non-circular regions by finding the circle enclosing a region and warping the segmentation map to a circle. 


2/2/2014
--------
I have defined some basic rules for occlusion. A partially occluding segment S of an object B with expected extent E
maintains the following:
1. A covers part (not all) of E.
2. E covers part (not all) of A.  (1 and 2 mean that the XOR of A,E is nonempty).
3. part of E covered by A is less than half of E.

Given this, I am training classifiers for objects, using only objects which are partially occluding to A. 

I should also try to tell if a general object is partially occluded or not. I can do this by "completing" the shape of an object given the visible part. A first estimate would be that objects are generally convex. 

3/2/2014
--------
Have a main script for doing the following:
1. Detecting occlusions
2. Detecting objects or their parts. 

					*-*-*-*-*-*-*-*-*-*-*-
Back to analyzing very specific cases:
1. discard images with landmarks which were detected incorrectly (i.e, face score is too low)
2. further reduce the area of interest by defining a sector depending on the pose of the face.
3. analyze features of objects docked at the mouth.
4. refineoutline, checkdirectionalrois 

Have all the data readily available.
Crop all images to the region of interest spanning ~2x the area of the head to avoid excessive computations.

Think: how to tell if there is a top of cup, or bottle, or some other object docked at the mouth?
Possible solutions:
1. (Suggested by shimon) : Look at the contours near the mouth region and try to fit a contour with the proper shape, e.g, tip of bottle , or rim of cup, by traversing the different paths in the contour network.
2. Train HOG classifiers for tips of bottles, corners (or full tops) of cups, and see if they fire in this very constrained location, at the correct orientation. 

5/2/2014
--------
The training of discriminative patches for tips of bottles, cups, etc is showing some promise. 
I am currently computing a heat-map summing the detector score over the small face region. It seems to lock on nicely on
the area of the action. Further reducing this using the techniques discussed on 3/2/2014 (previous entry) should improve the results. 
Finally, I can learn a feature vector which has to do with the responses of these local detectors - for example, their sum in the desired area.
Can also try different patch sizes (currently using 5x5), and sampling method. Should see why the sampling of some
classes produces funky results.

TODO:
1. remove old sge-parallel files automatically after job has finished. 
make sure that discriminative patches are detected only within their original bounding boxes...

Add the occluders info to the drinking object detections.
Make sure the cluster you've created are from the tops of objects only.
One way to do this is to augment the features with xy coords while clustering. Another is just to limit the extent to the top part. 

* Start doing a summary things you did so far (not more than 1 hour a week).

9/2/2014
--------
Extract a subset of sub-images containing only the faces with high enough score. Keep the subwindows surrounding the mouth area with meta-data describing, in these coordinates, the face-mask, mouth-mask, etc, as well as the segmentation.

This will allow subsequent calculations to be much easier. You can train the classifiers against windows in this set of images. 

The shape of the occluder can also play a role, if you can extract it with sufficient accuracy. 
again, make the set of negative images relevant to the once you are testing against. 


I want something to point me at the right direction : For example, a rough detector to tell me that there is a chance of a person, looking to the left, drinking something at ~25 degrees. Then I can look for the object of interest with much less degrees of freedom. 

So I will add a (latent?) variable of where the object of interest is. 


10/2/2014
---------
How to proceed:

1. create the augmented imageData datastructure with the cropped area centered at the mouths.
2. crop areas around the mouth for the positive classes as well as the negatives and train classifiers for them,
versus the negative images.

Solve for a given subclass.
Why are there so many sub-classes? Is this a trait of this problem, or of other object classes as well? 


model components:
1. score of face
2. location of mouth
3. chance that mouth / face at this point is occluded
4. score of object at offset relative to mouth
5. distance of location of object from mouth (penalty for offset from mouth)
6. compatability of object detection with estimated face pose (directional ROI)

Learning the model (model parameters)
1. T_face : face score threshold
2. G_mouth : gaussian with respect to location of mouth
3. W_occ : weight of occlusion
4. S_obj : weight given to classifier score of object
5. W_roi: weight of object within region of interest


G_mouth : preset (e.g, gaussian or mask)
T_face: -.6
W_occ : delta (detection must overlap with occluding region)
S_obj : svm score (sum of detector scores?)
W_roi : delta (detection must overlap region of interest)


TODO: 
1.incorporate occlusions (create occluding segment map for each subImage in newImageData)
2.allow detectors to fire only on the range of poses in which they were trained.
3.enrich/enhance training set by either finding best detection of classifiers in the training images, jittering the training images or detecting on the extended drinking dataset.
4. As initial detectors, add the upper body detector, phrasal recognizer, etc. 
5. tabula rasa. 



11/2/2014
---------
TODO : continue with the occlusion reasoning. Try to remove silly false alarms for occlusions. Look at image 1342 in imageData.test, for example ("feeding_a_horse_023.jpg"). Try to remove weirdly- shaped occlusions : an occluder should be locally convex (remember the "U" shape?):

Let's say the region A is deemed to occlude B. Then convexifying the part of A inside B will not add to the part of A inside B a lot of area. 

12/2/2014
---------
The occlusion detection is going well; but I have yet to find a good classifier for the cups, etc. Maybe an NN classifier? 

* Find the desired objects by a set of features:
1. HOG features
2. Shape features
3. bag-of-words style features. What are the words? shape context, self-similarity features, etc. 

13/2/2014
----------
Make sure that the geometric features are compatible for training / testing,

**Break into more sub-cases; do it perhaps not manually but add desired attributes to learn,
such as lips occluded / not occluded, exact occlusion pattern (which means where to look and shape of occlusion),
and then build the classifiers using these features.

Subclasses are identified by:
A. type of object
B. type of occlusion / interaction: 
	1. near face (but not touching)
	2. touching mouth (docked)
	3. occluding mouth
C. pose of person (primarily side and frontal facing)

Also, you can compute features on facial keypoints rather than freely around the face; normalize the face to a constant scale and calculate features on keypoints, or on a grid warped to match the face's keypoints. 

17/2/2014
---------
todo - after the test:
make a plan, with a weekly resolution. consolidate previous "plans" into a new plan and break it up into small parts. 
Specifically, collect manually samples of sub-classes as discussed previously. Do it also for the extended drinking dataset. 
And start thinking about object functionality in terms of attributes (metallic, man-made, long, has opening, can be held, round, shiny, ...)
Can also try discriminative subcategorization instead of k-means... 
So make a plan for each of these and a framework to benchmark the contribution of each. 

12/3/2014
---------
We are now focusing on the case where a face is detected with high confidence, i.e, throw out cases where this is not so. 
We want to first achieve a state where most (near 100%) of the people with objects near the mouth are detected, and the precision is not too low (e.g, 20%).
This will allow us to further delve into details regarding the object near the mouth. 
The first thing to notice is that I can come up with reasonable candidates using e.g, deformable part models on already detected faces. 

We don't expect the bottom-up algorithm (e.g. DPM) to tell the difference between different kind of objects near the mouth, but it should bring up good candidates of people at a certain pose with an object near the mouth. From that point on, I can apply further tests. 
Another possible way to bring up candidates is use the visual phrases model :
1 run the visual phrase of "person drinking" on the dataset. 
2 train a visual phrase of your own.

I should improve, within reasonable bounds, the process to find candidates for the people drinking. Should I train the drinking person detector against non-person images, or against images with people in them? For the first option, the detector might latch onto the "face" part. For the second option, the detection might latch onto the "bottle" part, which is weak on it's own. But that is ok, if I only look for bottles in people of the same pose, where the mouth is already at the same location. 
I can also add / use a bag-of-words model to bring up good candidates.

3 calculate feature-maps for all of the subImages in the >-.6 faces set, in a reproducible way. Be able to quickly use these as features for future usages. (is this important?)

Now, I believe that given a good facial landmark localization, I can use the facial landmarks as cues for interpretation, and for ruling out false candidates. 
4. pick a specific class, e.g, bottles. For this class, find class-specific cues, using everything you can, which are able to verify that this is indeed drinking from a bottle.
How to do this? 

train a classifier for very very local appearance and test it only there : this doesn't quite work out of the box, for various HOG parameterizations.
I am running CPMC to get some more segments to make the occlusion extraction better. Do it also on the cropped faces/upper bodies for better local resolution.


20/3/2014
---------
A bottle region should be (almost) totally contained within one of the sectors outside the face; i.e, it should be within a "thin" sector; 
that is a sector whos difference between start and end angles is not too large. What about large can-line structures near the face? 
the measure angle will be too large if the object is wide. So I can measure the angle to it's furthest point. But it's becoming ambiguous.
I can measure the major / minor axis , make sure the object is pointing towards the mouth with it's major axis: The vector between the center of mouth and the object's 
furthest point coincides with the object's major axis up to some margin. 

It's improving, posing some more constraints on the segments near the head / mouth area removes false alarms.
Some interesting success  /failure cases:
explanation: Seg=segmentation,G = Geometry, A = appearance, F=face detection not accurate
name			success	explanation 
drinking_019		+
drinking_245		+
drinking_028		-	Seg
drinking_101		-	Seg
drinking_161		-	Seg
texting_message_013	-	Seg
brushing_teeth_121	-	G/A
smoking_118		-	G/A
drinking_130		+
writing_on_a_book_179	-	F
drinking_235*		+	
waving_hands_031	-	G/A


*nice examples(s)

how to score the occluders ? 
geometric score : shape (both absolute and shape mask relative to mouth), relative scale, size out of face, 

for the actual calculation of the occlusion, try also finding the best image segment which is covered by the face detector;
assuming that this is the "real" person, non-occluding objects will manifest as non-overlapping segments. 
Try to infer some occlusion cues from appearance, as has been done by other work.
Note that a bottle is only slightly occluding, when viewed from the side. So if we have a side-view, should think of something else...

*How to tell if the face is occluded or not, at all? if the region of the face pops out easily as a low-level segment, then it is not occluded.
Try to use this cue, to remove easy cases which cause you trouble.
The problem with this is that the low level segment may well correspond to some parts of the face, but for example, the neck will be 
in the same connected component as the rest of the face, so...

* another condition: either the occluding object is very close to / on the mouth, or it is **strictly** occluding the face.
Strictly occluding means not only touching, but actually inside the face region.For now, instead of really checking this,
actually check the ratio of the segment within the face : a large enough ratio is required for objects not actually touching the mouth.

Actually, now trying to just make the threshold for distance-from-mouth more strict.

Search for objects near the mouth using another strategy - multiple graph-cuts, seeded by regions outside the mouth. 

will sorting the objects by distance to mouth help? yes , it does. 

The graph-cut approach should have seeds as face occluders(i.e, green bottle's cap) and high probability at the desired region of interest 
(directional roi).

face parts are all wrong... (their numbering). so maybe I misinterpreted it, or I need to change something. 
***Found out : side has 26 points in convex hull, and front has 27 points (BUT YOU SHOULD TELL THE DIFFERENCE MORE RIGOROUSLY) --> ok, did this 
and indeed it improves the results.

For strictly occluded face regions, make sure that there is indeed occlusion : some examples include faces whos chin region is not occluded,
but since the face outline includes the chin but not the neck, the chin+neg segment is mistaken as occluding the chin area. 

Ideas for solution: (see relevant images)
climbing_214.jpg : object should approach face from relevant direction: for a right looking face, object occluding face from below is 
not likely.
waving_hands_058.jpg : object occludes eyes, a large portion of object is within face. If an object largely occludes a face, 
try assuring that a large portion of it is outside as well (not sure about this heuristic);

add again the lip scores, or the scores for "clear" mouths.

Try fitting segments to the face region. Where the segmentation agrees, this cannot be occluded. (see chin remark above).
This will help in some places but does not have to be mandatory. 
For example, in taking_photos_043.jpg, it doesn't help at all - because the GPB segmenation simply doesn't agree with the face 
outline. 

Ok, let's have a look at watching_TV_142.jpg. Here, the 2-expanded regions cover the face area well. So
the face outline agrees with the best covering segment, and any segment crossing this outline cannot be an occluder.
This is interesting but unstable. Instead, try using the score obtained by each local filter, or the agreement between
the orientation of the image patch at each location and that of the contour at that location. 


* want to improve the face detection/landmark localization. Don't spend too much time on this, but here are some options:
A. 
	1. Detect faces with pose-specific detectors (i.e, using piotr dollar's code).
	2. Apply only a specific pose in the landmark localization process.
B. 	1. Given a face detection, determine the yaw using regression and then do A.2
C.	1. Given a face detection, do not allow locations outside the face segment to be assigned keypoints. 


** trust a graph-cut region only if it partially contained by a gpb region. 


** continue extracting the lip images of all sorts of poses (maybe filter to upright faces only?), experiment-33
** occluding segment cannot have too much of a common border with the real segment of the face.

2/4/2014
--------
Detecting bottles from the side seems to work well now, with some exceptions. But a larger boost in detection can now come from finding other sub-categories as well,
such as e.g, straws. First, I shall try to make the old code work again, where I already did some work on straw using gabor filters. 

9/4/2014
--------
Straws: 
1. a straw may not be a part of a large occluding object - i.e, if there is something large occluding the face and the straw is within it, discard the straw.
2. limit the appearance of the lips interacting with the straw : lips should be closed, pressed together.
3. look at the far end of the straw 
4. try finding the extent of the mouth with higher accuracy.
5. look along the straw for color variation. This should enable you to make a distinction between straw, cigarette, etc. 
6. the straw must end with a closing line (I think so)
7. the lips should be visible except where the straw is occluding them
8. if the mouth is too open, then it's probably brushing teeth

23/4/2014 
---------
1. Use bottom-up image support to rule out false positive for the straws
2. get the code from guy for the image support
3. check how UCM and other cues can help remove false positives
4. show some top scoring false positives for the bottle class (see ullman's email)
**5. guys leave-one-out method: If you have a classifier A whos output you want to use as input for B, but a small training set,
break the training into P, N1, N2 . Do leave-one-out training to train A on P and N1, which will produce |P| different score for elements of P, and
apply to N2 (any of the classifiers trained on A,N1). Then use the outputs on P,N2 to train B. 

24/4/2014
---------
Look at some examples for the straw:
smoking_143 : tip of finger creates straw-like shadow. Maybe rule out based on scale w.r.t mouth?

writing_on_a_board_028.jpg : shadow in corner of mouth is falsly detected as straw. 
1. it's in the wrong direction
2. the scale of the mouth is totally off w.r.t the face
3. The straw ends well inside the face with no apparent none-face object (==occluder!!) on the other side!

brushing_teeth_045.jpg
very short, dark ,inside region of mouth (which is not accurately detected). Not the most prominent object, that is,
we have a better occluder.

pouring_liquid_183.jpg : totally wrong face pose. But if it were true, same as was said for writing_on_a_board_028.jpg , which is 
that there is no occluder here although straw ends inside face. 

smoking_014.jpg : finger detected instead of cigarrette. Note it covers the mouth and is somewhat above it. This is not apparent
from the chosen line segments, but it is from the region of the finger. 

playing_guitar_199.jpg: again, corner of mouth creates shadow detected as straw. face not occluded in any way. should be easy to rule out.

brushing_teeth_141.jpg : detected line are totally parallel to mouth, across it (as opposed to "sticking out of it"). Make sure that
object "penetrates" mouth and is not simply a part thereof. 

running_203.jpg : not occluded at all. 

Summary:
1. **Make sure there is at least a minimal occlusion / interaction
2. Straw should either end outside face or end with something that is occluding the part of the face where it ended.
3. Straw should "penetrate" the mouth, not cross it. 
4. Straw's direction should be consistent with that of face (no right-pointing straws for left looking faces, for instance)

29/4/2014
----------
Now there are reasonable straw candidates. 
I am now extracting a rectangular region in the direction of the detected straw-like object in order to extract features from it and classify straw/not straw.
This is done with fisher kernel, but I will later want to do it with the deep-cnn features. For now it seems enlarging 
the area around the straw does a fine job (size 200x80 pix). Now I am examining the false alarms. 
I want to: 
1. examine the shape of the lips
2. determine in an even better way if there is interaction and what are the candidate objects
3. maybe have several candidates for the interacting object, if it is missed (or not first place).

4. Organize code a bit (break into smaller functions).
5. Analize lips: detect lip landmarks, find if mouth is open or closed, are lips pressed together, etc. 

more straw features: 
1. length
2. near center of mouth (nearest center of mouth relative to other candidates)
3. can give higher scores to segments supported by UCM 
Analyze lips.

You just switch from removing segments to making their scores low. Debug this to check you did it right-->seems right.


* I played a bit with spagglom and it brings very nice results for "foreground" extraction. This can be used to prune false-positives, I'm sure. 


11/5/2014
---------
Now need to zoom in on the object being analyzed, extracting specific features from it. 

1. Segment out (or assess carefully) the outline of the object, specifically start and end of it. 
2. Extract features (ie. fisher kernel) while paying attention to the object boundaries (i.e, only from within the object, or separately from the object and its surroundings)
3. pay attention to the beginning and end of the object, which have special importance (the tip of straw vs cigarrete, the start of a toothbrush).
4. Self similarity of two ends of straw. ++


Scan, as with the bottles, several directions around the mouths to detect the straw, etc. 
Note that you have corrected the code for the graph-cut usage, so you can check the impact of this on the occlusion detection in the case of bottles. 

Some parameters to play with:

1. Find the best GPB segment to match the hypothesized straw. Only if this isn't found , try out the grab-cut approach.
2. Check if adding the Canny edges actually hurts performance or not.
3. Use all the data you can : why detect the straws when you already have their annotations, for the training set? 
4. Find candidates by checking gpb segments


13/5/2014
---------
On finding occluding regions:
I do not always get the desired result. Sometimes there aren't simply enough candidates, and although an occluding region 
is found partially, it does not adhere to the conditions of being partially out the face, etc. So it is totally missed,
or tossed in favor of a less preferable (i.e, not occluding) region.

I want to have more region candidates but not hallucinate regions. This can be done be having more grab-cut seeds, such
as I started to make, but try to fine-tune the graph-cut to be conservative : don't "force" it to hallucinate regions.
Another method is to use the spagglom code, or some other foreground extraction method. 
Finally, I want to find a good estimate of the outline of the faces, maybe "shape sharing for object segmentation" can help here,
or "scene completion using millions of photographs". Or just a nearest neighbor approach using the (presumably) visible parts of the face. 

Start some experiment with sp-agglom : extract "foreground" regions and see how they relate to the segmentation-->
this seems to create quite a lot of false regions. 
One example for a bubble-wand which is totally missed is in image 'blowing_bubbles_089.jpg'. Try this one for started. 

25/5/2014
---------
I have improved the method to find occluded regions in the faces; I think that many of the errors stem from bad  localization of the facial region;
I'll try to improve this next, for example run rcpr on the images.
---> Ran RCPR on the images, need to see how to use this. There is some more improvement in the mask detection, need to bring in some additional features / 
better pose estimation (i.e, rcpr). Example of additional features - T-junctions, shape-sharing masks, segmentation transfer or grab-cut,
some sort of saliency or difference with rest of face, breaking of symmetry or expected boundaries...
rcpr finished running, results are incorporated into the feature extraction. 

28/5/2014
---------

* training again everything, this time using some negatives from all over the dataset (i.e, faces which have a high enough score, this is just to have more negatives, and avoid overfitting) --> this really screwed up the results, need to check why - probaby a bug, though. run again tonight. Remember to run the rcpr features on the rest of the faces...

* annotate all the faces in the training set for the 4 classes. Also, if a face you annotated does not fit the ground-truth,
prefer your annotation for purposes of landmark localization / feature extraction (only in training)

----> you ran again all the facial landmarks of RCPR, only on the faces above score -.6. Everything can be reviewed again in due to this - specifically, occlusion.

----->you once marked the action objects for many classes, check how to revive this data.here it is:
~/storage/action_rois. -- > run mark-action-rois in peripersonal

3/6/2014
-------
I've added annotations for the locations of faces and objects for all 4 classes. Allowing the algorithm
to know the locations of the objects at both train and test time significantly boosts the results.
Now adding global features to see how well they perform on their own. ---> The global features do not provide a good boost 
at all .

Need to show that it is not only the location of the object, but also it's span, that aids the recognition.
I have added for several more action classes (out of the hard ones) the annotation of the objects. 
It would be interesting to show (do a psychophysical test) that without these bounding boxes (blur / hide them)
humans have a much worse perception of the action. Tell ullman...

--> at test time, replace the manual face with the automatic one -- > this has been done and works well.

--> note that the indices of data do not necessarily correspond to those of validindices - be sure to find the correct index of data before accessing it. (data contains info about the best upper body found). 

--> apply your stuff to phrasal recognition, willow action dataset or some other popular one. 
--> have an option to turn off the global features, they're not so interesting. 
--> use salient features extracted around, but not including the upper body or the facial area. 
--> continue working on predicting the location of the action object.
--> do not crop the faces so tightly before applying the feature extraction!!!!

~/storage/misc/upper_bodies.mat --> where you save the upper body data

---> UBC body configurations: check it out! (downloaded...) -- > this seems like an even better upper body detector than the one previously used. However, keep using the previous one, as, coupled with the foreground saliency measure, seems to do a pretty decent job.

--> together with the saliency measure, your upper body detectors seem to outperform the "normal" ones by a large margin.... 

---> you're just trying the new,fast, segmentaion by Arbelaez et al--> it works out of the box,
you can see if this improves things. 

---> you're just about to train person detectors at different poses using DPMs
--> doing this with 20 hard negatives per round and 200 max. hard negatives. (/home/amirro/code/3rdparty/llda-dpm-release ) 
---> this is still too slow... find out how to make it faster, or use only root models, or exemplar-lda, or something...
---> there's a bug using the smoking object prediction, now looking into it. 

---> pose estimation is a fraud. It doesn't work. The results by, e.g, k-poselets are appaling.... I'm trying to make (experiment 43) a decision process to reach the right image area. 

15/6/2014
---------
What would be the model for competing on a dataset? Since you now have a fully automated version (which requires
no intervention in the test phase), you can run the code on the full test set and see the results. 

WishList
--------
* facial action units - see if you can train something to discriminate the shape of the lips.
* also determine if the lips are visible - can apply the same procedure you did to faces, but to lips; find a probability mask for the location of the lips and see if there are occluding segments. 
* downloaded some neil data, check it out (in voc-release 5, the big mat file)--> works, but no so useful
* Write full pipeline with caching
* Improve face detection by having a strong classifier for face/non face. For example try downloading 
the code from github (girshick, malik) for using dcnn + selective search and training on faces. 
* use e.g, spagglom to prune false positives for objects (faces) by using only likely "foreground" regions
* Incroporate occlusion boundaries
* Incorporate the face detector from dallal (which is tested on ESOGU)
* Running the vl-feat experiments section to train a full fisher-kernel encoder and validate on pascal VOC
(matlab <4>) --->did this, and got some results. Can use their fisher-kernel for classification. 
* run the head detector (inside the upper body detectors...) and only within that run the face detection
Can score the face as upper body + head + face score
*handle short straws....
* use the spagglom 
* apply share-sharing code-->running, and segmentation transfer code
* install FFLD (exact acceleration of linear detectors)
* iteratively estimate the mask of the face by rotating to an upright position
* run shape sharing on the head regions alone
* you downloaded 3rdParty/symmetry_1.0 : this can be used to detect symmetry in images, try to use this 
as a feature for:
	a. detecting straws / occluding objects (which are presumabely symmetric? 
	b. finding non-symmetric faces, which might indicate occlusion
* learn a mapping from the detected keypoints to a desired face mask. use this face mask to segment the face, while adhering to image boundaries, but not ignoring possible occluders
* mark all ground-truth regions in the subset to show that using the "real" regions is beneficial
* add a saliency measure to finding the object masks 
* the new saliency measure (cpvr2014saliency, in windows), seems to provide really nice results for foreground extraction.
use this to: 
	1. extract multi-scale saliency from the images
	2. remove false alarms
	3. extract saliency on the face  sub-images as well
	4. guide the face detection/ landmark localization
	5. guide the final face segmentation after finding the probability map - otherwise, try to
	find the face independently using this measure
* run the full pipeline of hand detection code (including segmentation).

* poses / dpm: 
* want to predict the location of the action object. This can be inferred from:
 the bounding box the person
bounding box of the head
generic features
pose enstimation, but use a set of poses learned from training, i.e, try to match the pose of the test image to one of a set of learned poses and use this pose as a prior for the location of the object ( this is actually similar to what you did with the poselets) . 

use the bcp (experiment 40) to predict useful locations from discriminative parts, check if there is the LDA fast training code in there. 

---> detecting faces on all upper bodies currently found in the 

** ruining the context for image classification : 
* take PASCAL + a state of the art classifier. Destroy the images in such a way that will show that the learned models
use more context than the appearance of the object itself (notice hard/truncated objects). Show that there should not be,
except for cases that really need context, a difference between classification and detection. 

22/6/2014
---------
1. Define a new dataset (challenge) of Face-Related actions ; 
The dataset will be comprised of :
1. upper bodies of people doing one of the actions (can extend later): drink, smoke, blow bubbles, brush teeth,phone
2. Face and action object annotations
3. Labels
4. segmentation masks / annotations of the action objects.

Train and test baseline classifiers as you already have. 
Show how you can find the action objects and improve state-of-the-art.

26/6/2014
--------- 
1. Finish extracting features , i.e, experiment 44
2. train dpm's for : specific action hands, specific action heads & objects, entire "phrase"

29/6/2014
---------
1. For dpm / object training : canonize object orientation (even mark it) for more consistent models. 
Currently: ran all dpm models for head,hand,object on the fra_db images, result not very good.
2. Trying to predict location of action objects using my ann* model, added image flips. Looks nice,
now need to use these as region from which to extract features, instead of using the ground-truth regions.
3. add some jittering and rescaling to the ground-truth regions.

30/6/2014
---------
Attempting to predict object locations. Tried to do so by using the ANN*. Although results often seem reasonable, the test set results disagree with this. Object location is critical! 
A major difference between the manual and automatic method: in the manual method, I am providing the location of the *action object* regardless of the class. In the automatic method, I try to find the location of an action object of specific type, and then extract features. So there are many more possible false candidates; in addition, such examples were not viewed in training. 
More Concisely : The method should find the action object regardless of its class, if it is to tell the difference between action objects and not image patches in general. "Actionness"


Alternative solutions:
1. Try to find the action object, regardless of class.
2. Add many more negative examples for each class -- > add random sample from the negative classes
---> now retraining with added random negatives.  
3. After having done 2, you can also try to use a spatial pyramid... 
---Extracting again all the features, there was a bug in the previous method. Speeded things up a bit by setting the step size of sift to 2 pixels. 
--> still did not try the spatial pyramid features. 


!! As predicting a "drinking" location is sort of unpredictable for non-drinking objects, train
the classifier against this location as well: train the drinking appearance classifier v.s. the prediction, on the training 
set (non-drinking images) of drinking locations. -- > waiting for calculation of all predictions to finish, then will train

4. train hand locations as well!  
5. train a generic "action object" detector by combining all action objects at once -->currently extending exp. 43_fra for this purpose -->it looks interesting, I will now train one for hands as well. 

2/7/2014
--------
6. The action object detection doesn't work so well - according to the performance reported in exp 44_2 : 
in all manually found regions, the object clearly adds to the performance. In all automatically found regions, the
overall score is not affected so much by the object score. 
Some options: 
1. The detection works well in class images but badly in non-class (i.e, does not find objects of the other action,
which it was not trained against)
2. The detection works badly in class images (and so it doesn't matter much what it found in non-class images).

For 1, let's try setting the manual boxes for the class only images, or for the non-class only images.
For 2, use the locations of hands: first, predict the location of the objects from those of the hand and the face. 
Second, use a hand detector. This will serve both to replace the manually given hands by automatically given ones, and by
adding more prediction power to the location of the object.
--> Also add in the generic hand detector, maybe saliency, etc.

3/7/2014
--------
1. Add a generic baseline, using the face as a center with an extended window around it. 
2. Check where the Ann* model fails to find the face.
3. try a model where the action object is predicted solely based on facial keypoints. 

todo : you have bugs in the DPM learning code (for some reason some classes not learned)
also check exp44_2, where the features for different regions are concatenated instead of being re-split (i think)...



6/7/2014
--------
Checking again my ANN* code : it seems that keeping images small and getting a small number of nearest neighbors is the good thing to do. 
*Make a benchmark for the action object detection accuracy. 
*evaluate the chance that this is indeed the action object using the probability obtained by the voting, somehow.
*make a model more sensitive to features which are nearby the object (and then you can sample more densly near the object itself)
*predict / get the extent of the action object as well.
* sample more densly: using dense-sift, sample inversely proportional to distance from object - both density and scale. 
* make sure to complete the training process for the baseline, to show how poorly it performs.
---> things seems to be going in the right direction in your ANN* algorithm, keep it going; 
you just sampled more closely to the object itself, and more densly (sift...). 


---TODO: you were just about to create a prediction for the mask of the object, by remembring the mask of each voting object and convolving it with its vote map.

8/7/2014
--------
* Infer simultaneously the location of face, hand,object. 
* play with number of nearest neighbors, number of comparisons in kdtree
* make a prediction for the bounding box of the object. 

10/7/2014
----------
Good - rootsift really improves the results

Note the fact that in extremely low resolution the prediction of the action region is done quite well; 
try to quantify this. 

11/7/2014
---------
Note that only objects on their own are indeed the most critical component, while hands on their own seem to have pretty bad performance. However, their combination is usually by far better than each alone.


12/7/2014
---------
Although I seem to have a pretty good detection for the object areas, it seems that performance is still much worse than using the manual regions. In part, this is relieved by discarding examples where there is no visible "object" 
for the manually marked examples --> note that there are also -inf where no hand is visible (or the hand is irrelevant to the action object). It would be nice to somehow "know" when the object is not visible. 
Another thing to note is the different extent; I will try replacing the manual regions by those which are centered on the same center but scaled as if they were found automatically (an arbitrary scaling which seems reasonable), to see how much the extent has to do with the final scoring.

To avoid problems with missing features, for now, test only images where all elements are present: object, hand and face. 

*What does the manual annotation do for non-class images? check how this affects gt_obj_feats
14/7/2014
* add a head+hand,head+obj,hand+obj descriptors for early feature fusion. 
* train the classifier using square boxes as predicted by the automatic method, to better cope with extent issues.
--> maybe make sure to cover the entire object (greedily) by bounding boxes, or jitter the inputs a bit.
* attempt to model for each pixel if it belongs or not to the action object.
* try to make the bounding boxes around discovered objects as tight as possible. First have a simple model
* that tries to find the average bounding box size before predicting. 
* second, try to maximize the score by scanning over a few possible bounding boxes, maybe guided by segmentation. 

1. calculate some statistics about the bounding box size for the different objects. 
2. Try to predict the bounding box / object shape by matching the detected object to training examples.
3. Try to vary the size of the bounding box / it's shape / aspect ratio to maximize the score w.r.t this class. 
4. note that you flip the results for the test images as well when obtaining ground-truth locations. So it would be fair to do this when "predicting" bounding boxes too. 

currently debugging : when I tried to optimized the bounding box to get a higher score, I found out the original bounding box did not score as well as expected.
---> flipping make a big difference!
---> continue parallelizing the feature extraction from predicted location. Then test things again,
and then continue optimizing the score by changing the bounding box...

---> still a very big difference. Why is this? maybe because in the manual setting, only one object is marked in each image. The distinction is between a cup in a drinking image and a cigarrete in a smoking image. Not between a cup in drinking and something arbitrary in smoking. Try to determine a single action object for each image.

Aggregating the object detection results looks pretty good. should also aggregate the hand, head detection results.
In addition, there is information in the maximal value of the probability map generated by the object detectors (a higher maximum indicates that this indeed may be the action of interest, or at least the correct place to look).
---> The probability maps hold relevant class information; Only using these, we can tell between the classes quite well. 

1. Try to add the probability-map score to the other kinds of scores. 
* note that you're currently using 64 visual words for the fisher vector. 
2. Why the large score gap? 
Maybe because we "force" a classifier to classify a region in each image. for class A, do not search images of type B for objects of class A. Instead, classify the regions found by the B detector. 
---> This did not help much either - it did bring an improvement in the object scores, but not a large one.

Does the size of the region affect the score? 

3. search strategies for the objects: 

4. Ullmans suggestion: look at the action mirc region only. add this as a location from which to extract features


20/7/2014
---------
(remember to run again the ann* for the "mouth" type region)
1. Check different normalization schemes for the fisher vectors: (1) none, (2) normalized (3) square root (4) improved
--> checking <2> (currently running again the classification schemes)
---> conclusion: remain with the "improved" normalization. reverting to this now. 
2. Region candidates : 
(1) selective search --> currently checking. 
(2) new gbp segmentation  --- > replaced, as a good (probably better) alternative for 4. 
(3) saliency (high-dimensional color saliency, the other saliency), either the saliency value or distance to a salient region, object shape / size
(4) relation to facial landmarks, this should be pose dependent
(5) 256 component codebook instead of 64.

(6) check for each (test) image which is the best combination : count the number of times each combination brought the best performance. This can serve as an upper data "oracle" performance. 

(7) NOTE: you changed lambda to be a constant 0.0001 in Pegasos. This did not seem to matter so much. Now add many negative samples from non-class images. Also trying to remove the balancing between positive and negative training samples. --> this, too did not seem to matter so much. Although, with no additional negatives, there is only  about x4  times negatives as positives. 


aaaaaa! surely you have a bug in your object classification process. find it! 
21/7/2014
---------
found the bug! :-) - this improves things, but not enough. continuing to check things. 

OK : Now the manual and automatic are quite the same, but there is still a large gap in the detection.

---> Time to turn to specific features, or objects anchored at specific locations:
either hand or face.
(1) For faces, check objects anchored at locations specific to the given action. Learn the appearance at the anchor points. Same for hands. 
(2) detecting action objects: for some actions, the action object should connect the hand and the face

Note that improving to 1x1+2x2 spatial pyramid further improves the results - 
but now we can see a bigger difference in the performance of the classifiers over manually/automatically given object regions. 

Check what is the best performance attainable per image, i.e, find the set of bounding boxes given by selective search so the automatic classifier + these boxes has the maximal performance. 

Constrain the location of the object using the head and the hand:
1. Show some statistics about the distances of objects to hands, heads, mouths-- > something is buggy. 
for now , revert to 1-d histograms (i.e, the distance from either the hand or the head. 
The relative configuration between mouth, hand, object is also relevant. The distance of the  centroid is not always such as good metric since the object may be elongated. The distance to the object itself (it's boundary) is actually more indicative. 

Try to make the voting stronger, i.e, using fisher vectors over the bounding boxes. In addition, try to replace the 64 dictionary with 256. 

24/7/2014
----------
Make a decision based on the mirc area alone:
1. Make sure that this is plausible, i.e, if the object can be seen in the mouth region. 

2. Parse the visible area. 
Attributes to find out:
1. Mouth occluded? If not, shape of mouth. 
2. Shape,appearance of interacting object, form of interaction

1. How to tell if mouth is occluded or not: 
Try to interpret the seen image region, match it to a known image. 

27/7/2014
---------
In order to fully interpret the mouth region, and the action being performed, I want to have the following features:
1. Pose of the face
2. Boundary of the face with the background
3. Facial expression and shape of mouth
4. Existence of intercating object 
5. Location, shape, appearance of interacting object, 
6. Form of interaction: near face, inside mouth, touching which part of mouth.
7. Hands or part of hands near the face

Discussion:
1. Pose: Why do I need the pose? in order to find the boundary (2) and location of mouth, and to facilitate other checks.
How to find the pose? 
options: For training, I may know the pose. 
For testing, use either nearest neighbor, or some sort of regression, or use the pose from the entire facial region (although this is apparently not strictly necessary). 
2. Boundary with background: Important for finding occluding  objects. Another way of formulating this is a pixel-wise map predicting for each pixel whether it belongs to the face or not. This too can be done based on learning. I can also use a pose-depended prior here.
3. The shape of the mouth can be inferred some pose estimation algorithm, but I can also try to model it directly, by defining templates for a drinking mouth, smoking mouth, etc... It is important to first be able to tell if the mouth is occluded or not, since attempting to interpret an occluded mouth can of course lead to false results. 
4. 
	[1] compare to non-occluded faces to find unlikely regions
	[2] use some form of saliency, be it color,texture, whatever
	[3] find if there is an occlusion of specific forms
	[4] apply detectors for specific objects and their parts
	[5] observe features anchored at specific, informative locations
5. This should be largely facilitated by (4)
6. This is also pose dependent, and relies on the localization of edges of the mouth, etc. 

Now, assume that I have the pose of the face, center of the mouth and its corners, and the outline of the occluding object.
Some features that can be extracted here are geometric features, which are
(1) the location relative to the mouth of the object (i.e, occupancy w.r.t to the mouth bounding box and 3x3 bounding boxes in the 8 neighborhood)
(2) "approach" of the object (orientation of major axis). 
1 and two can be summarized with the point on the object which is closest to the mouth center and that which is nearest. 
Some appearance feature can be extracting by making the frame of the object canonical and then extracting more features.
Furthermore, find the nature of occlusion. Something too "blobby" cannot be a cigarret and therefore there is no need to compare it; 
---> It seems that given the object mask w.r.t the boundary of the face and location of the mouth, I can be almost done with the classification task. 

What approach shall I use in order to find the extent and locations of such action objects?
Each action object has its specific 

* TODO: correct some bad annotations.

Foreground saliency seems to be a good cue for the location of the action object. This can be quantified. 
Say that I have some candidates for the action objects after filtering with foreground saliency. I should start learning the features which define them. 

1. Extract a set of candidate image blobs.
2. Compute for each blob :
	FEATURES:
	--------
	geometric (shape, size, location relating to mouth, face) features
	appearance features : generic features (fisher-vectors?), specific features (contours...)
	saliency features : min, max, mean (median? ) saliency values
	occlusion features : can it be an occluder? 


28/7/2014:
Wrote basic code for extracting the set of occluders. Now, I see that It is possible to remove many of the "bad" regions based on their bounding boxes alone,since the geometric location of those bounding boxes does not make sense. 
I want to rule-out as many bounding boxes as possible while keeping the possibly good ones. To that end, I should:
1. Write code to quickly access the bounding boxes (so there's no need to load the regions themselves. 
Define each bounding box by an occupancy map (maybe 7x7, something quite rough), which will then be used as a feature vector. Can also
use center, width, height , or xmin,ymin,xmax,ymax (normalized) to train an svm to tell apart bad from good examples. Boxes which are rejected
with high confidence (i.e, scores too low) will never be considered. 
The bounding-box classifier should apply to each class independently.
But what are negative bounding boxes? Non-object boxes from the same image? Or boxes which do not overlap with any positive example? 
Another way of doing it is building a probability distribution for the bounding box locations, and checking the probability of a new
bounding box by overlapping it with the ground-truth ones. 

Actually this can lead to several geometric features relating to bounding boxes, such as overlap with object locations, overlap/distance from face, overlap/distance from mouth, or any other facial landmark. 

--> Ok, scoring bounding box by their probability based on geometry only works well as a means for ruling out unlikely ones and bringing out the correct region. 
Now, I need to decide on my positive and negative examples for training the occlusion detector.
For positive samples, take the ground truth segments. For negative samples, take the segments residing well outside the bounding box of these segments (i.e, overlap between these segments and the ground-truth segment bounding box is < .1). 
This is also serving as a sanity check, to see that ground-truth segments indeed pass as good occluders.
As a first stage, just collect all ground-truth segments at once (class agnostic), in order to learn their properties. 

- write code to turn the occlusion pattern data into a big matrix. Pay specific attention to NaN values.
- train, try to classify new segments using several techniques; 
1. svm (w. appropriate feature scaling)
2. random forest.

---> I have tried this a bit, using random forest

The above are all methods to try to find a good segment. 
There are two main barriers to overcome. 
1. Findng the correct action object's region
2. Telling the action from the action object + face. 


(1) seems to work pretty well already simply using the bounding box based prediction. 
<---(2) Given the region of interaction, How to tell the specific action?  extract features from this segment which will allow you to classify. These are, again, features that you have already considered in 27/7/2014. 
To mention again (and expand) the features:
	FEATURES:
	--------
	geometric (shape, size, location relating to mouth, face) features
	appearance features : generic features (fisher-vectors?), specific features (contours...)
	saliency features : min, max, mean (median? ) saliency values
	occlusion features : can it be an occluder? 
	pose of the person to normalize the features accordingly.

I wonder, which matters more, appearance, or shape: It seems likely that having a blob in the correct shape interacting properly with the face is quite enough, and that appearance is of lesser importance.  In addition, I wonder if applying test to the exact segment might bring too-optimistic results, since finding the right segment is unrealistic. 
For now, assume it is not unrealistic and that the tests will rule out wrong segments.

Geometric features: 
1. size : area of the object
2. shape: possible shape encodings : contour fragments paper, occupancy mask , different region-props (orientation, etc).
Note the occupancy mask also specifies whether the segment has pixels above the mouth, etc.
Stroke width : width of stroke starting from point nearest to mouth and going along the major axis, or some skeleton (in case the object is curved).

*a question: how can I segment the image so well (as a human performer)? 
Appearance: color, texture

Other:
 . *identity* : is this segment a hand? (part of) object? 
-->   shape of lips, lip occlusion ratio and shape.

Context features: what surrounds the object? What does it end with.

What would some optimal high-level features? (i.e, what I understand from the image)
1. pose of person, including face segmentation mask
2. shape of mouth, open, closed, etc. 
3. location of object, interaction with mouth (inside / docked) ,with hand (type of grasp)

Some class specific features:
Drinking: Mouth is either entirely occluded by cup,or cup lower rim is docked at the bottom of the mouth. Mouth encloses drinking vessel, (straw, bottle). 
Smoking : If inside mouth, mouth closed around it, maybe held in a specific way between a couple of fingers. Cigarrete ends in "thin air". Cigarrete has specific appearance features. 
Brushing teeth : mouth sometimes open, teeth showing, appearance and shape of toothbrush, approach angle. 
Blowing bubbles: bubble wand with a ring at the end of it.  if mouth visible, lips are round, in typical blowing shape. 

How to capture appearance of objects? Orient them using major axis first. Tell the difference between heavily occluded objects and mostly visible ones (not much of an appearance to a small straw showing between the hands - in this case the hand is the cue). Maybe use object functionality attributes? (round, long, has-opening...)

4/8/20
Sequence of actions:
1. Face detection, including rough pose (in order to avoid per-pose reasoning)
2. Location of mouth  : is mouth occluded ? 
	a. fully occluded: what is the occluder?  shape, appearance,what is extent of occlusion? is there a hand / fingers? 
	b. partially occluded: shape of mouth, expression (open, closed, showing teeth, etc). Properties of occlusion w.r.t mouth and occluder. Shape, appearance, functionality of occluder. 

For occlusion, you can use the extent of the object and of the mouth, if you are able to recover them. 

* A graphical model can be used to determine for each pixel its identity and then determine the interaction. 
Still contemplating how to extract these high level features. Start with a simple set of features, which includes:
mouth open/closed
geometry of object w.r.t mouth
object appearance : try to abstract out the object...: apparently there is some importance in the object's appearance, though not always.
Important: model the amount of occlusion relative to mouth coordinates. 

6/8/2014
--------
Think up specific tests with which you will be able to tell the difference between the different categories.
1. Toothbrush: enter from one side: does not appear on both sides of mouth.
2. cigarrette : always touching mouth (unlike bubble wand). color pattern along cigarrete (sometimes)
3. blowing bubbles: occludes mouth /face but does not enter mouth. shape of mouth.
4. Drink : object topology: There is no hole in glass through which mouth is visible, as in bubble wand. 
Object must interact with mouth, it cannot be fully below or above it (must touch).

These are a bit hard to perform without a more fine-grained interpretation of the facial area - i.e, where does it begin,
where does it end, etc.
Next, apply landmark detectors, as you already have, and estimate the borders of the face and the location of the mouth corners, etc.

The landmark detector work so-so. 

11/8/2014
---------
Now that we have good results for the classification, need to predict 1: the mouth center, 2: the object location.
fra_demo.m predicts a probability for the action object (using selective search + prior)
experiment__0043_fra.m is the setup for your ann* code, you were just about to try to make specific parameters for predicting the location of the mouth.
1. finish creating the specific parameters, fix the code you didn't finish writing (done)
2. create a unified location prediction, using all the images you can (including external images, perhaps)

12/8/2014
---------
1.Mouth prediction works almost perfectly. Seems like you can do pose estimation of the head as well
--> use the pascal annotations in order to predict head pose, eye locations, etc ( a bit later).
--> now, replace location of the mouth in the test images with the detected one. 
	--> good, the results seem to be stable under this.
--> now, replace the location of the object + it's extent by your own prediction:
	---> seems like prediction of the object bounding box is hindered by not centering the image around the mouth.
	---> do this next (make votes around the mouth center).
	---> done. 

2.for the box prediction, try to make this pose dependent, or dependent on the location of the mouth. 

TODO: later fix the dataset, make sure each image has only one mouth center, etc. 

Think about a clever way to turn your object location predictions into good masks. 

It is possible that fra_demo has a bug due to faulty bounding box logic...
--->continue experimenting with this, it already worked way better than it currently does. -- > fixed. 

Sort out the x,y order!!! --> done. prediction using the selective search boxes seems to work nicely now, but for a few misses here and there.

I've also changed a bit the parameters for the object detection, also improves somewhat. 
As for the classification, I'm trying to find out if the tight segment / orientation are necessary, as they will be harder to infer.
[*** by the way, maybe infer the orientation of an object by comparing it to all the other objects of that kind and returning the best orientation. Another 
Check the following changes (and combinations)
1. Transform the tight segment into a bounding box (didn't harm performance too much, so keep it that way. Note that this is 
inside the implementation of FisherFeatureExtractor) --->trying this again just to make sure. 
2. Use / don't use the rotated image for extracting features
3. turn the bounding box into a fixed-size box -->hinders results. don't do it. 
4. jittering the training examples (flip, slightly rotate, etc)

check influence of oracle mouths, oracle objects : my own mouths are ok, on cigarettes they even probably improve things a bit. 
For the situation where you have discovered the object but not the orientation, there are some options:
1. try several orientations and choose the best one. (continue to think about this later).

How to find the "right" object extent? Try to find in the training set a similar object: using only the features inside
the high-probability area, compute a fisher vector and calculate the distance to each of those of the ground-truth
objects. I am currently trying this, but doesn't work so well. Specifically, I am not pre-rotating the object images,
so maybe I should start with that. It can also be a matter of the resolution. --->Pre-rotating doesn't seem to help as well. 

It is interesting to look at the nearest neighbors of images. The configuration of the object, hand, mouth should be somewhat the same. 

18/8/2014
----------
Need to find the correct object orientation. 
either : 1. perform selective search on the small region, to obtain plausible blobs
2 .learn hog detectors (exemplar svm) for the object types
3. find the main orientation in the region, using local gradients

---> now trying (1) : the segmentation doesn't seem to work well enough.
4. try refining the prediction and finding the voting classes. 

19/8/2014

--->Matching raw sift descriptors doesn't seem to work so well.
Now trying to train exemplar svms per training object. 
* For each training object, extract a mask and learn an exemplar svm.
Use the svms to vote for the orientation of the object. 

5.* learn * the orientation by regressing from local hog descriptors : collect a large number of hog patches and use those to estimate 
the orientation. 
Or try to vote for the orientation using sift descriptors...

* I think you had a bug in the fisher vector extraction - it is possible that there was a scale mismatch between
the scale of the training images / testing .check it . 

** check what are plausible orientation for each class. An image may not match a class if it was rotated to an unplausible orientation in order to match it. 

21/8/2014
---------
*object orientation:
if the object is clearly visible, the orientation should be easy.
if not, it can be inferred by the location of the hand relative to the mouth (can it??)
* it looks like we can find the action object from low resolution images. So will a basic form of saliency work for most 
cases? how can we find the cases for which it doesn't? 

2/9/2014
--------
First collect the results from your experiments to tell how critical is each feature:
mouth location, object location, square regions for mouth, object.

Based on these conclusions, continue to tackle directions. 
Preliminary results show that the location of the object is (of course... ) the most critical - or the location of the mouth is found with good precision already. 

4/9/2014
--------
Based on the experiment, I draw the following conclusions:
1. Finding the mouth's location automatically is sufficient: It lowers the results a bit, but not significantly, related to the manual mode.
2. The most important bit is the object location and extent. Rotation helps as well but not as much as knowing this. 
This can be partially explained by observing that for many objects of the same kind, the orientation is similar. 

How to find the object's location more correctly? 
1. Rule out unlikely locations given the location / pose of the head. 
2. Add more cues such as color / shape saliency

Need for each face: 
pose of face, location of mouth, are the interacting objects : objects occluding mouth / sticking out of mouth. 
Interpret what you see. 

----> You have started running the ann* code in higher resolution and results seem to be more accurate. Try again using this for
predicting the location and extent of the action object, for automatic classification. 


Use facial landmark location w.r.t the objects as a feature. 
**** estimate facial landmarks using some model, for example, the rcpr model, or your ann*;
	then , use your occlusion reasoning to find occluding objects. 
* use selective search in the higher resolution images in order to narrow down the candidate options for the objects.

--> voting using the shape-masks seems to bring out nicely, at times, the shape of the object. 
refine this using either superpixels, graph-cut, simple thresholding, etc. 

*** Distinguish between objects docked / hiding / not docked at mouth.
For docked objects, find nature of docking and shape of mouth.
For occluding object, find shape of occluder and amouth of occlusion. 
For non docked objects, find relative location to mouth and shape. 

7/9/2014
-------
Want to have a pixelwise map of of hand/object/face/background. 
Let's say that I have a good ground-truth of this. 

Estimate for each pixel it's identity using a learned classifier. 
Estimate for each pixel it's identity using self similarity to the estimated face region,
using local color statistics, or color in a large enough segment spanning thhe facial region. 

* Check for each object two locations: the hand and the face/mouth. 
These are all made to estimate the extent of the object.
but another method is just an interpretation of thhe mouth region itself:
1. estimate where is the mouth region.
2. check if the mouth is occluded or not. 
3. report the properties of the occluder / object docked in the mouth. 

---> currently checking how my new prediction of object location works (but with square masks).
	---> there is either a bug with how I checked things or the prediction isn't so good. 


todo:
* compute saliency maps using both extractSaliencyMap.m and HDCT (in 3rd party) for each image, add this to 
your prediction of the action object
* compute pose and facial landmarks
* predict if mouth is occluded or not and to which extent

9/9/2014
--------
Here's what I see when examining an image of the region of the mouth:
1. Identify that it's a human face*
---> This is already verified by the choice of fra_db images
2. Tell the pose : use the bells/whistles face detector. --> seems to work well enough. 
3. Identify some landmarks such as nose, lips, corners of mouth --> working on this, see below. 
4. Indetify expression : mouth open, closed, teeth showing, shape of lips (smile, round, flat)
5. Identify borders of face (if visible) and occluding objects : 
--> to do this, say you have the pose of the face and location of the mouth. 
6. Identify properties such as skin color, beard, 


* I can tell when it's not a part of a face.

Ok, to solve once and for all the pose estimation problem, I am using my ann* to produce predictions for 3 
facial keypoints; this is done for several facial keypoints, learned from the aflw dataset. 

Once I have that, I'll be able to tell more intelligent things about the close vicinity of the mouth.
Specifically, I will be able to tell how likely a mouth region is, and to what other mouth regions it is similar (in the aflw dataset). 

* Note : in order to make the multi-scale voting better, keep the image size and descriptor size as is, only change the extent a bit. This way the same amount of descriptors will be produced, but for an effectively different scale. 


---> you were just editing untitled.m where you are revising experiment 49, in order to incorporate 
pose data from the DPM detector, to avoid looking for non-existant keypoint. 

----> after this, use some foreground mask or saliency for object detection near the mouth...

12/9/2014
---------
Things to do:
1. Finish with the facial landmark localization

---> Finished for now : it looks pretty good; I am using nearest neighbors from aflw and estimating a probability
for each point. The probability look good in that it reflect the number of training images for which 
each keypoint was found. So I can use it to "drop" unreliable keypoints. This isn't using ANN* at all; 
using ANN* can refine it but it seems fine for now. 
	--> Now you can further refine the location of the action object, by limiting its location w.r.t all of the keypoints. Make pose specific classifiers, learned on the training data and inferred keypoints, for the location of the action objects.
	
	*How to make it pose specific? Think about it a bit later. 
	

2. Download hmbd51 and explore it a bit. Think about how a lot of data (possibly not annotated) can help.
3. Think: what would you do with code that can extract facial occlusions correctly. 
4. Think: are you missing training data? Given any training data that you would like (pixel-wise segmentations of objects, hands, faces, poses, facial landmarks, etc.) , what would then be the learning framework?


15/9/2014
---------
Using the entire hog image for regression using piotr dollar brings good results. One can play around with the parameters for even better results.
For relatively frontal faces, the keypoint (c++) detector you downloaded seems to work well. 
17/9/2014
---------
Nothing seems to work quite as robustly as my nearest neighbor based implementation. It does have it's own downfalls but it will have to suffice for now. 

To continue from where I left off, there is an estimation, using ANN*, of the action object's location. 
Now combine this with the pose. 

---> I am running the bells & whistles face detector on the entire stanford 40. 
----> I am re-running my prediction for the location of objects on fra_db, since the current configuration seems quite good. 

---> think : can you show state-of the art results on the entire stanford-40 ? or another dataset...

18/9/2014
---------
Trying now to create masks for the action objects by applying graph-cut to the probabilities produced by ann*. 
This seems to work nicely, except in cases where the result is degenerate. Thinking on how to solve it. Sending Ullman some examples.
19/9/2014
--------
Some results are degenerate and other are really nice. 
Can incorporate several factors here, and these should be learned:
1. Saliency
2. ANN* probability
3. Local features : texture, attributes.
4. Location w.r.t facial features
5. pose

** Looking at many examples of extracted facial keypoints, it seems that I did a pretty good job there. I think that this is more robust than state of the are for many cases, although it's really simple. 
I can also ensure missing features are completed using some approach. This has to be pose-dependant, so I won't complete missing features which are really missing from the respective poses. 

21/9/2014
---------
It is interesting to see how grabcut can often recover the action object. It depends on size. I think I can incorporate saliency into this as well, to remove "background" regions, as well as find regions which belong mainly to the face. 

Try to find regions in the segmentation which belong to the face but have a low score in the maskPrediction image. 

---> continue writing extractAllFeatures


---> try learning, instead of the ann*, a **regressor** for the voting! fun music!


26/9/2014
---------
Continue writing extractAllFeatures.
for training, consider two stages:
1. find all objects which may be action objects
2. differentiate action objects of certain classes.
3. in order to include mean probability for object prediction in training, make sure each training image does not participate in it's own voting.  -- > done


30/9/2014
---------
Shimon pointed out that it is important to focus on the mouth area. The goal now is to train a classifier for the mouth region, which will be used if the mouth is not fully occluded.
1. Find out if the mouth is occluded or not.
2. Create local classifiers based on the mouth region for discriminating between action types.  
3. Add code to reason about hands near the mouth. If a segment near the mouth area is likely to be a hand, this can be a good cue for the action as well, especially if the object is not so visible. 

How to determine if the mouth is occluded or not ?
use several stages. First, obtain a face region. If this region is well aligned with the estimation of the face outline (i.e, it got a good grade using the e.g, Zhu face detector, and the segmentation agrees with this), concluded the face is not occluded. 
Also, if the only object in the vicinity of the mouth is of high overlap to the estimated location of the mouth, this is probably the mouth segment.
Third, train a detector/classifier for different (non-occluded) mouth appearances and use the output of this detector as a cue; train a deformable part model for different parts such as eye, mouth, mouth corner, etc. Or a bag-of-words style classifier.

* boosting seems to improve things over a random forest. I wonder why. 
* need to take care of missing facial keypoints (location == 0)
-----

2/10/2014
--------
To train : given exact mouth coordinates, an appearance based classifier. 
This is trained per pose and object class.

Assume that you *know* if the mouth is occluded or not. Actually the partial occlusions are the challenging ones. 
Let's ignore occlusions for now. 
Given a well-localized mouth image, extract features and classify. 
Have a model which, given a mouth image, assuming no full occlusion(solve this a bit later),extracts
the location of mouth features, location (if any) of occluding object, and uses this to classify. The spatial arrangement of the difference elements and their appearance will determine the difference for the action classes. 
Must first determine chance mouth is occluded by large object, otherwise there's no chance to do this. 
When I can really determine it, then I can proceed to the fine checks.

What will these fine checks be? Apply a well-trained detector for the different mouth landmarks. 
Find simple geometric constructs, such as parallel lines, ellipses, etc. 

** Your code for find the good region works well in many cases; you just need to find out when it doesn't,
and solve using the specific features in those cases. Also add more candidates using e.g, selective search. 




todo list:
* 

* incorporate pose information, either implicitly by the keypoint locations or explicitly, 
can be done in several ways: 1. keep a variable for the estimated pose
2. use a 3D model, which estimates the pose of the face and the spatial (in 3D ) location of the action object relative to it. 
* add, using cpmc, or the like, more candidate for the object region
* add even more candidates by using graph-cut with multiple oriented windows (with variable widths) around the mouth region (you've done something of the sort before, for the bottle class). Add also slic superpixels as candidates.
* sort out the problem where an image is invalidated since there's not enough "object" pixels within this image (for example, image 911 in fra_db)
* estimate the outline of the head either by regressing from the keypoint locations, or using the face detection, or using UCM
* multi-stage classifier : 
A. rule out segments using a simple metric such as boosting over the location mask. 
B. for the remaining segments, use more interesting locations
 
* another feature to add is the amount of overlap with the face region, i.e, the convex hull / rectangle of the face
* predict bad keypoint locations using the non-bad ones. 
* show the effectiveness of your keypoint localization on more datasets
* adding masks created by examining parallel-lines works ok now. Consider making the mask a parallelogram by taking the long / short edge and duplicating it instead of taking the short one.

4/10/2014
---------
Following the plan:
1. making sure that you have ELSD and lineseg information for all of the images in fra_db.-- > lineseg done 
	(have to make sure for elsd)
2. learning piotr dollar's code to annotate in fra_db with the aflw landmarks.
---> from looking at the aflw landmarks, it seems you have used the calculated pose instead of the real landmarks!!
Otherwise, check that the landmarks are as you tansferred them after cropping, etc. 

--> you were just annotating FRA_DB with mouth keypoints. --> finished - including eyes chin and nose!!


--> use the fra_db train images as well for predicting the test-images keypoint locations. 
--> bad images are currently those where the object does not occupy enough of the image.
	Need to check these cases: if a hand is the reason, then this is not a bad image, but one where the hand
	is more indicative of the action than the object. 


* you were: 
1. running the landmark detection on AFW,COFW
2. running the feature extraction code on FRA_DB, in test mode, to retain all the segments along with their features.

** check what the rots variable is doing (damaging!!) in experiment 49...  look for rots(iu)

* add flip to add the train images... 


12/10/2014
----------
CVPR 2015 plan
(brainstorm)
benchmark on fra_db all of the actions
take care of valid/invalid images, decide what to do with the invalid images
complete the pipeline on the voc3000 (phrasal recognition) dataset
download and run on the test set of the PASCAL actions / willow actions to show on e.g, phone
find recent references on action recognition
try SVM classification on features instead of a random forest. 
extract fisher vectors from training and test images
For all images on Stanford 40 images:
1. detect faces
2. create an fra_db-like datastructure, for compatability with the rest of the algorithm. (started in scratch.m)
3. extract keypoints, predict location of action object

* create a method where you first apply adaboost to remove the non-action segments. 
* train a classifier based on fisher appearance to further refince the segment classification. 
* save as a struct the s40_fra_db struct, which should contain the face detection(s) and facial landmarks for each image. apply to this the pipeline.
	-->> extend the pipeline to calculate all stages if they have not already been calculated, such as the segmentation, prediction of location of action object, etc. 
--> change the implementation of the prediction of the action object so it accepts the detected face ; this will probably create a better scaling of the image.

19/10/2014
----------
Generalize your functions (make everything cachable)
1. Facial landmark localization: split into 
	(1) data preparation, with specific parameters, and saving of results.
	(2) landmark localization with specific required landmarks. 
2. Face detection, pay attention to keeping the results in original image coordinates.
3. Segmentation
4. Saliency (note that some nice features seem to be pacakged in the drfi code you recently downloaded)
5. prediction of action object locations
* pay attention to when get_full_image is used and when it is not. 
[6] create a function to create an fra_db like structure automatically out of all of the results for a given dataset. 
* add flipping, maybe rotation to the ground-truth to enhance it. 
---> fix the discrepancy caused by not taking the new s40_fra faceBoxes to be the fra_db faceBoxes. 
---> you started deleting these files. Next, re-run the scripts in (2)-(4) , and then see if the full pipeline can work on the entire s40 dataset. 
* allow multiple face detections 

[7] add mouth region classifiers. 

[8] you were about to check out cudaconvnet, this might be a good way to improve on all your features (where sift and hog are used...). -->at first glance this didn't work so well.

[9] try to classify for each image how likely it is to be a face...

[10] work more on the features you're extracting. Try removing the heavy region props and add some bounding box related features, which are both easy to compute and informative.

[11] take care of different poses, what happens when a keypoint is missing? 

[12] add features:
	12.1 segment score (from mcg, that is) (think about a solution regarding the score stemming from pairs of line segments)
	12.2 bounding box features
	12.3 the drfi color features
	12.4 fisher vectors, for making final classifications
	12.5 drinking mouths vs non drinking mouths
	12.6 fisher vector for entire image? 


---> you had a problem with an invalid image, in the validation set . What to do with such images? 
23/10/2014
----------	
[13]
In order to train the more specific classifier,
1. split train into training / validation.
extract all segments for both training and validation.

	1. For train images, make a training set with ground truth + overlapping regions as positive and non-overlapping as negative.
	2. For validation images, apply classifier to all segments. Keep k top scoring false regions (regions with small overlap or those from wrong-class images). Also keep ground-truth regions as above.
	switch training and validation to obtain another set of candidate false and true regions.
	
	For all regions, train a strong appearance based (e.g, fisher vector) classifier.


2. Train a classifier to find top-scoring regions in the validation set. To that end, keep all the segments in the training set.
Find the top-scoring false-positive segments according to the boosted classifier. These will be negative samples
and the ground-truth segments will be positive. Feed these to an SVM with strong appearance features. 

[-->Simpler: keep all segments in the training set. Keep the label of each segment as well as it's overlap score with the original ground-truth segment. Split this set so it contains, for training, the ground-truth]

TODO: find the source of nan values...

26/10/2014
----------
1. Add as a classifier the appearance of the mouth: 
2. Think of a nice, better formulation for the paper
3. Show results of your facial keypoint on FRA-DB compared to another popular facial-keypoint detector 

27/10/2014:
As for adding mouth appearance classifiers: it seems this works ok with neural nets. either use this or the fisher vectors. 
Try with HOG descriptors, and also add some jittering to the images.
Can also use neural nets to describe the regions. Use more region jittering to create virtual examples. This can be done for the positive action object regions, too. Try using maybe the CIFAR-10 network, to have less parameters...

% use the image-net very deep features
* create figures for the paper...
---> good, the neural net + fisher vector seems to discern nicely between mouths of different actions. 
Add also mouths of "doing nothing" as negatives examples for all classes. this should further improve the results. 

--> add more training data to smoking, blowing bubbles, brushing teeth?? 

[1] add face score and some global score for early rejections :
	doing this now, based on face score + global DNN feat score
	--> good, we remain with %58 of images while retaining roughly 90% of positives
[2] extract features from all remaining images, so the candidate region detection will be more precise

---> add negative faces (for all classes) from the aflw dataset
----> train a dnn to score faces vs. non faces; detect faces with BAW on VOC-non-person-ids, vs aflw faces... 

 0.325    0.4122    0.6281    0.5010
drink	smoke blow bubbles brush teeth
* .1968	.1459	.3355	.164

 0.3409    0.3083    0.6402    0.5247
--> you ran keypoints on cofw

annotate all faces of people doing stuff in fra-db.
--->continue annotating.

can run the face detector at 4x the original resolution (depending on original images size),
where the face detector fails, run an upper body detector.

L2 normalization for dcnn features?

--> make sure your new fra_db w. annotations is correctly build, extract features based on this and re-train
--> go back to the idea of discarding many bad segments early. 


---> train on entire s40 actions, and use global features where faces are usually not detected correctly (by avg. face score on that category)

matlab<2> checking the 128D neural network output --> worked pretty badly.
-->	 now I'll try the 2048M, hopefully this will be much better, as in the paper.
--> now the -s network...

TODO:	
---> next, remove the action object from all s-40 images and *then* apply the deep learner. See what happens! 
---> you have action_rois ready for many categories




matlab<3> - extending to phrasal recognition --> make a "classID" for phrasal recognition. 


11/11/2014
---------
1: analyze contributions of different parts
2. run classifier on entire s40 faces
what's with the ovps?

(1) stage 1: face-detector score + bb overlap scores + "easy" scores to compute (allregions)

18/11/2014
----------
1. run face detection on TUHOI
2. train a classifier using *all* s40 face images for the classes of interest
3. classify results of tuhoi

----------
24/11/2014
In classification,
The easiest images in imagene are animals,mostly mammals. 
The hardest classes are usually untextured, man-made objects.
In localization, 
The easiest are again, animals.
The hardest are almost all man made objects, many are untextured. 
In * detection*:
The easiest are some animals, a couple of balls
The hardest are mostly man-made, except ski, which is a scene. 

* Selective search seems to localize objects with high recall and a very good
bounding box overlap score. You can use this as a preliminary state for ruling out interaction objects.

* suggested mini-project: find the breaking point resolution of state-of-the-art classifiers by gradually decreasing the resolution of objects and checking how the classifier's average precision changes accordingly. 

26/11/2014
----------
Directions for future (ongoing)
1. expand the dataset : find more face related actions, using known datasets.
2. work on the presentation: make a skeleton + draft, send to ullman
3. make the current work more complete, do not limit so much the extent of the bounding box around the face;
the distance of action object w.r.t the face should be learned instead of chosen arbitrarily, and most candidate regions should be pruned early as they do not meet simple criteria. 

2/12/2014
---------
Today: 
1. 9-10 : presentation -- > done 
2. 10-11 : explore more datasets for action recognition : 
	Candidates:
		TUHOI (Treto Universal Human Object Interaction dataset)
		MPII Human Pose Dataset
		HMDB (Video)
		Stanford 40, of course		
3. 11-12 : plan and write smarter code for the tiny objects classification:
	1. 
4. 13-14: plan how to organize your pipeline and make it more generic and streamlined. 
	
	1. rewrite the process, using the main components:
	Generic pipeline
	----------------
	* make a function to convert any dataset to a single format, 
	* where image paths, labels and ground-truth regions, if any, are represented	
	* store intermediate results as sub-directories of a main directory, with fixed names
	A. Detect faces + facial landmarks, store in a convenient struct
	B. Extract candidate region bounding boxes
	C. Extract relative features, etc, and apply classification
	
	Learning pipeline
	-----------------
	* define a "training" mode for the generic pipeline, where feature types are added incrementally
	in order to avoid killing the system with too much features, or manually add more features as you go. 
	For example, at first use only bounding boxes which are not too far away , too large, etc. 


* before that, a thought(trivial?): for each segment, really make sure that this is an **interacting** segment with the person - This is really important ! 

Dillema : should I deal with the entire image or just a small region around the face? The main problem with the second approach is that it will cause some of the training examples to be invalid because the ground-truth objects are left out.
On the other hand, dealing with the entire image creates some more technical problems and there will be no longer a situation where a small area of the image is focused on. It will also be inevitable to incorporate the usage of hands here. 

A unifying framework would be to combine face, hands and gaze, each contributing its own indication. It is important to recognize the different elements involved in the action. 

Put more simply, the general problem has two aspects: 1. finding the relevant image region 2. analysing it to infer the action class; (1) requires mainly good pose estimation , (2) is the main analysis and happens after the small region has been found. So, not worrying myself too much now about pose estimation , I should focus on (2).

** A general framework would be to find : (1) the object the person/agent is interacting with, and the type of interaction (2) the effect, if any of the object being used (if it's being used as a tool). 

---------------------------------------------------------------------------------------
can have a "lite" mode where I am using only the mouth area. 
make sure you're not usng any ground truth data while testing the object prediction module. 

Dec 11 2014:
-----------
Seems like you have some bug in you training procedure, as, for example training an svm on bounding box properties
returns an avg. precision of 0.5...look into this

todo : incorporate gaze direction into prediction of location of action object

todo : check what is being done with noisy keypoint predictions-->I think that this is no longer a problem, as you have solved it in a previous version

todo: you need to find out if the candidate regions cover with good enough accuracy the required action-object regions. 
It currently seems that a boosting based method better predicts the location of the bounding box. 
This may be due to feature normalization. Random forests seem to do nicely as well, but noticeably slower, which is surprising and probably due to some technical issue such as running single threaded. Continue to explore this direction.

15/12/2014
----------
It seems that the random forest, (or boosting) can point nicely to the location of the action object, especially
if you consider the top k bounding boxes (e.g., k==5). 
Consider the following scheme:
1. Detect the location of the action object using the learned classifier
2. Run a classifier on the appearance of the action object.

For training, train the classifier in a cross-validation manner to obtain "realistic" bounding box predictions. 

Training : For each training image, collect the top 5 predicted image regions. Each region will be given its overlap w.r.t 
the ground truth region. These will be positive and negative examples.



17/12/2014
---------- 
plan how to make the entire training process as unsupervised as possible, in order to be able to extend more quickly to other classes.

23/12/2014
---------
I am trying to rule out bodyparts where no action happens.
This is done by comparing the detected mouth to the mouths of the training set and checking the distribution of action classes. Given this distribution I can deduce if this mouth is likely to be involved in an action or not. 
Well, it doesn't seem that the nearest neighbors based on dcnn are very good, but perhaps I should jitter the examples a bit around them... continue thinking about this. One option is to break the image into superpixels and try to assign
to each superpixel a probability that it belongs to face, object, hand or background. Can also use low-level occlusion cues here...

28/12/2014
----------
trying to improve on the landmark localization by regressing from the fc6 features, but it seems I should first find the yaw, pitch ,etc before doing that. 

29/12/2014
----------
after a productive talk with guy - focus only on lip areas, so what you can get from that.

----------

30/12/2014
-----------
Points from shimons talk:
1. We will evantually expand to broader categories, for now it important to show that you are able to provide a good 
internal interpretation of the action object.
2. it is important to specify exactly the scope of the work. For example, for now, only images which include
the mouth and an action object but do not include the hand.
----> This implies the following:
1. obtain the ability to annotate with good accuracy the mouth's keypoints and the action object. Finding the action object will be much easier given it's extent. its extent will be of course easier to determine once you find it...

Needed modules: 
1. accurate keypoint annotation for mouth structure
2. prediction of occluded areas
3. prediction of action objects
4. features of action object + relation to mouth

* where can I collect a large amount of data about people appearace? 
1. kohn-canade facial expressions, afw, aflw, the paper about landmark transfer (maybe the code can be applied,...),
2. trying to predict keypoints using the linear regression
3. maybe to train an occlusion classifier by generating many synthetic examples, or collecting more data from the internet
4. need to reason about pose as well, cannot continue with this unified pose for all classifiers.... 


31/12/2014
----------
1. obtain a good keypoint localization that also infers the visibility of facial landmarks:
2. 
(it can be nice to train an additional fc6 based classifier to tell faces from non faces)
---------
My Projects: 
1. General : learning a classifier with negative examples similar to positive ones - what if there are negative examples which are similar to the positive class? what will the classifier learn? how about making the clustering discriminative as well? 
one solution is to cluster the data before classifying. Make a dataset to demonstrate this. 
2. Unsupervised/semi supervised action recognition: learn a sequence of decisions on image regions for action recognition.
3. Mouth/face related actions: Look only at the small vicinity of the mouth. Make the most out of this. Test if this is informative enough or not, and if not, move on to other regions.
4. improve my face detection/landmark localization using cross-validation, use existing datasets to show robustness of results.

6/1/2014
--------
Good, linear regression on the fc6 features seems to work fine for finding the rough location of facial landmarks. 

1. There's something weird with removing the mean value of each feature vector, as I do this only in testing
2. maybe can use this to also better locate the bounding box of the face (location, scale) before actually predicting keypoints
3. probably a better idea to  deduce first the pose
4. try some simpler methods, such as extracting hog features for the regression
5. add structure between the keypoints - > learn using the fitlm function of matlab and use the more sophisticated models.


11/1/2014
--------
you're training a much better face detector by:
1. detecting many non-faces on pascal-non-person images
2. using very many positive faces from various datasources
3. training a classifier using fc6 features to tell the difference
4. running a detector using edgeboxes + your classifier

faces can be obtain by aflw faces, for example, also fldb, also also: you downloaded the consent from here,
make sure to sign it!
http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html
in addition, find non-face images by getting the ms-coco annotations, and using images where there are 
no faces at all

read this:
http://arxiv.org/pdf/1411.7923v1.pdf

5. try to train your own deep neural net for faces vs non faces
	5.1 train this neural net to predict the locations of the facial landmarks, too, and their occlusion flags
6. try to find out if intersecting "edge boxes" with face detections helps
(i.e, retaining only faces which are good according to edge boxes)
7. using edge boxes, bring up candidate bounding boxes which are smaller than the size
detectable by the normal face detector, and apply the face detection to a "blown up" version of them as well


13/1/2015
1. training svm on labels
2. extract faces from train2014, keep for each image the face crop + fc6 features
3. look at the casia web faces...


--> instead of predicting exact mouth location, first predict the rough center of the mouth, then make the prediction finer.
annotate missing mouth keypoints + objects (so you'll know which is occluded and which isn't)
----> I now have a better face detector : simply re-rank detection of dpm using the fc6 classifier.
next, use the edgeboxes to create candidate for each image and retain the top 3.

Think about a model and implement it.
Here's a beginning:
------------------
For a given face, find the mouth region (ignore areas where this is not "feasible").
Find if there is any mouth interaction:
	1. Find if there is an object blocking the mouth:
		-> 1: find the pose of the mouth (i.e, the face
		-> assess where is the boundary of the face (learn this)
		-> assess where are mouth keypoints and if each is occluded or not (find a suitable dataset and learn it,
		or learn from unoccluded images, which are plentiful)
		-> find candidate objects by segmentation or detect them using object detectors
		

	
Assign to each pixel/superpixel whether it belongs to face or non-face.
For each face we can discover: 
1. face pose
2. facial landmarks -->facial expression
3. occluded regions

possible flow: 
phase 0:
0. detect faces, compute superpixels
phase 1:
 estimate face pose  ----> estimate facial landmarks
 for each superpixel estimate if it is a face superpixel or not, using color & texture (classify+use context),
	also use the location of the superpixel (i.e, inside mouth, general face, eyes, etc.),
and finally, can use local occlusion cues to learn occlusion. 
given the estimate of the face/non-face region, re-estimate the facial landmarks 
phase 2:
For given non-face regions, use their properties (shape, relation to the face, appearance), to infer functionality.

For this, I need to learn how to infer if a face is occluded or not. I can of course create 
a synthetic dataset

face detection -> done
superpixels : slic (get the new slic from kovesi) , or the ones generated by mcg, whatever
 facial landmarks: either use my own or a known good model, trying to find...

--> you were just reasoning about how to do some figure-ground segmentation; and looking at variou
s papers and datasets concerning this. see the latest additions in the reading list 
and related datasets.

---> How to learn?
Assume I have a big ground-truth of faces annotated with face visibility masks + keypoint locations +
action objects with their types annotated. Well, I sort of have it, except for the visibility masks. 
I can learn visibility masks from perhaps other sources. For example,
a large dataset of faces, I can infer visibility from mean-saliency ? 
--->Mini-project - say you applied saliency to many images; transfer expected saliency to current image.
Where there are object interfering with this expected mask, this is a possible occlusion.

Can learn a local classifier to differentiate face/non face pixel. 
Need to infer if facial keypoints are visible or where would they be if visible.

16/2/2015
----------
Want to train action classification which outputs a full interpretation: facial landmarks + occlusion + action object boundaries
+ action class. 

I have started by extracting deep network features from several image locations, including entire image,face bounding box,
eye, mouth, etc. 
Now an SVM with the concatenation of these features performs well, but I want to go further: 
learn a deep net to discriminate between the appearance of body parts given different actions. 
Then learn how to predict for a given body-part some landmarks as well as a segmentation+occlusion mask. How to learn this? 
Another option - learn to predict, for each pixel, if it belongs to an action object or not. 

29/4/2015
--------
Experiment 64.
I have extract very deep conv. features from all of s40 images. I used those to classify the images and look at false positives.
The results of these are very interesting, and they allow us to understand what the features are missing.
Among the common causes, we can see:
1. Wrong human pose (clapping vs applauding)
2. Wrong human-object relation (near a horse in feeding vs on a horse when riding)
3. Wrong object (cup in mouth instead of toothbrush) / missing object
4. Wrong / missing human-object interation : A person near a dog is not walking the dog if there is no leash attaching the dog to the person

To solve, we need to :
1. detect the people and objects (including scene properties, such as "wall")
2. find the form of interaction between them
3. do good pose estimation


** In addition, find systematically which image regions, when removed or otherwise abstructed, hinder the classifier performance
show that the relevant regions do not cause this lowering in performance and thus the classifier misses the point!


things to check:
---------------
fully convolutional net for prediction of semantic segmentation masks
showing region importance (experiment 64, getRegionImportance)

given the predictions you now have in exp. 64, group them into regions using e.g, edgeboxes and find meaningful 
areas, then figure relations about these areas.


